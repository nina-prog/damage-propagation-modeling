{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:37.900316500Z",
     "start_time": "2024-05-09T15:31:37.830560800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:37.980322700Z",
     "start_time": "2024-05-09T15:31:37.900316500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Topic: EX2 - Turbofan RUL Prediction\n",
    "**Task**: Predict the remaining useful life (RUL) of turbofan engines based on given sensor data (time series data). It is a forcasting problem, where the goal is to predict the number of cycles an engine will last before it fails.\n",
    "**Data**: Turbofan engine degradation simulation data (NASA) - [Link](https://data.nasa.gov/dataset/Turbofan-Engine-Degradation-Simulation-Data-Set/vrks-gjie). See also in the topic [introduction notebook](https://github.com/nina-prog/damage-propagation-modeling/blob/2fb8c1a1102a48d7abbf04e4031807790a913a99/notebooks/Turbofan%20remaining%20useful%20life%20Prediction.ipynb).\n",
    "\n",
    "**Subtasks**:\n",
    "1. Perform a deep **exploratory data analysis (EDA)** on the given data.\n",
    "2. Implement a more efficient **sliding window method** for time series data analysis. -> 🎯 **Focus on this task**\n",
    "3. Apply **traditional machine learning methods** (SOTA) to predict the remaining useful life. Includes data preparation, feature extraction, feature selection, model selection, and model parameter optimization.\n",
    "4. Create **neural network models** to predict the remaining useful life. Includes different architectures like Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), or Attention Models. Note: You can search for SOTA research papers and reproduce current state-of-the-art models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports + Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:38.193478100Z",
     "start_time": "2024-05-09T15:31:37.980322700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# third-party libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_circles, make_classification, make_moons\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "# previous\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from sklearn.svm import SVC\n",
    "import sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:38.304026600Z",
     "start_time": "2024-05-09T15:31:38.200491900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/niklasquendt/Documents/Uni/PSDA/Uebung2/damage-propagation-modeling\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'winsorize' from 'scipy.stats' (/Users/niklasquendt/Documents/Uni/PSDA/Uebung2/damage-propagation-modeling/.venv/lib/python3.12/site-packages/scipy/stats/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m flatten\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_loading\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_data, load_config\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_cleaning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clean_data, format_dtype\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrolling_window_creator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m calculate_RUL, RollingWindowDatasetCreator\n",
      "File \u001b[0;32m~/Documents/Uni/PSDA/Uebung2/damage-propagation-modeling/src/data_cleaning.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Union, List\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_logger\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutlier_detection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m remove_outliers\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrolling_window_creator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m calculate_RUL\n\u001b[1;32m      9\u001b[0m logger \u001b[38;5;241m=\u001b[39m setup_logger(\u001b[38;5;18m__name__\u001b[39m, level\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDEBUG\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Change the level to 'DEBUG' to see more information\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Uni/PSDA/Uebung2/damage-propagation-modeling/src/outlier_detection.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_logger\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneighbors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LocalOutlierFactor\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m winsorize\n\u001b[1;32m     15\u001b[0m logger \u001b[38;5;241m=\u001b[39m setup_logger(\u001b[38;5;18m__name__\u001b[39m, level\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDEBUG\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Change the level to 'DEBUG' to see more information\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_outliers_zscore\u001b[39m(df: pd\u001b[38;5;241m.\u001b[39mDataFrame, soft_drop: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, threshold_sd: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'winsorize' from 'scipy.stats' (/Users/niklasquendt/Documents/Uni/PSDA/Uebung2/damage-propagation-modeling/.venv/lib/python3.12/site-packages/scipy/stats/__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Zeigt das aktuelle Arbeitsverzeichnis an\n",
    "print(os.getcwd())\n",
    "# Setzt das Arbeitsverzeichnis auf das Projektverzeichnis\n",
    "os.chdir('/Users/niklasquendt/Documents/Uni/PSDA/Uebung2/damage-propagation-modeling')\n",
    "from src.utils import flatten\n",
    "\n",
    "from src.data_loading import load_data, load_config\n",
    "from src.data_cleaning import clean_data, format_dtype\n",
    "from src.rolling_window_creator import calculate_RUL, RollingWindowDatasetCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:38.400456500Z",
     "start_time": "2024-05-09T15:31:38.304026600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"Set2\")\n",
    "sns.set(rc={\"figure.dpi\":100, 'savefig.dpi':200})\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:38.492590100Z",
     "start_time": "2024-05-09T15:31:38.400456500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/niklasquendt/Documents/Uni/PSDA/Uebung2/damage-propagation-modeling'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure to execute this cell only once for one kernel session, before running any other cell below.\n",
    "os.chdir(\"/Users/niklasquendt/Documents/Uni/PSDA/Uebung2/damage-propagation-modeling\") # set working directory to root of project\n",
    "os.getcwd() # check current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:38.652221700Z",
     "start_time": "2024-05-09T15:31:38.560578800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PATH_TO_CONFIG = \"configs/config.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load Config + Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:38.732034900Z",
     "start_time": "2024-05-09T15:31:38.652221700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mload_config\u001b[49m(PATH_TO_CONFIG) \u001b[38;5;66;03m# config is dict\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_config' is not defined"
     ]
    }
   ],
   "source": [
    "config = load_config(PATH_TO_CONFIG) # config is dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:39.154853100Z",
     "start_time": "2024-05-09T15:31:38.740553100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_data_1, test_data_1, test_RUL_data_1 \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m(config_path\u001b[38;5;241m=\u001b[39mPATH_TO_CONFIG, dataset_num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_data' is not defined"
     ]
    }
   ],
   "source": [
    "train_data_1, test_data_1, test_RUL_data_1 = load_data(config_path=PATH_TO_CONFIG, dataset_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-25 10:02:58 [\u001b[34msrc.utils:60\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loading data set 2...\u001b[0m\n",
      "2024-05-25 10:02:58 [\u001b[34msrc.utils:89\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loaded raw data for dataset 2.\u001b[0m\n",
      "2024-05-25 10:02:58 [\u001b[34msrc.utils:90\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Train Data: (53759, 26)\u001b[0m\n",
      "2024-05-25 10:02:58 [\u001b[34msrc.utils:91\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test Data: (33991, 26)\u001b[0m\n",
      "2024-05-25 10:02:58 [\u001b[34msrc.utils:92\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test RUL Data: (259, 1)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_data_2, test_data_2, test_RUL_data_2 = load_data(config_path=PATH_TO_CONFIG, dataset_num=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-25 10:03:11 [\u001b[34msrc.utils:60\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loading data set 3...\u001b[0m\n",
      "2024-05-25 10:03:11 [\u001b[34msrc.utils:89\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loaded raw data for dataset 3.\u001b[0m\n",
      "2024-05-25 10:03:11 [\u001b[34msrc.utils:90\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Train Data: (24720, 26)\u001b[0m\n",
      "2024-05-25 10:03:11 [\u001b[34msrc.utils:91\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test Data: (16596, 26)\u001b[0m\n",
      "2024-05-25 10:03:11 [\u001b[34msrc.utils:92\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test RUL Data: (100, 1)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_data_3, test_data_3, test_RUL_data_3 = load_data(config_path=PATH_TO_CONFIG, dataset_num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-25 10:03:21 [\u001b[34msrc.utils:60\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loading data set 4...\u001b[0m\n",
      "2024-05-25 10:03:21 [\u001b[34msrc.utils:89\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loaded raw data for dataset 4.\u001b[0m\n",
      "2024-05-25 10:03:21 [\u001b[34msrc.utils:90\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Train Data: (61249, 26)\u001b[0m\n",
      "2024-05-25 10:03:21 [\u001b[34msrc.utils:91\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test Data: (41214, 26)\u001b[0m\n",
      "2024-05-25 10:03:21 [\u001b[34msrc.utils:92\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test RUL Data: (248, 1)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_data_4, test_data_4, test_RUL_data_4 = load_data(config_path=PATH_TO_CONFIG, dataset_num=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [train_data_1, train_data_2, train_data_3, train_data_4]\n",
    "test_data = [test_data_1, test_data_2, test_data_3, test_data_4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 📍 << Models >>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "[TEMPLATE]\n",
    "\n",
    "Findings:\n",
    "* Interpretation of plots\n",
    "* or other key take aways from previous code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:39.220343800Z",
     "start_time": "2024-05-09T15:31:39.160435700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# [TEMPLATE] - save processed data (as pickle)\n",
    "df = pd.DataFrame()\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "df.to_pickle(f\"{config['paths']['processed_data_dir']}ex2_topic_{timestamp}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:39.318667400Z",
     "start_time": "2024-05-09T15:31:39.220343800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# [TEMPLATE] - save data predictions (as csv)\n",
    "df = pd.DataFrame()\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "df.to_csv(f\"{config['paths']['prediction_dir']}ex2_topic_{timestamp}.csv\", sep=',', decimal='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:39.485116500Z",
     "start_time": "2024-05-09T15:31:39.320350800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 900x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [TEMPLATE] - save plot results (as png)\n",
    "fig = plt.figure(figsize=(9, 6))\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "fig.savefig(f\"{config['paths']['plot_dir']}ex2_topic_{timestamp}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:39.522027500Z",
     "start_time": "2024-05-09T15:31:39.474058600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from sklearn.svm import SVC\n",
    "#from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "klassische Ansätze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainingsdaten in Trainings- und Validierungsdaten aufteilen\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_data_1[[\"Sensor Measure 2\", \"Sensor Measure 3\", \"Sensor Measure 4\", \"Sensor Measure 7\", \"Sensor Measure 8\", \"Sensor Measure 9\", \"Sensor Measure 11\", \"Sensor Measure 12\", \"Sensor Measure 13\", \"Sensor Measure 14\", \"Sensor Measure 15\", \"Sensor Measure 17\", \"Sensor Measure 20\", \"Sensor Measure 21\"]],\n",
    "train_data_1[\"Cycle\"],test_size=0.2, random_state=42)\n",
    "\n",
    "X_test = test_data_1[[\"Sensor Measure 2\", \"Sensor Measure 3\", \"Sensor Measure 4\", \"Sensor Measure 7\", \"Sensor Measure 8\", \"Sensor Measure 9\", \"Sensor Measure 11\", \"Sensor Measure 12\", \"Sensor Measure 13\", \"Sensor Measure 14\", \"Sensor Measure 15\", \"Sensor Measure 17\", \"Sensor Measure 20\", \"Sensor Measure 21\"]]\n",
    "y_test = test_data_1 [\"Cycle\"]\n",
    "train_df = pd.DataFrame(X_train)\n",
    "train_df['cycle'] = y_train\n",
    "\n",
    "val_df = pd.DataFrame(X_val)\n",
    "val_df['cycle'] = y_val\n",
    "\n",
    "test_df = pd.DataFrame(X_test)\n",
    "test_df['cycle'] = y_test\n",
    "\n",
    "# Features und Zielvariable trennen\n",
    "X_train = train_df.drop('cycle', axis=1)\n",
    "y_train = train_df['cycle']\n",
    "X_val = val_df.drop('cycle', axis=1)\n",
    "y_val = val_df['cycle']\n",
    "\n",
    "X_test = test_df.drop('cycle', axis=1)\n",
    "y_test = test_df['cycle']\n",
    "\n",
    "# Standardisieren der Daten\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niklasquendt/anaconda3/envs/bda/lib/python3.8/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Cross-Validation Accuracy: 0.0095 ± 0.0015\n",
      "SVC Training Accuracy: 0.008238429852192876\n",
      "SVC Training Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00        25\n",
      "           2       0.00      0.00      0.00        18\n",
      "           3       0.01      0.05      0.02        19\n",
      "           4       0.05      0.05      0.05        22\n",
      "           5       0.00      0.00      0.00        22\n",
      "           6       0.02      0.05      0.02        22\n",
      "           7       0.00      0.00      0.00        22\n",
      "           8       0.00      0.00      0.00        24\n",
      "           9       0.00      0.00      0.00        23\n",
      "          10       0.00      0.00      0.00        24\n",
      "          11       0.00      0.00      0.00        16\n",
      "          12       0.00      0.00      0.00        18\n",
      "          13       0.00      0.00      0.00        25\n",
      "          14       0.00      0.00      0.00        22\n",
      "          15       0.00      0.00      0.00        16\n",
      "          16       0.00      0.00      0.00        16\n",
      "          17       0.02      0.04      0.03        24\n",
      "          18       0.04      0.08      0.05        25\n",
      "          19       0.00      0.00      0.00        18\n",
      "          20       0.00      0.00      0.00        24\n",
      "          21       0.00      0.00      0.00        16\n",
      "          22       0.00      0.00      0.00        24\n",
      "          23       0.00      0.00      0.00        25\n",
      "          24       0.03      0.07      0.04        14\n",
      "          25       0.00      0.00      0.00        20\n",
      "          26       0.00      0.00      0.00        17\n",
      "          27       0.00      0.00      0.00        16\n",
      "          28       0.00      0.00      0.00        19\n",
      "          29       0.00      0.00      0.00        19\n",
      "          30       0.00      0.00      0.00        21\n",
      "          31       0.00      0.00      0.00        25\n",
      "          32       0.00      0.00      0.00        32\n",
      "          33       0.00      0.00      0.00        22\n",
      "          34       0.03      0.05      0.04        19\n",
      "          35       0.00      0.00      0.00        28\n",
      "          36       0.00      0.00      0.00        24\n",
      "          37       0.00      0.00      0.00        22\n",
      "          38       0.00      0.00      0.00        22\n",
      "          39       0.00      0.00      0.00        26\n",
      "          40       0.00      0.00      0.00        23\n",
      "          41       0.00      0.00      0.00        15\n",
      "          42       0.00      0.00      0.00        29\n",
      "          43       0.00      0.00      0.00        24\n",
      "          44       0.00      0.00      0.00        24\n",
      "          45       0.00      0.00      0.00        19\n",
      "          46       0.00      0.00      0.00        20\n",
      "          47       0.00      0.00      0.00        18\n",
      "          48       0.00      0.00      0.00        18\n",
      "          49       0.00      0.00      0.00        18\n",
      "          50       0.00      0.00      0.00        18\n",
      "          51       0.04      0.04      0.04        23\n",
      "          52       0.00      0.00      0.00        13\n",
      "          53       0.00      0.00      0.00        14\n",
      "          54       0.00      0.00      0.00        20\n",
      "          55       0.00      0.00      0.00        15\n",
      "          56       0.00      0.00      0.00        17\n",
      "          57       0.00      0.00      0.00        14\n",
      "          58       0.00      0.00      0.00        23\n",
      "          59       0.00      0.00      0.00        20\n",
      "          60       0.00      0.00      0.00        23\n",
      "          61       0.00      0.00      0.00        14\n",
      "          62       0.00      0.00      0.00        18\n",
      "          63       0.00      0.00      0.00        20\n",
      "          64       0.05      0.05      0.05        19\n",
      "          65       0.00      0.00      0.00        19\n",
      "          66       0.00      0.00      0.00        16\n",
      "          67       0.00      0.00      0.00        20\n",
      "          68       0.01      0.06      0.02        17\n",
      "          69       0.00      0.00      0.00        22\n",
      "          70       0.00      0.00      0.00        20\n",
      "          71       0.04      0.07      0.05        15\n",
      "          72       0.00      0.00      0.00        16\n",
      "          73       0.00      0.00      0.00        22\n",
      "          74       0.09      0.05      0.06        22\n",
      "          75       0.00      0.00      0.00        19\n",
      "          76       0.00      0.00      0.00        20\n",
      "          77       0.00      0.00      0.00        22\n",
      "          78       0.00      0.00      0.00        20\n",
      "          79       0.00      0.00      0.00        24\n",
      "          80       0.00      0.00      0.00        21\n",
      "          81       0.00      0.00      0.00        20\n",
      "          82       0.00      0.00      0.00        17\n",
      "          83       0.00      0.00      0.00        19\n",
      "          84       0.00      0.00      0.00        24\n",
      "          85       0.00      0.00      0.00        22\n",
      "          86       0.00      0.00      0.00        25\n",
      "          87       0.00      0.00      0.00        31\n",
      "          88       0.00      0.00      0.00        16\n",
      "          89       0.00      0.00      0.00        28\n",
      "          90       0.00      0.00      0.00        25\n",
      "          91       0.00      0.00      0.00        17\n",
      "          92       0.00      0.00      0.00        19\n",
      "          93       0.00      0.00      0.00        15\n",
      "          94       0.00      0.00      0.00        29\n",
      "          95       0.00      0.00      0.00        21\n",
      "          96       0.00      0.00      0.00         8\n",
      "          97       0.00      0.00      0.00        23\n",
      "          98       0.01      0.04      0.02        25\n",
      "          99       0.00      0.00      0.00        19\n",
      "         100       0.00      0.00      0.00        14\n",
      "         101       0.00      0.00      0.00        18\n",
      "         102       0.05      0.04      0.04        24\n",
      "         103       0.00      0.00      0.00        19\n",
      "         104       0.00      0.00      0.00        17\n",
      "         105       0.00      0.00      0.00        15\n",
      "         106       0.00      0.00      0.00        22\n",
      "         107       0.01      0.07      0.02        14\n",
      "         108       0.00      0.00      0.00        18\n",
      "         109       0.02      0.05      0.03        19\n",
      "         110       0.00      0.00      0.00        18\n",
      "         111       0.00      0.00      0.00        22\n",
      "         112       0.00      0.00      0.00        24\n",
      "         113       0.00      0.00      0.00        22\n",
      "         114       0.00      0.00      0.00        19\n",
      "         115       0.04      0.06      0.05        18\n",
      "         116       0.00      0.00      0.00        16\n",
      "         117       0.00      0.00      0.00        20\n",
      "         118       0.14      0.04      0.06        26\n",
      "         119       0.00      0.00      0.00        16\n",
      "         120       0.00      0.00      0.00        23\n",
      "         121       0.02      0.04      0.03        25\n",
      "         122       0.00      0.00      0.00        23\n",
      "         123       0.00      0.00      0.00        20\n",
      "         124       0.00      0.00      0.00        20\n",
      "         125       0.00      0.00      0.00        17\n",
      "         126       0.00      0.00      0.00        20\n",
      "         127       0.00      0.00      0.00        25\n",
      "         128       0.00      0.00      0.00        27\n",
      "         129       0.03      0.12      0.05        17\n",
      "         130       0.00      0.00      0.00        25\n",
      "         131       0.03      0.05      0.04        19\n",
      "         132       0.00      0.00      0.00        13\n",
      "         133       0.00      0.00      0.00        18\n",
      "         134       0.00      0.00      0.00        16\n",
      "         135       0.00      0.00      0.00        28\n",
      "         136       0.00      0.00      0.00        17\n",
      "         137       0.00      0.00      0.00        15\n",
      "         138       0.00      0.00      0.00        19\n",
      "         139       0.00      0.00      0.00        19\n",
      "         140       0.00      0.00      0.00        21\n",
      "         141       0.15      0.09      0.11        23\n",
      "         142       0.00      0.00      0.00        19\n",
      "         143       0.00      0.00      0.00        19\n",
      "         144       0.00      0.00      0.00        19\n",
      "         145       0.00      0.00      0.00        13\n",
      "         146       0.00      0.00      0.00        16\n",
      "         147       0.00      0.00      0.00        23\n",
      "         148       0.00      0.00      0.00        24\n",
      "         149       0.00      0.00      0.00        10\n",
      "         150       0.00      0.00      0.00        21\n",
      "         151       0.00      0.00      0.00        24\n",
      "         152       0.00      0.00      0.00        12\n",
      "         153       0.00      0.00      0.00        15\n",
      "         154       0.00      0.00      0.00        12\n",
      "         155       0.00      0.00      0.00        15\n",
      "         156       0.00      0.00      0.00        20\n",
      "         157       0.00      0.00      0.00        21\n",
      "         158       0.00      0.00      0.00        18\n",
      "         159       0.00      0.00      0.00        25\n",
      "         160       0.04      0.08      0.06        12\n",
      "         161       0.00      0.00      0.00        16\n",
      "         162       0.00      0.00      0.00        14\n",
      "         163       0.00      0.00      0.00        18\n",
      "         164       0.00      0.00      0.00        14\n",
      "         165       0.00      0.00      0.00        19\n",
      "         166       0.04      0.07      0.05        15\n",
      "         167       0.00      0.00      0.00        14\n",
      "         168       0.00      0.00      0.00        13\n",
      "         169       0.07      0.08      0.07        13\n",
      "         170       0.00      0.00      0.00        18\n",
      "         171       0.00      0.00      0.00        16\n",
      "         172       0.00      0.00      0.00        23\n",
      "         173       0.00      0.00      0.00        25\n",
      "         174       0.00      0.00      0.00        11\n",
      "         175       0.04      0.07      0.05        14\n",
      "         176       0.10      0.17      0.12        12\n",
      "         177       0.00      0.00      0.00        19\n",
      "         178       0.00      0.00      0.00        14\n",
      "         179       0.00      0.00      0.00        18\n",
      "         180       0.00      0.00      0.00        16\n",
      "         181       0.00      0.00      0.00        11\n",
      "         182       0.00      0.00      0.00        13\n",
      "         183       0.00      0.00      0.00         9\n",
      "         184       0.00      0.00      0.00        20\n",
      "         185       0.00      0.00      0.00        10\n",
      "         186       0.00      0.00      0.00        11\n",
      "         187       0.00      0.00      0.00        11\n",
      "         188       0.00      0.00      0.00        15\n",
      "         189       0.00      0.00      0.00        13\n",
      "         190       0.09      0.09      0.09        11\n",
      "         191       0.03      0.08      0.04        12\n",
      "         192       0.00      0.00      0.00        12\n",
      "         193       0.00      0.00      0.00        11\n",
      "         194       0.05      0.12      0.07         8\n",
      "         195       0.00      0.00      0.00        10\n",
      "         196       0.00      0.00      0.00        10\n",
      "         197       0.00      0.00      0.00        12\n",
      "         198       0.00      0.00      0.00         7\n",
      "         199       0.00      0.00      0.00        13\n",
      "         200       0.00      0.00      0.00         4\n",
      "         201       0.00      0.00      0.00        10\n",
      "         202       0.00      0.00      0.00        10\n",
      "         203       0.00      0.00      0.00        10\n",
      "         204       0.00      0.00      0.00         8\n",
      "         205       0.00      0.00      0.00         8\n",
      "         206       0.00      0.00      0.00         8\n",
      "         207       0.00      0.00      0.00         9\n",
      "         208       0.00      0.00      0.00         6\n",
      "         209       0.00      0.00      0.00         9\n",
      "         210       0.00      0.00      0.00         7\n",
      "         211       0.00      0.00      0.00         4\n",
      "         212       0.00      0.00      0.00         4\n",
      "         213       0.00      0.00      0.00         8\n",
      "         214       0.00      0.00      0.00         5\n",
      "         215       0.00      0.00      0.00         5\n",
      "         216       0.00      0.00      0.00         8\n",
      "         217       0.00      0.00      0.00         8\n",
      "         218       0.00      0.00      0.00         5\n",
      "         219       0.00      0.00      0.00         4\n",
      "         220       0.00      0.00      0.00        14\n",
      "         221       0.00      0.00      0.00         5\n",
      "         222       0.00      0.00      0.00         5\n",
      "         223       0.00      0.00      0.00         4\n",
      "         224       0.00      0.00      0.00         5\n",
      "         225       0.00      0.00      0.00         5\n",
      "         226       0.00      0.00      0.00         4\n",
      "         227       0.00      0.00      0.00         8\n",
      "         228       0.00      0.00      0.00         7\n",
      "         229       0.00      0.00      0.00         2\n",
      "         230       0.00      0.00      0.00         3\n",
      "         231       0.00      0.00      0.00         5\n",
      "         232       0.00      0.00      0.00         4\n",
      "         233       0.00      0.00      0.00         2\n",
      "         234       0.00      0.00      0.00         2\n",
      "         235       0.00      0.00      0.00         4\n",
      "         236       0.00      0.00      0.00         4\n",
      "         237       0.00      0.00      0.00         4\n",
      "         238       0.00      0.00      0.00         2\n",
      "         239       0.00      0.00      0.00         3\n",
      "         240       0.00      0.00      0.00         2\n",
      "         241       0.00      0.00      0.00         4\n",
      "         242       0.00      0.00      0.00         3\n",
      "         243       0.00      0.00      0.00         3\n",
      "         244       0.00      0.00      0.00         2\n",
      "         245       0.00      0.00      0.00         2\n",
      "         246       0.00      0.00      0.00         5\n",
      "         247       0.00      0.00      0.00         3\n",
      "         248       0.00      0.00      0.00         3\n",
      "         249       0.00      0.00      0.00         3\n",
      "         250       0.00      0.00      0.00         2\n",
      "         251       0.00      0.00      0.00         5\n",
      "         252       0.00      0.00      0.00         3\n",
      "         253       0.00      0.00      0.00         4\n",
      "         254       0.00      0.00      0.00         5\n",
      "         255       0.00      0.00      0.00         3\n",
      "         256       0.00      0.00      0.00         3\n",
      "         257       0.00      0.00      0.00         4\n",
      "         258       0.00      0.00      0.00         2\n",
      "         259       0.00      0.00      0.00         1\n",
      "         260       0.00      0.00      0.00         3\n",
      "         261       0.00      0.00      0.00         2\n",
      "         263       0.00      0.00      0.00         2\n",
      "         264       0.00      0.00      0.00         1\n",
      "         265       0.00      0.00      0.00         1\n",
      "         266       0.00      0.00      0.00         3\n",
      "         267       0.00      0.00      0.00         5\n",
      "         268       0.00      0.00      0.00         3\n",
      "         269       0.00      0.00      0.00         3\n",
      "         270       0.00      0.00      0.00         2\n",
      "         271       0.00      0.00      0.00         3\n",
      "         272       0.00      0.00      0.00         1\n",
      "         273       0.00      0.00      0.00         1\n",
      "         274       0.00      0.00      0.00         3\n",
      "         275       0.00      0.00      0.00         3\n",
      "         276       0.00      0.00      0.00         2\n",
      "         278       0.00      0.00      0.00         1\n",
      "         280       0.00      0.00      0.00         2\n",
      "         282       0.00      0.00      0.00         4\n",
      "         283       0.00      0.00      0.00         1\n",
      "         284       0.00      0.00      0.00         2\n",
      "         285       0.00      0.00      0.00         1\n",
      "         287       0.00      0.00      0.00         2\n",
      "         288       0.00      0.00      0.00         1\n",
      "         290       0.00      0.00      0.00         1\n",
      "         291       0.00      0.00      0.00         1\n",
      "         295       0.00      0.00      0.00         2\n",
      "         297       0.00      0.00      0.00         3\n",
      "         301       0.00      0.00      0.00         1\n",
      "         302       0.00      0.00      0.00         1\n",
      "         303       0.00      0.00      0.00         2\n",
      "         304       0.00      0.00      0.00         1\n",
      "         306       0.00      0.00      0.00         2\n",
      "         309       0.00      0.00      0.00         1\n",
      "         312       0.00      0.00      0.00         2\n",
      "         314       0.00      0.00      0.00         1\n",
      "         316       0.00      0.00      0.00         1\n",
      "         317       0.00      0.00      0.00         1\n",
      "         318       0.00      0.00      0.00         1\n",
      "         321       0.00      0.00      0.00         1\n",
      "         323       0.00      0.00      0.00         1\n",
      "         324       0.00      0.00      0.00         1\n",
      "         325       0.00      0.00      0.00         1\n",
      "         331       0.00      0.00      0.00         2\n",
      "         332       0.00      0.00      0.00         0\n",
      "         333       0.00      0.00      0.00         1\n",
      "         334       0.00      0.00      0.00         1\n",
      "         335       0.00      0.00      0.00         2\n",
      "         338       0.00      0.00      0.00         0\n",
      "         340       0.00      0.00      0.00         0\n",
      "         344       0.00      0.00      0.00         1\n",
      "         347       0.00      0.00      0.00         1\n",
      "         348       0.00      0.00      0.00         1\n",
      "         354       0.00      0.00      0.00         1\n",
      "         357       0.00      0.00      0.00         1\n",
      "         358       0.00      0.00      0.00         1\n",
      "         359       0.00      0.00      0.00         1\n",
      "         362       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.01      4127\n",
      "   macro avg       0.00      0.01      0.00      4127\n",
      "weighted avg       0.01      0.01      0.01      4127\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niklasquendt/anaconda3/envs/bda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/niklasquendt/anaconda3/envs/bda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/niklasquendt/anaconda3/envs/bda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/niklasquendt/anaconda3/envs/bda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/niklasquendt/anaconda3/envs/bda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/niklasquendt/anaconda3/envs/bda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine implementieren\n",
    "svm = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Cross-Validation\n",
    "svm_cv_scores = cross_val_score(svm, X_train_scaled, y_train, cv=5)\n",
    "print(f\"SVC Cross-Validation Accuracy: {svm_cv_scores.mean():.4f} ± {svm_cv_scores.std():.4f}\")\n",
    "\n",
    "# Training\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Vorhersagen und Bewerten des SVM Classifiers auf den Validierungsdaten\n",
    "svm_predictions = svm.predict(X_val_scaled)\n",
    "print(\"SVC Training Accuracy:\", accuracy_score(y_val, svm_predictions))\n",
    "print(\"SVC Training Classification Report:\\n\", classification_report(y_val, svm_predictions))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = svm.predict(X_test_scaled)\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot((prediction[:2000]), label=\"Prediction\")\n",
    "plt.plot((test_RUL_data_1[:2000]), label=\"Reale RUL\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [100, 20631]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sqrt\n\u001b[0;32m----> 3\u001b[0m rms \u001b[38;5;241m=\u001b[39m sqrt(\u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_RUL_data_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe RMSE auf Trainingsdaten ist :\u001b[39m\u001b[38;5;124m\"\u001b[39m, rms)\n",
      "File \u001b[0;32m~/anaconda3/envs/bda/lib/python3.8/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/bda/lib/python3.8/site-packages/sklearn/metrics/_regression.py:474\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m    405\u001b[0m     {\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    415\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, multioutput\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    416\u001b[0m ):\n\u001b[1;32m    417\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m    0.825...\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 474\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    478\u001b[0m     output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/anaconda3/envs/bda/lib/python3.8/site-packages/sklearn/metrics/_regression.py:99\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_reg_targets\u001b[39m(y_true, y_pred, multioutput, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     66\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m        correct keyword.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    101\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/anaconda3/envs/bda/lib/python3.8/site-packages/sklearn/utils/validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    405\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    410\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [100, 20631]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "rms = sqrt(mean_squared_error(x.values.reshape(-1), prediction))\n",
    "print(\"The RMSE auf Trainingsdaten ist :\", rms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Process Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m gaussian_process \u001b[38;5;241m=\u001b[39m GaussianProcessRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#Cross Validation\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m gp_cv_scores \u001b[38;5;241m=\u001b[39m cross_val_score(gaussian_process, \u001b[43mX_train_scaled\u001b[49m, y_train, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGaussian Process Regressor Cross-Validation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgp_cv_scores\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ± \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgp_cv_scores\u001b[38;5;241m.\u001b[39mstd()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Gaussian Process Regressor trainieren\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "gaussian_process = GaussianProcessRegressor(random_state=6)\n",
    "\n",
    "#Cross Validation\n",
    "gp_cv_scores = cross_val_score(gaussian_process, X_train_scaled, y_train, cv=5)\n",
    "print(f\"Gaussian Process Regressor Cross-Validation Accuracy: {gp_cv_scores.mean():.4f} ± {gp_cv_scores.std():.4f}\")\n",
    "\n",
    "# Gaussian Process Regressor trainieren\n",
    "gaussian_process.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Vorhersagen und Bewerten des Gaussian Process Regressors\n",
    "gp_predictions = gaussian_process.predict(X_val_scaled)\n",
    "gp_predictions_rounded = np.round(gp_predictions)\n",
    "gp_predictions_rounded = np.clip(gp_predictions_rounded, 0, None)\n",
    "print(\"Gaussian Process Regressor Accuracy:\", accuracy_score(y_val, gp_predictions_rounded))\n",
    "print(\"Gaussian Process Regressor Classification Report:\\n\", classification_report(y_val, gp_predictions_rounded))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niklasquendt/anaconda3/envs/bda/lib/python3.8/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/niklasquendt/anaconda3/envs/bda/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/niklasquendt/anaconda3/envs/bda/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/niklasquendt/anaconda3/envs/bda/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/niklasquendt/anaconda3/envs/bda/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/niklasquendt/anaconda3/envs/bda/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier Cross-Validation Accuracy: 0.0069 ± 0.0019\n",
      "MLP Classifier Accuracy: 0.052445349231738644\n",
      "MLP Classifier Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.07      0.04      0.05       100\n",
      "           2       0.03      0.06      0.04       100\n",
      "           3       0.04      0.11      0.06       100\n",
      "           4       0.04      0.05      0.04       100\n",
      "           5       0.05      0.02      0.03       100\n",
      "           6       0.01      0.01      0.01       100\n",
      "           7       0.07      0.06      0.07       100\n",
      "           8       0.12      0.07      0.09       100\n",
      "           9       0.04      0.04      0.04       100\n",
      "          10       0.16      0.05      0.08       100\n",
      "          11       0.03      0.03      0.03       100\n",
      "          12       0.09      0.03      0.04       100\n",
      "          13       0.05      0.01      0.02       100\n",
      "          14       0.04      0.02      0.03       100\n",
      "          15       0.05      0.15      0.07       100\n",
      "          16       0.04      0.11      0.06       100\n",
      "          17       0.04      0.12      0.06       100\n",
      "          18       0.03      0.13      0.05       100\n",
      "          19       0.07      0.03      0.04       100\n",
      "          20       0.05      0.10      0.06       100\n",
      "          21       0.02      0.02      0.02       100\n",
      "          22       0.04      0.03      0.03       100\n",
      "          23       0.01      0.01      0.01       100\n",
      "          24       0.04      0.11      0.05       100\n",
      "          25       0.06      0.01      0.02       100\n",
      "          26       0.05      0.04      0.04       100\n",
      "          27       0.05      0.03      0.04       100\n",
      "          28       0.03      0.02      0.02       100\n",
      "          29       0.00      0.00      0.00       100\n",
      "          30       0.04      0.07      0.05       100\n",
      "          31       0.05      0.20      0.08       100\n",
      "          32       0.07      0.03      0.04       100\n",
      "          33       0.00      0.00      0.00       100\n",
      "          34       0.04      0.04      0.04       100\n",
      "          35       0.05      0.05      0.05       100\n",
      "          36       0.02      0.03      0.02       100\n",
      "          37       0.05      0.05      0.05       100\n",
      "          38       0.04      0.06      0.05       100\n",
      "          39       0.05      0.04      0.05       100\n",
      "          40       0.03      0.02      0.02       100\n",
      "          41       0.03      0.04      0.04       100\n",
      "          42       0.02      0.01      0.01       100\n",
      "          43       0.04      0.02      0.03       100\n",
      "          44       0.05      0.08      0.06       100\n",
      "          45       0.03      0.08      0.04       100\n",
      "          46       0.04      0.06      0.05       100\n",
      "          47       0.04      0.06      0.05       100\n",
      "          48       0.08      0.03      0.04       100\n",
      "          49       0.02      0.01      0.01       100\n",
      "          50       0.05      0.05      0.05       100\n",
      "          51       0.03      0.10      0.04       100\n",
      "          52       0.00      0.00      0.00       100\n",
      "          53       0.06      0.04      0.05       100\n",
      "          54       0.02      0.01      0.01       100\n",
      "          55       0.04      0.03      0.04       100\n",
      "          56       0.06      0.08      0.07       100\n",
      "          57       0.04      0.08      0.06       100\n",
      "          58       0.07      0.02      0.03       100\n",
      "          59       0.04      0.07      0.05       100\n",
      "          60       0.02      0.01      0.01       100\n",
      "          61       0.04      0.02      0.03       100\n",
      "          62       0.07      0.05      0.06       100\n",
      "          63       0.10      0.01      0.02       100\n",
      "          64       0.05      0.03      0.04       100\n",
      "          65       0.07      0.05      0.06       100\n",
      "          66       0.05      0.01      0.02       100\n",
      "          67       0.03      0.04      0.03       100\n",
      "          68       0.01      0.04      0.02       100\n",
      "          69       0.03      0.05      0.04       100\n",
      "          70       0.08      0.04      0.05       100\n",
      "          71       0.04      0.05      0.05       100\n",
      "          72       0.00      0.00      0.00       100\n",
      "          73       0.02      0.03      0.02       100\n",
      "          74       0.04      0.02      0.03       100\n",
      "          75       0.06      0.03      0.04       100\n",
      "          76       0.04      0.04      0.04       100\n",
      "          77       0.04      0.04      0.04       100\n",
      "          78       0.08      0.02      0.03       100\n",
      "          79       0.03      0.02      0.02       100\n",
      "          80       0.03      0.02      0.02       100\n",
      "          81       0.06      0.02      0.03       100\n",
      "          82       0.03      0.01      0.02       100\n",
      "          83       0.04      0.03      0.03       100\n",
      "          84       0.05      0.06      0.05       100\n",
      "          85       0.02      0.01      0.01       100\n",
      "          86       0.04      0.08      0.05       100\n",
      "          87       0.04      0.01      0.02       100\n",
      "          88       0.00      0.00      0.00       100\n",
      "          89       0.03      0.05      0.04       100\n",
      "          90       0.03      0.03      0.03       100\n",
      "          91       0.00      0.00      0.00       100\n",
      "          92       0.04      0.03      0.03       100\n",
      "          93       0.02      0.03      0.02       100\n",
      "          94       0.01      0.01      0.01       100\n",
      "          95       0.04      0.02      0.03       100\n",
      "          96       0.08      0.07      0.07       100\n",
      "          97       0.05      0.03      0.04       100\n",
      "          98       0.03      0.04      0.03       100\n",
      "          99       0.00      0.00      0.00       100\n",
      "         100       0.06      0.05      0.05       100\n",
      "         101       0.02      0.05      0.03       100\n",
      "         102       0.03      0.01      0.02       100\n",
      "         103       0.00      0.00      0.00       100\n",
      "         104       0.03      0.03      0.03       100\n",
      "         105       0.05      0.15      0.08       100\n",
      "         106       0.03      0.02      0.02       100\n",
      "         107       0.04      0.04      0.04       100\n",
      "         108       0.06      0.07      0.07       100\n",
      "         109       0.06      0.04      0.05       100\n",
      "         110       0.05      0.04      0.04       100\n",
      "         111       0.00      0.00      0.00       100\n",
      "         112       0.05      0.04      0.05       100\n",
      "         113       0.08      0.04      0.05       100\n",
      "         114       0.02      0.03      0.03       100\n",
      "         115       0.07      0.06      0.06       100\n",
      "         116       0.02      0.04      0.03       100\n",
      "         117       0.04      0.06      0.05       100\n",
      "         118       0.04      0.01      0.02       100\n",
      "         119       0.05      0.07      0.06       100\n",
      "         120       0.06      0.05      0.06       100\n",
      "         121       0.02      0.01      0.01       100\n",
      "         122       0.08      0.07      0.08       100\n",
      "         123       0.09      0.12      0.10       100\n",
      "         124       0.05      0.03      0.04       100\n",
      "         125       0.05      0.07      0.06       100\n",
      "         126       0.05      0.08      0.06       100\n",
      "         127       0.04      0.08      0.06       100\n",
      "         128       0.02      0.01      0.01       100\n",
      "         129       0.05      0.18      0.08        99\n",
      "         130       0.03      0.01      0.02        99\n",
      "         131       0.05      0.05      0.05        99\n",
      "         132       0.06      0.04      0.05        99\n",
      "         133       0.04      0.08      0.05        99\n",
      "         134       0.05      0.04      0.04        99\n",
      "         135       0.05      0.09      0.06        99\n",
      "         136       0.09      0.07      0.08        98\n",
      "         137       0.05      0.03      0.04        98\n",
      "         138       0.05      0.06      0.06        96\n",
      "         139       0.04      0.08      0.05        96\n",
      "         140       0.07      0.04      0.05        96\n",
      "         141       0.07      0.03      0.04        96\n",
      "         142       0.05      0.03      0.04        96\n",
      "         143       0.03      0.02      0.03        96\n",
      "         144       0.04      0.04      0.04        96\n",
      "         145       0.03      0.02      0.02        96\n",
      "         146       0.09      0.03      0.05        96\n",
      "         147       0.04      0.03      0.03        96\n",
      "         148       0.06      0.02      0.03        94\n",
      "         149       0.05      0.05      0.05        94\n",
      "         150       0.06      0.02      0.03        94\n",
      "         151       0.04      0.05      0.05        93\n",
      "         152       0.07      0.06      0.07        93\n",
      "         153       0.06      0.05      0.06        93\n",
      "         154       0.09      0.02      0.03        92\n",
      "         155       0.06      0.07      0.06        90\n",
      "         156       0.07      0.07      0.07        89\n",
      "         157       0.04      0.05      0.05        87\n",
      "         158       0.00      0.00      0.00        87\n",
      "         159       0.05      0.11      0.07        84\n",
      "         160       0.02      0.01      0.02        84\n",
      "         161       0.07      0.05      0.06        84\n",
      "         162       0.08      0.01      0.02        84\n",
      "         163       0.17      0.01      0.02        84\n",
      "         164       0.04      0.05      0.05        82\n",
      "         165       0.07      0.02      0.04        82\n",
      "         166       0.06      0.09      0.07        81\n",
      "         167       0.05      0.05      0.05        80\n",
      "         168       0.04      0.01      0.02        80\n",
      "         169       0.08      0.01      0.02        79\n",
      "         170       0.00      0.00      0.00        79\n",
      "         171       0.20      0.01      0.02        77\n",
      "         172       0.08      0.05      0.06        77\n",
      "         173       0.06      0.08      0.07        76\n",
      "         174       0.00      0.00      0.00        76\n",
      "         175       0.05      0.07      0.06        75\n",
      "         176       0.14      0.04      0.06        75\n",
      "         177       0.06      0.08      0.07        75\n",
      "         178       0.02      0.03      0.02        75\n",
      "         179       0.00      0.00      0.00        74\n",
      "         180       0.09      0.05      0.07        73\n",
      "         181       0.00      0.00      0.00        71\n",
      "         182       0.08      0.06      0.07        70\n",
      "         183       0.08      0.04      0.06        70\n",
      "         184       0.00      0.00      0.00        70\n",
      "         185       0.03      0.04      0.04        70\n",
      "         186       0.00      0.00      0.00        67\n",
      "         187       0.03      0.01      0.02        67\n",
      "         188       0.09      0.01      0.03        67\n",
      "         189       0.14      0.19      0.16        64\n",
      "         190       0.05      0.10      0.07        63\n",
      "         191       0.13      0.08      0.10        63\n",
      "         192       0.04      0.05      0.05        62\n",
      "         193       0.10      0.05      0.07        60\n",
      "         194       0.12      0.05      0.07        59\n",
      "         195       0.05      0.04      0.04        57\n",
      "         196       0.00      0.00      0.00        53\n",
      "         197       0.14      0.02      0.03        52\n",
      "         198       0.00      0.00      0.00        52\n",
      "         199       0.00      0.00      0.00        51\n",
      "         200       0.05      0.08      0.06        48\n",
      "         201       0.00      0.00      0.00        46\n",
      "         202       0.13      0.09      0.11        45\n",
      "         203       0.08      0.05      0.06        42\n",
      "         204       0.00      0.00      0.00        42\n",
      "         205       0.00      0.00      0.00        42\n",
      "         206       0.07      0.10      0.08        42\n",
      "         207       0.11      0.12      0.11        42\n",
      "         208       0.00      0.00      0.00        40\n",
      "         209       0.00      0.00      0.00        39\n",
      "         210       0.00      0.00      0.00        38\n",
      "         211       0.05      0.03      0.04        37\n",
      "         212       0.00      0.00      0.00        37\n",
      "         213       0.00      0.00      0.00        37\n",
      "         214       0.05      0.03      0.04        32\n",
      "         215       0.25      0.03      0.06        30\n",
      "         216       0.06      0.03      0.04        29\n",
      "         217       0.07      0.04      0.05        28\n",
      "         218       0.00      0.00      0.00        27\n",
      "         219       0.00      0.00      0.00        27\n",
      "         220       0.08      0.04      0.05        27\n",
      "         221       0.09      0.04      0.05        27\n",
      "         222       0.00      0.00      0.00        27\n",
      "         223       0.00      0.00      0.00        26\n",
      "         224       0.08      0.04      0.05        26\n",
      "         225       0.00      0.00      0.00        26\n",
      "         226       0.00      0.00      0.00        26\n",
      "         227       0.00      0.00      0.00        26\n",
      "         228       0.08      0.27      0.12        26\n",
      "         229       0.00      0.00      0.00        26\n",
      "         230       0.25      0.04      0.07        25\n",
      "         231       0.07      0.04      0.05        24\n",
      "         232       0.07      0.10      0.08        21\n",
      "         233       0.00      0.00      0.00        21\n",
      "         234       0.25      0.05      0.08        21\n",
      "         235       0.00      0.00      0.00        19\n",
      "         236       0.00      0.00      0.00        19\n",
      "         237       0.09      0.16      0.12        19\n",
      "         238       0.08      0.05      0.06        19\n",
      "         239       0.15      0.37      0.22        19\n",
      "         240       0.14      0.05      0.08        19\n",
      "         241       0.12      0.12      0.12        17\n",
      "         242       0.14      0.18      0.15        17\n",
      "         243       0.09      0.12      0.10        17\n",
      "         244       0.00      0.00      0.00        17\n",
      "         245       0.09      0.06      0.07        17\n",
      "         246       0.15      0.12      0.13        17\n",
      "         247       0.12      0.06      0.08        17\n",
      "         248       0.11      0.06      0.08        17\n",
      "         249       0.12      0.18      0.15        17\n",
      "         250       0.17      0.24      0.20        17\n",
      "         251       0.05      0.06      0.05        17\n",
      "         252       0.17      0.06      0.09        17\n",
      "         253       0.12      0.06      0.08        17\n",
      "         254       0.07      0.18      0.10        17\n",
      "         255       0.00      0.00      0.00        17\n",
      "         256       0.33      0.06      0.10        17\n",
      "         257       0.06      0.06      0.06        16\n",
      "         258       0.50      0.13      0.21        15\n",
      "         259       0.08      0.43      0.13        14\n",
      "         260       0.07      0.31      0.11        13\n",
      "         261       0.10      0.08      0.09        13\n",
      "         262       0.00      0.00      0.00        13\n",
      "         263       0.00      0.00      0.00        13\n",
      "         264       0.00      0.00      0.00        13\n",
      "         265       0.17      0.08      0.11        13\n",
      "         266       0.05      0.31      0.09        13\n",
      "         267       0.12      0.23      0.16        13\n",
      "         268       0.25      0.08      0.12        12\n",
      "         269       0.33      0.25      0.29        12\n",
      "         270       0.07      0.09      0.08        11\n",
      "         271       0.15      0.18      0.17        11\n",
      "         272       0.00      0.00      0.00        11\n",
      "         273       0.12      0.18      0.14        11\n",
      "         274       0.20      0.18      0.19        11\n",
      "         275       0.15      0.18      0.17        11\n",
      "         276       0.19      0.30      0.23        10\n",
      "         277       0.09      0.33      0.15         9\n",
      "         278       0.05      0.33      0.08         9\n",
      "         279       0.09      0.62      0.16         8\n",
      "         280       0.12      0.25      0.17         8\n",
      "         281       0.05      0.12      0.07         8\n",
      "         282       0.21      0.50      0.30         8\n",
      "         283       0.18      0.38      0.24         8\n",
      "         284       0.15      0.67      0.25         6\n",
      "         285       0.00      0.00      0.00         6\n",
      "         286       0.06      0.50      0.11         6\n",
      "         287       0.22      0.33      0.27         6\n",
      "         288       0.13      0.60      0.21         5\n",
      "         289       0.11      0.40      0.17         5\n",
      "         290       0.50      0.20      0.29         5\n",
      "         291       0.60      0.60      0.60         5\n",
      "         292       0.27      0.80      0.40         5\n",
      "         293       0.20      0.60      0.30         5\n",
      "         294       0.30      0.75      0.43         4\n",
      "         295       0.20      0.75      0.32         4\n",
      "         296       0.14      0.50      0.22         4\n",
      "         297       0.50      1.00      0.67         4\n",
      "         298       0.38      0.75      0.50         4\n",
      "         299       0.18      1.00      0.31         4\n",
      "         300       0.14      0.25      0.18         4\n",
      "         301       0.27      0.75      0.40         4\n",
      "         302       0.19      0.75      0.30         4\n",
      "         303       0.25      0.25      0.25         4\n",
      "         304       0.14      0.50      0.22         4\n",
      "         305       0.25      1.00      0.40         4\n",
      "         306       0.08      0.25      0.12         4\n",
      "         307       0.00      0.00      0.00         4\n",
      "         308       0.43      0.75      0.55         4\n",
      "         309       0.22      0.50      0.31         4\n",
      "         310       0.14      0.50      0.22         4\n",
      "         311       0.40      0.50      0.44         4\n",
      "         312       0.18      0.50      0.27         4\n",
      "         313       0.20      0.50      0.29         4\n",
      "         314       0.33      1.00      0.50         3\n",
      "         315       0.38      1.00      0.55         3\n",
      "         316       0.20      0.67      0.31         3\n",
      "         317       0.17      0.67      0.27         3\n",
      "         318       0.00      0.00      0.00         3\n",
      "         319       0.00      0.00      0.00         3\n",
      "         320       0.20      0.67      0.31         3\n",
      "         321       0.67      0.67      0.67         3\n",
      "         322       0.25      0.67      0.36         3\n",
      "         323       0.12      0.33      0.18         3\n",
      "         324       0.12      0.67      0.21         3\n",
      "         325       0.20      0.33      0.25         3\n",
      "         326       0.50      1.00      0.67         3\n",
      "         327       0.40      0.67      0.50         3\n",
      "         328       0.40      0.67      0.50         3\n",
      "         329       0.00      0.00      0.00         3\n",
      "         330       0.22      0.67      0.33         3\n",
      "         331       0.25      0.67      0.36         3\n",
      "         332       0.19      1.00      0.32         3\n",
      "         333       0.50      0.33      0.40         3\n",
      "         334       0.40      0.67      0.50         3\n",
      "         335       0.14      0.33      0.20         3\n",
      "         336       0.43      1.00      0.60         3\n",
      "         337       0.50      1.00      0.67         2\n",
      "         338       0.40      1.00      0.57         2\n",
      "         339       0.29      1.00      0.44         2\n",
      "         340       0.40      1.00      0.57         2\n",
      "         341       0.33      1.00      0.50         2\n",
      "         342       1.00      1.00      1.00         1\n",
      "         343       1.00      1.00      1.00         1\n",
      "         344       1.00      1.00      1.00         1\n",
      "         345       0.50      1.00      0.67         1\n",
      "         346       1.00      1.00      1.00         1\n",
      "         347       0.33      1.00      0.50         1\n",
      "         348       0.50      1.00      0.67         1\n",
      "         349       0.25      1.00      0.40         1\n",
      "         350       0.50      1.00      0.67         1\n",
      "         351       1.00      1.00      1.00         1\n",
      "         352       0.50      1.00      0.67         1\n",
      "         353       0.25      1.00      0.40         1\n",
      "         354       1.00      1.00      1.00         1\n",
      "         355       0.50      1.00      0.67         1\n",
      "         356       0.50      1.00      0.67         1\n",
      "         357       0.50      1.00      0.67         1\n",
      "         358       1.00      1.00      1.00         1\n",
      "         359       0.25      1.00      0.40         1\n",
      "         360       0.25      1.00      0.40         1\n",
      "         361       1.00      1.00      1.00         1\n",
      "         362       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.05     20631\n",
      "   macro avg       0.13      0.20      0.14     20631\n",
      "weighted avg       0.05      0.05      0.04     20631\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niklasquendt/anaconda3/envs/bda/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/Users/niklasquendt/anaconda3/envs/bda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/niklasquendt/anaconda3/envs/bda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/niklasquendt/anaconda3/envs/bda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# MLPClassifier implementieren\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)\n",
    "\n",
    "# Cross-Validation\n",
    "mlp_cv_scores = cross_val_score(mlp, X_train_scaled, y_train, cv=5)\n",
    "print(f\"MLPClassifier Cross-Validation Accuracy: {mlp_cv_scores.mean():.4f} ± {mlp_cv_scores.std():.4f}\")\n",
    "\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Vorhersagen und Bewerten des MLPClassifiers\n",
    "mlp_predictions = mlp.predict(X_val_scaled)\n",
    "print(\"MLP Classifier Accuracy:\", accuracy_score(y_val, mlp_predictions))\n",
    "print(\"MLP Classifier Classification Report:\\n\", classification_report(y_val, mlp_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random Forest implementieren\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Cross-Validation\n",
    "rf_cv_scores = cross_val_score(rf, X_train_scaled, y_train, cv=5)\n",
    "print(f\"RandomForest Cross-Validation Accuracy: {rf_cv_scores.mean():.4f} ± {rf_cv_scores.std():.4f}\")\n",
    "\n",
    "# Random Forest Classifier trainieren\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Vorhersagen und Bewerten des Random Forest Classifiers\n",
    "rf_predictions = rf.predict(X_val_scaled)\n",
    "print(\"Random Forest Classifier Accuracy:\", accuracy_score(y_val, rf_predictions))\n",
    "print(\"Random Forest Classifier Classification Report:\\n\", classification_report(y_val, rf_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# AdaBoost implementieren\n",
    "ada = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Cross-Validation\n",
    "ada_cv_scores = cross_val_score(ada, X_train_scaled, y_train, cv=5)\n",
    "print(f\"AdaBoost Cross-Validation Accuracy: {ada_cv_scores.mean():.4f} ± {ada_cv_scores.std():.4f}\")\n",
    "\n",
    "# AdaBoost Classifier trainieren\n",
    "ada.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Vorhersagen und Bewerten des AdaBoost Classifiers\n",
    "ada_predictions = ada.predict(X_val_scaled)\n",
    "print(\"AdaBoost Classifier Accuracy:\", accuracy_score(y_val, ada_predictions))\n",
    "print(\"AdaBoost Classifier Classification Report:\\n\", classification_report(y_val, ada_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-Nearest Neighbors implementieren\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter tuning\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
