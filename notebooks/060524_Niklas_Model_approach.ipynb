{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:37.900316500Z",
     "start_time": "2024-05-09T15:31:37.830560800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:37.980322700Z",
     "start_time": "2024-05-09T15:31:37.900316500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Topic: EX2 - Turbofan RUL Prediction\n",
    "**Task**: Predict the remaining useful life (RUL) of turbofan engines based on given sensor data (time series data). It is a forcasting problem, where the goal is to predict the number of cycles an engine will last before it fails.\n",
    "**Data**: Turbofan engine degradation simulation data (NASA) - [Link](https://data.nasa.gov/dataset/Turbofan-Engine-Degradation-Simulation-Data-Set/vrks-gjie). See also in the topic [introduction notebook](https://github.com/nina-prog/damage-propagation-modeling/blob/2fb8c1a1102a48d7abbf04e4031807790a913a99/notebooks/Turbofan%20remaining%20useful%20life%20Prediction.ipynb).\n",
    "\n",
    "**Subtasks**:\n",
    "1. Perform a deep **exploratory data analysis (EDA)** on the given data.\n",
    "2. Implement a more efficient **sliding window method** for time series data analysis. -> üéØ **Focus on this task**\n",
    "3. Apply **traditional machine learning methods** (SOTA) to predict the remaining useful life. Includes data preparation, feature extraction, feature selection, model selection, and model parameter optimization.\n",
    "4. Create **neural network models** to predict the remaining useful life. Includes different architectures like Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), or Attention Models. Note: You can search for SOTA research papers and reproduce current state-of-the-art models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports + Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:38.193478100Z",
     "start_time": "2024-05-09T15:31:37.980322700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# third-party libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_circles, make_classification, make_moons\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "# previous\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from sklearn.svm import SVC\n",
    "import sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:38.304026600Z",
     "start_time": "2024-05-09T15:31:38.200491900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/niklasquendt/Documents/Uni/PSDA/UÃàbung 2/damage-propagation-modeling\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/Users/niklasquendt/Documents/Uni/PSDA/UÃàbung 2/damage-propagation-modeling')\n",
    "# Zeigt das aktuelle Arbeitsverzeichnis an\n",
    "print(os.getcwd())\n",
    "# Setzt das Arbeitsverzeichnis auf das Projektverzeichnis\n",
    "\n",
    "#from src.utils import flatten\n",
    "\n",
    "from src.data_loading import load_data, load_config\n",
    "from src.data_cleaning import clean_data, format_dtype\n",
    "from src.rolling_window_creator import calculate_RUL, RollingWindowDatasetCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:38.400456500Z",
     "start_time": "2024-05-09T15:31:38.304026600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"Set2\")\n",
    "sns.set(rc={\"figure.dpi\":100, 'savefig.dpi':200})\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:38.492590100Z",
     "start_time": "2024-05-09T15:31:38.400456500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/niklasquendt/Documents/Uni/PSDA/UÃàbung 2/damage-propagation-modeling'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure to execute this cell only once for one kernel session, before running any other cell below.\n",
    "os.chdir(\"/Users/niklasquendt/Documents/Uni/PSDA/UÃàbung 2/damage-propagation-modeling\") # set working directory to root of project\n",
    "os.getcwd() # check current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:38.652221700Z",
     "start_time": "2024-05-09T15:31:38.560578800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PATH_TO_CONFIG = \"configs/config.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load Config + Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:38.732034900Z",
     "start_time": "2024-05-09T15:31:38.652221700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = load_config(PATH_TO_CONFIG) # config is dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:39.154853100Z",
     "start_time": "2024-05-09T15:31:38.740553100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-26 20:15:33 [\u001b[34msrc.data_loading:43\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loading data set 1...\u001b[0m\n",
      "2024-05-26 20:15:33 [\u001b[34msrc.data_loading:72\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loaded raw data for dataset 1.\u001b[0m\n",
      "2024-05-26 20:15:33 [\u001b[34msrc.data_loading:73\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Train Data: (20631, 26)\u001b[0m\n",
      "2024-05-26 20:15:33 [\u001b[34msrc.data_loading:74\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test Data: (13096, 26)\u001b[0m\n",
      "2024-05-26 20:15:33 [\u001b[34msrc.data_loading:75\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test RUL Data: (100, 1)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_data_1, test_data_1, test_RUL_data_1 = load_data(config_path=PATH_TO_CONFIG, dataset_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-26 20:15:36 [\u001b[34msrc.data_loading:43\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loading data set 2...\u001b[0m\n",
      "2024-05-26 20:15:36 [\u001b[34msrc.data_loading:72\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loaded raw data for dataset 2.\u001b[0m\n",
      "2024-05-26 20:15:36 [\u001b[34msrc.data_loading:73\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Train Data: (53759, 26)\u001b[0m\n",
      "2024-05-26 20:15:36 [\u001b[34msrc.data_loading:74\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test Data: (33991, 26)\u001b[0m\n",
      "2024-05-26 20:15:36 [\u001b[34msrc.data_loading:75\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test RUL Data: (259, 1)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_data_2, test_data_2, test_RUL_data_2 = load_data(config_path=PATH_TO_CONFIG, dataset_num=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-26 20:15:38 [\u001b[34msrc.data_loading:43\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loading data set 3...\u001b[0m\n",
      "2024-05-26 20:15:38 [\u001b[34msrc.data_loading:72\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loaded raw data for dataset 3.\u001b[0m\n",
      "2024-05-26 20:15:38 [\u001b[34msrc.data_loading:73\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Train Data: (24720, 26)\u001b[0m\n",
      "2024-05-26 20:15:38 [\u001b[34msrc.data_loading:74\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test Data: (16596, 26)\u001b[0m\n",
      "2024-05-26 20:15:38 [\u001b[34msrc.data_loading:75\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test RUL Data: (100, 1)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_data_3, test_data_3, test_RUL_data_3 = load_data(config_path=PATH_TO_CONFIG, dataset_num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-26 20:15:39 [\u001b[34msrc.data_loading:43\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loading data set 4...\u001b[0m\n",
      "2024-05-26 20:15:39 [\u001b[34msrc.data_loading:72\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loaded raw data for dataset 4.\u001b[0m\n",
      "2024-05-26 20:15:39 [\u001b[34msrc.data_loading:73\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Train Data: (61249, 26)\u001b[0m\n",
      "2024-05-26 20:15:39 [\u001b[34msrc.data_loading:74\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test Data: (41214, 26)\u001b[0m\n",
      "2024-05-26 20:15:39 [\u001b[34msrc.data_loading:75\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test RUL Data: (248, 1)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_data_4, test_data_4, test_RUL_data_4 = load_data(config_path=PATH_TO_CONFIG, dataset_num=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [train_data_1, train_data_2, train_data_3, train_data_4]\n",
    "test_data = [test_data_1, test_data_2, test_data_3, test_data_4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# üìç << Models >>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "[TEMPLATE]\n",
    "\n",
    "Findings:\n",
    "* Interpretation of plots\n",
    "* or other key take aways from previous code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:39.220343800Z",
     "start_time": "2024-05-09T15:31:39.160435700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# [TEMPLATE] - save processed data (as pickle)\n",
    "df = pd.DataFrame()\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "df.to_pickle(f\"{config['paths']['processed_data_dir']}ex2_topic_{timestamp}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:39.318667400Z",
     "start_time": "2024-05-09T15:31:39.220343800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# [TEMPLATE] - save data predictions (as csv)\n",
    "df = pd.DataFrame()\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "df.to_csv(f\"{config['paths']['prediction_dir']}ex2_topic_{timestamp}.csv\", sep=',', decimal='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:39.485116500Z",
     "start_time": "2024-05-09T15:31:39.320350800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 900x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [TEMPLATE] - save plot results (as png)\n",
    "fig = plt.figure(figsize=(9, 6))\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "fig.savefig(f\"{config['paths']['plot_dir']}ex2_topic_{timestamp}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-26 20:15:47 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
      "2024-05-26 20:15:47 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
      "2024-05-26 20:15:47 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
      "2024-05-26 20:15:47 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_data_1 = format_dtype(train_data_1)\n",
    "train_data_2 = format_dtype(train_data_2)\n",
    "train_data_3 = format_dtype(train_data_3)\n",
    "train_data_4 = format_dtype(train_data_4)\n",
    "train_data = [train_data_1, train_data_2, train_data_3, train_data_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:134\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Cleaning train and test data...\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:136\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Formatting column types...\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:141\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Handling duplicates...\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:146\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Removing outliers...\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:150\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Filter features based train data...\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:26\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 features with missing values above the threshold of 0.1.\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:46\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 features with only a single unique value: []\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:103\u001b[0m] [DEBUG\u001b[0m] >>>> Found 1 uncorrelated features with a correlation threshold of 0.3: ['UnitNumber']\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Dropping features based on missing values, single unique values, and no target correlation...\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:172\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Data cleaning completed.\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:173\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original train DataFrame shape: (20631, 16), Resulting train DataFrame shape: (20631, 16)\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:174\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original test DataFrame shape: (13096, 16), Resulting test DataFrame shape: (13096, 16)\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:134\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Cleaning train and test data...\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:136\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Formatting column types...\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:141\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Handling duplicates...\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:146\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Removing outliers...\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:150\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Filter features based train data...\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:26\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 features with missing values above the threshold of 0.1.\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:46\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 features with only a single unique value: []\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:103\u001b[0m] [DEBUG\u001b[0m] >>>> Found 1 uncorrelated features with a correlation threshold of 0.3: ['UnitNumber']\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Dropping features based on missing values, single unique values, and no target correlation...\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:172\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Data cleaning completed.\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:173\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original train DataFrame shape: (53759, 2), Resulting train DataFrame shape: (53759, 2)\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:174\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original test DataFrame shape: (33991, 2), Resulting test DataFrame shape: (33991, 2)\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:134\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Cleaning train and test data...\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:136\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Formatting column types...\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:141\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Handling duplicates...\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:146\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Removing outliers...\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:150\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Filter features based train data...\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:26\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 features with missing values above the threshold of 0.1.\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:46\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 features with only a single unique value: []\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:103\u001b[0m] [DEBUG\u001b[0m] >>>> Found 1 uncorrelated features with a correlation threshold of 0.3: ['UnitNumber']\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Dropping features based on missing values, single unique values, and no target correlation...\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:172\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Data cleaning completed.\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:173\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original train DataFrame shape: (24720, 14), Resulting train DataFrame shape: (24720, 14)\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:174\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original test DataFrame shape: (16596, 14), Resulting test DataFrame shape: (16596, 14)\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:134\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Cleaning train and test data...\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:136\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Formatting column types...\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:141\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Handling duplicates...\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:146\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Removing outliers...\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:150\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Filter features based train data...\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:26\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 features with missing values above the threshold of 0.1.\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:46\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 features with only a single unique value: []\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:103\u001b[0m] [DEBUG\u001b[0m] >>>> Found 1 uncorrelated features with a correlation threshold of 0.3: ['UnitNumber']\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Dropping features based on missing values, single unique values, and no target correlation...\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:172\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Data cleaning completed.\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:173\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original train DataFrame shape: (61249, 2), Resulting train DataFrame shape: (61249, 2)\u001b[0m\n",
      "2024-05-26 22:27:24 [\u001b[34msrc.data_cleaning:174\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original test DataFrame shape: (41214, 2), Resulting test DataFrame shape: (41214, 2)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "cleaned_train_1, cleaned_test_1 = clean_data(train_data_1, test_data_1, method=None, ignore_columns=['UnitNumber', 'Cycle'], threshold_missing=0.1, threshold_corr=0.3)\n",
    "cleaned_train_2, cleaned_test_2 = clean_data(train_data_2, test_data_2, method=None, ignore_columns=['UnitNumber', 'Cycle'], threshold_missing=0.1, threshold_corr=0.3)\n",
    "cleaned_train_3, cleaned_test_3 = clean_data(train_data_3, test_data_3, method=None, ignore_columns=['UnitNumber', 'Cycle'], threshold_missing=0.1, threshold_corr=0.3)\n",
    "cleaned_train_4, cleaned_test_4 = clean_data(train_data_4, test_data_4, method=None, ignore_columns=['UnitNumber', 'Cycle'], threshold_missing=0.1, threshold_corr=0.3)\n",
    "\n",
    "cleaned_train = [cleaned_train_1, cleaned_train_2, cleaned_train_3, cleaned_train_4]\n",
    "cleaned_test = [cleaned_test_1, cleaned_test_2, cleaned_test_3, cleaned_test_4]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20631, 16)\n",
      "Index(['UnitNumber', 'Cycle', 'Sensor Measure 2', 'Sensor Measure 3',\n",
      "       'Sensor Measure 4', 'Sensor Measure 7', 'Sensor Measure 8',\n",
      "       'Sensor Measure 9', 'Sensor Measure 11', 'Sensor Measure 12',\n",
      "       'Sensor Measure 13', 'Sensor Measure 14', 'Sensor Measure 15',\n",
      "       'Sensor Measure 17', 'Sensor Measure 20', 'Sensor Measure 21'],\n",
      "      dtype='object')\n",
      "(53759, 2)\n",
      "Index(['UnitNumber', 'Cycle'], dtype='object')\n",
      "(24720, 14)\n",
      "Index(['UnitNumber', 'Cycle', 'Sensor Measure 2', 'Sensor Measure 3',\n",
      "       'Sensor Measure 4', 'Sensor Measure 7', 'Sensor Measure 8',\n",
      "       'Sensor Measure 9', 'Sensor Measure 10', 'Sensor Measure 11',\n",
      "       'Sensor Measure 12', 'Sensor Measure 13', 'Sensor Measure 14',\n",
      "       'Sensor Measure 17'],\n",
      "      dtype='object')\n",
      "(61249, 2)\n",
      "Index(['UnitNumber', 'Cycle'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_train_1.shape)\n",
    "print(cleaned_test_1.columns)\n",
    "print(cleaned_train_2.shape)\n",
    "print(cleaned_test_2.columns)\n",
    "print(cleaned_train_3.shape)\n",
    "print(cleaned_test_3.columns)\n",
    "print(cleaned_train_4.shape)\n",
    "print(cleaned_test_4.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/niklasquendt/Documents/Uni/PSDA/UÃàbung 2/damage-propagation-modeling/data/processed/feature_list.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# feature_list for dataset 1\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# TODO: create function to make variable for each dataset to ease optimization\u001b[39;00m\n\u001b[1;32m      6\u001b[0m currentpath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetcwd()\n\u001b[0;32m----> 7\u001b[0m ft_list \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrentpath\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/data/processed/feature_list.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/pickle.py:185\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/niklasquendt/Documents/Uni/PSDA/UÃàbung 2/damage-propagation-modeling/data/processed/feature_list.pkl'"
     ]
    }
   ],
   "source": [
    "# Currently using minimal to ease optimization so feature_list is not necessary\n",
    "feature_list = []\n",
    "\n",
    "# feature_list for dataset 1\n",
    "# TODO: create function to make variable for each dataset to ease optimization\n",
    "currentpath = os.getcwd()\n",
    "ft_list = pd.read_pickle(currentpath+ \"/data/processed/feature_list.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_ts = 5\n",
    "max_ts = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "rwCreator = RollingWindowDatasetCreator(max_timeshift=max_ts,min_timeshift=min_ts,feature_extraction_mode= 'minimal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-26 22:28:31 [\u001b[34msrc.rolling_window_creator:117\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Creating rolling windows for train data...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:03<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-26 22:28:34 [\u001b[34msrc.rolling_window_creator:123\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Extracting features for train data...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:19<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-26 22:28:56 [\u001b[34msrc.rolling_window_creator:131\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Calculating target for train data...\u001b[0m\n",
      "2024-05-26 22:28:56 [\u001b[34msrc.rolling_window_creator:117\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Creating rolling windows for test data...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:01<00:00, 11.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-26 22:28:58 [\u001b[34msrc.rolling_window_creator:123\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Extracting features for test data...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Extraction: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 21.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-26 22:28:59 [\u001b[34msrc.rolling_window_creator:159\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Datasets created successfully.\u001b[0m\n",
      "2024-05-26 22:28:59 [\u001b[34msrc.rolling_window_creator:160\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Shape of X_train: (20131, 140)\u001b[0m\n",
      "2024-05-26 22:28:59 [\u001b[34msrc.rolling_window_creator:161\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Shape of y_train: (20131, 1)\u001b[0m\n",
      "2024-05-26 22:28:59 [\u001b[34msrc.rolling_window_creator:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Shape of X_test: (100, 140)\u001b[0m\n",
      "2024-05-26 22:28:59 [\u001b[34msrc.rolling_window_creator:163\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Shape of y_test: (100, 1)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_1, y_train_1, X_test_1, y_test_1 = rwCreator.create_rolling_windows_datasets(train_data=cleaned_train_1, test_data=cleaned_test_1,test_RUL_data=test_RUL_data_1,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-26 22:29:04 [\u001b[34msrc.rolling_window_creator:117\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Creating rolling windows for train data...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:03<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-26 22:29:08 [\u001b[34msrc.rolling_window_creator:123\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Extracting features for train data...\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not guess the value column! Please hand it to the function as an argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train_2, y_train_2, X_test_2, y_test_2 \u001b[38;5;241m=\u001b[39m \u001b[43mrwCreator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_rolling_windows_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcleaned_train_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcleaned_test_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_RUL_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_RUL_data_2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Uni/PSDA/UÃàbung 2/damage-propagation-modeling/src/rolling_window_creator.py:156\u001b[0m, in \u001b[0;36mRollingWindowDatasetCreator.create_rolling_windows_datasets\u001b[0;34m(self, train_data, test_data, test_RUL_data)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create the rolling windows datasets.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m:param train_data: The training dataset.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m:rtype: tuple\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_window_sizes(train_data, test_data)\n\u001b[0;32m--> 156\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(test_data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, test_RUL_data)\n\u001b[1;32m    159\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasets created successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Uni/PSDA/UÃàbung 2/damage-propagation-modeling/src/rolling_window_creator.py:124\u001b[0m, in \u001b[0;36mRollingWindowDatasetCreator._process_data\u001b[0;34m(self, data, data_type, test_RUL_data)\u001b[0m\n\u001b[1;32m    121\u001b[0m     rolled_data \u001b[38;5;241m=\u001b[39m rolled_data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumn_id)\u001b[38;5;241m.\u001b[39mtail(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_timeshift)\n\u001b[1;32m    123\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting features for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 124\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrolled_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mcolumn_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_sort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_sort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mdefault_fc_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_fc_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mimpute_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimpute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_warnings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m X\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mrename([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumn_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumn_sort])\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tsfresh/feature_extraction/extraction.py:164\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(timeseries_container, default_fc_parameters, kind_to_fc_parameters, column_id, column_sort, column_kind, column_value, chunksize, n_jobs, show_warnings, disable_progressbar, impute_function, profile, profiling_filename, profiling_sorting, distributor, pivot)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 164\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43m_do_extraction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeseries_container\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_kind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_kind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_sort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_sort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_warnings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_warnings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_fc_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_fc_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkind_to_fc_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind_to_fc_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistributor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistributor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpivot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpivot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# Impute the result if requested\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m impute_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tsfresh/feature_extraction/extraction.py:260\u001b[0m, in \u001b[0;36m_do_extraction\u001b[0;34m(df, column_id, column_value, column_kind, column_sort, default_fc_parameters, kind_to_fc_parameters, n_jobs, chunk_size, disable_progressbar, show_warnings, distributor, pivot)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_extraction\u001b[39m(\n\u001b[1;32m    194\u001b[0m     df,\n\u001b[1;32m    195\u001b[0m     column_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m     pivot,\n\u001b[1;32m    207\u001b[0m ):\n\u001b[1;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    Wrapper around the _do_extraction_on_chunk, which calls it on all chunks in the data frame.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m    A chunk is a subset of the data, with a given kind and id - so a single time series.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m    :rtype: pd.DataFrame\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mto_tsdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_kind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_sort\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m distributor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Iterable):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tsfresh/feature_extraction/data.py:478\u001b[0m, in \u001b[0;36mto_tsdata\u001b[0;34m(df, column_id, column_kind, column_value, column_sort)\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m WideTsFrameAdapter(df, column_id, column_sort, [column_value])\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 478\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mWideTsFrameAdapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_sort\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(df, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TsDictAdapter(df, column_id, column_value, column_sort)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tsfresh/feature_extraction/data.py:184\u001b[0m, in \u001b[0;36mWideTsFrameAdapter.__init__\u001b[0;34m(self, df, column_id, column_sort, value_columns)\u001b[0m\n\u001b[1;32m    181\u001b[0m _check_nan(df, column_id)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value_columns:\n\u001b[0;32m--> 184\u001b[0m     value_columns \u001b[38;5;241m=\u001b[39m \u001b[43m_get_value_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_sort\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m _check_nan(df, \u001b[38;5;241m*\u001b[39mvalue_columns)\n\u001b[1;32m    187\u001b[0m _check_colname(\u001b[38;5;241m*\u001b[39mvalue_columns)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tsfresh/feature_extraction/data.py:152\u001b[0m, in \u001b[0;36m_get_value_columns\u001b[0;34m(df, *other_columns)\u001b[0m\n\u001b[1;32m    149\u001b[0m value_columns \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m other_columns]\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value_columns) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not guess the value column! Please hand it to the function as an argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m     )\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value_columns\n",
      "\u001b[0;31mValueError\u001b[0m: Could not guess the value column! Please hand it to the function as an argument."
     ]
    }
   ],
   "source": [
    "X_train_2, y_train_2, X_test_2, y_test_2 = rwCreator.create_rolling_windows_datasets(train_data=cleaned_train_2, test_data=cleaned_test_2,test_RUL_data=test_RUL_data_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-26 21:16:58 [\u001b[34msrc.rolling_window_creator:117\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Creating rolling windows for train data...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:03<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-26 21:17:01 [\u001b[34msrc.rolling_window_creator:123\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Extracting features for train data...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:21<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-26 21:17:25 [\u001b[34msrc.rolling_window_creator:131\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Calculating target for train data...\u001b[0m\n",
      "2024-05-26 21:17:25 [\u001b[34msrc.rolling_window_creator:117\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Creating rolling windows for test data...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  7.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-26 21:17:28 [\u001b[34msrc.rolling_window_creator:123\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Extracting features for test data...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 20.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-26 21:17:29 [\u001b[34msrc.rolling_window_creator:159\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Datasets created successfully.\u001b[0m\n",
      "2024-05-26 21:17:29 [\u001b[34msrc.rolling_window_creator:160\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Shape of X_train: (24220, 120)\u001b[0m\n",
      "2024-05-26 21:17:29 [\u001b[34msrc.rolling_window_creator:161\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Shape of y_train: (24220, 1)\u001b[0m\n",
      "2024-05-26 21:17:29 [\u001b[34msrc.rolling_window_creator:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Shape of X_test: (100, 120)\u001b[0m\n",
      "2024-05-26 21:17:29 [\u001b[34msrc.rolling_window_creator:163\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Shape of y_test: (100, 1)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_3, y_train_3, X_test_3, y_test_3 = rwCreator.create_rolling_windows_datasets(train_data=cleaned_train_3, test_data=cleaned_train_3,test_RUL_data=test_RUL_data_3,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-26 22:15:39 [\u001b[34msrc.rolling_window_creator:117\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Creating rolling windows for train data...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-26 22:15:44 [\u001b[34msrc.rolling_window_creator:123\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Extracting features for train data...\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not guess the value column! Please hand it to the function as an argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train_4, y_train_4, X_test_4, y_test_4 \u001b[38;5;241m=\u001b[39m \u001b[43mrwCreator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_rolling_windows_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcleaned_train_4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcleaned_train_4\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_RUL_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_RUL_data_4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Uni/PSDA/UÃàbung 2/damage-propagation-modeling/src/rolling_window_creator.py:156\u001b[0m, in \u001b[0;36mRollingWindowDatasetCreator.create_rolling_windows_datasets\u001b[0;34m(self, train_data, test_data, test_RUL_data)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create the rolling windows datasets.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m:param train_data: The training dataset.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m:rtype: tuple\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_window_sizes(train_data, test_data)\n\u001b[0;32m--> 156\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(test_data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, test_RUL_data)\n\u001b[1;32m    159\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasets created successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Uni/PSDA/UÃàbung 2/damage-propagation-modeling/src/rolling_window_creator.py:124\u001b[0m, in \u001b[0;36mRollingWindowDatasetCreator._process_data\u001b[0;34m(self, data, data_type, test_RUL_data)\u001b[0m\n\u001b[1;32m    121\u001b[0m     rolled_data \u001b[38;5;241m=\u001b[39m rolled_data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumn_id)\u001b[38;5;241m.\u001b[39mtail(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_timeshift)\n\u001b[1;32m    123\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting features for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 124\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrolled_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mcolumn_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_sort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_sort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mdefault_fc_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_fc_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mimpute_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimpute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_warnings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m X\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mrename([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumn_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumn_sort])\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tsfresh/feature_extraction/extraction.py:164\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(timeseries_container, default_fc_parameters, kind_to_fc_parameters, column_id, column_sort, column_kind, column_value, chunksize, n_jobs, show_warnings, disable_progressbar, impute_function, profile, profiling_filename, profiling_sorting, distributor, pivot)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 164\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43m_do_extraction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeseries_container\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_kind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_kind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_sort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_sort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_warnings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_warnings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_fc_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_fc_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkind_to_fc_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind_to_fc_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistributor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistributor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpivot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpivot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# Impute the result if requested\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m impute_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tsfresh/feature_extraction/extraction.py:260\u001b[0m, in \u001b[0;36m_do_extraction\u001b[0;34m(df, column_id, column_value, column_kind, column_sort, default_fc_parameters, kind_to_fc_parameters, n_jobs, chunk_size, disable_progressbar, show_warnings, distributor, pivot)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_extraction\u001b[39m(\n\u001b[1;32m    194\u001b[0m     df,\n\u001b[1;32m    195\u001b[0m     column_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m     pivot,\n\u001b[1;32m    207\u001b[0m ):\n\u001b[1;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    Wrapper around the _do_extraction_on_chunk, which calls it on all chunks in the data frame.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m    A chunk is a subset of the data, with a given kind and id - so a single time series.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m    :rtype: pd.DataFrame\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mto_tsdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_kind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_sort\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m distributor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Iterable):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tsfresh/feature_extraction/data.py:478\u001b[0m, in \u001b[0;36mto_tsdata\u001b[0;34m(df, column_id, column_kind, column_value, column_sort)\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m WideTsFrameAdapter(df, column_id, column_sort, [column_value])\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 478\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mWideTsFrameAdapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_sort\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(df, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TsDictAdapter(df, column_id, column_value, column_sort)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tsfresh/feature_extraction/data.py:184\u001b[0m, in \u001b[0;36mWideTsFrameAdapter.__init__\u001b[0;34m(self, df, column_id, column_sort, value_columns)\u001b[0m\n\u001b[1;32m    181\u001b[0m _check_nan(df, column_id)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value_columns:\n\u001b[0;32m--> 184\u001b[0m     value_columns \u001b[38;5;241m=\u001b[39m \u001b[43m_get_value_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_sort\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m _check_nan(df, \u001b[38;5;241m*\u001b[39mvalue_columns)\n\u001b[1;32m    187\u001b[0m _check_colname(\u001b[38;5;241m*\u001b[39mvalue_columns)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tsfresh/feature_extraction/data.py:152\u001b[0m, in \u001b[0;36m_get_value_columns\u001b[0;34m(df, *other_columns)\u001b[0m\n\u001b[1;32m    149\u001b[0m value_columns \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m other_columns]\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value_columns) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not guess the value column! Please hand it to the function as an argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m     )\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value_columns\n",
      "\u001b[0;31mValueError\u001b[0m: Could not guess the value column! Please hand it to the function as an argument."
     ]
    }
   ],
   "source": [
    "X_train_4, y_train_4, X_test_4, y_test_4 = rwCreator.create_rolling_windows_datasets(train_data=cleaned_train_4, test_data=cleaned_train_4,test_RUL_data=test_RUL_data_4,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:39.522027500Z",
     "start_time": "2024-05-09T15:31:39.474058600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from sklearn.svm import SVC\n",
    "#from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score,f1_score, root_mean_squared_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "klassische Ans√§tze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardisieren der Daten\n",
    "scaler_1 = StandardScaler()\n",
    "X_train_scaled_1 = scaler_1.fit_transform(X_train_1)\n",
    "#X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled_1 = scaler_1.transform(X_test_1)\n",
    "\n",
    "scaler_3 = StandardScaler()\n",
    "X_train_scaled_3 = scaler_3.fit_transform(X_train_3)\n",
    "#X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled_3 = scaler_3.transform(X_test_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.06544539353214\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 120 features, but SVC is expecting 140 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m svm_predictions_1 \u001b[38;5;241m=\u001b[39m svm\u001b[38;5;241m.\u001b[39mpredict(X_test_scaled_1)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(sklearn\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mroot_mean_squared_error(y_test_1, svm_predictions_1))\n\u001b[0;32m---> 17\u001b[0m svm_predictions_3 \u001b[38;5;241m=\u001b[39m \u001b[43msvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_scaled_3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(sklearn\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mroot_mean_squared_error(y_test_3, svm_predictions_3))\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#print(\"SVC Training Accuracy:\", accuracy_score(y_test, svm_predictions))\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#print(\"SVC Training Classification Report:\\n\", classification_report(y_test, svm_predictions))\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#print(\"=\"*60)\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/svm/_base.py:814\u001b[0m, in \u001b[0;36mBaseSVC.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    812\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 814\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39masarray(y, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/svm/_base.py:429\u001b[0m, in \u001b[0;36mBaseLibSVM.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    414\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform regression on samples in X.\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \n\u001b[1;32m    416\u001b[0m \u001b[38;5;124;03m    For an one-class model, +1 (inlier) or -1 (outlier) is returned.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;124;03m        The predicted values.\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_for_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m     predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse_predict \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dense_predict\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predict(X)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/svm/_base.py:607\u001b[0m, in \u001b[0;36mBaseLibSVM._validate_for_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    604\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel):\n\u001b[0;32m--> 607\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sp\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m    617\u001b[0m     X \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mcsr_matrix(X)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:654\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 654\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:443\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    446\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: X has 120 features, but SVC is expecting 140 features as input."
     ]
    }
   ],
   "source": [
    "# Support Vector Machine implementieren\n",
    "svm_1 = SVC(kernel='linear', random_state=42)\n",
    "svm_3 = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Cross-Validation\n",
    "#svm_cv_scores = cross_val_score(svm, X_train_scaled, y_train, cv=5)\n",
    "#print(f\"SVC Cross-Validation Accuracy: {svm_cv_scores.mean():.4f} ¬± {svm_cv_scores.std():.4f}\")\n",
    "\n",
    "# Training\n",
    "svm_1.fit(X_train_scaled_1, y_train_1)\n",
    "svm_3.fit(X_train_scaled_3, y_train_3)\n",
    "\n",
    "# Vorhersagen und Bewerten des SVM Classifiers auf den Validierungsdaten\n",
    "svm_predictions_1 = svm.predict(X_test_scaled_1)\n",
    "print(sklearn.metrics.root_mean_squared_error(y_test_1, svm_predictions_1))\n",
    "\n",
    "svm_predictions_3 = svm.predict(X_test_scaled_3)\n",
    "print(sklearn.metrics.root_mean_squared_error(y_test_3, svm_predictions_3))\n",
    "\n",
    "#print(\"SVC Training Accuracy:\", accuracy_score(y_test, svm_predictions))\n",
    "#print(\"SVC Training Classification Report:\\n\", classification_report(y_test, svm_predictions))\n",
    "#print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Process Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.17455298332204\n",
      "85.90521822355312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ngp_predictions_rounded = np.round(gp_predictions)\\ngp_predictions_rounded = np.clip(gp_predictions_rounded, 0, None)\\nprint(\"Gaussian Process Regressor Accuracy:\", accuracy_score(y_val, gp_predictions_rounded))\\nprint(\"Gaussian Process Regressor Classification Report:\\n\", classification_report(y_val, gp_predictions_rounded))'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "gaussian_process_1 = GaussianProcessRegressor(random_state=6)\n",
    "gaussian_process_3 = GaussianProcessRegressor(random_state=6)\n",
    "\n",
    "\n",
    "#Cross Validation\n",
    "#gp_cv_scores = cross_val_score(gaussian_process, X_train_scaled, y_train, cv=5)\n",
    "#print(f\"Gaussian Process Regressor Cross-Validation Accuracy: {gp_cv_scores.mean():.4f} ¬± {gp_cv_scores.std():.4f}\")\n",
    "\n",
    "# Gaussian Process Regressor trainieren\n",
    "gaussian_process_1.fit(X_train_scaled_1, y_train_1)\n",
    "gaussian_process_3.fit(X_train_scaled_3, y_train_3)\n",
    "\n",
    "# Vorhersagen und Bewerten des Gaussian Process Regressors\n",
    "gp_predictions_1 = gaussian_process_1.predict(X_test_scaled_1)\n",
    "print(sklearn.metrics.root_mean_squared_error(y_test_1, gp_predictions_1))\n",
    "\n",
    "gp_predictions_3 = gaussian_process_3.predict(X_test_scaled_3)\n",
    "print(sklearn.metrics.root_mean_squared_error(y_test_3, gp_predictions_3))\n",
    "\n",
    "\"\"\"\n",
    "gp_predictions_rounded = np.round(gp_predictions)\n",
    "gp_predictions_rounded = np.clip(gp_predictions_rounded, 0, None)\n",
    "print(\"Gaussian Process Regressor Accuracy:\", accuracy_score(y_val, gp_predictions_rounded))\n",
    "print(\"Gaussian Process Regressor Classification Report:\\n\", classification_report(y_val, gp_predictions_rounded))\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1101: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1101: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.32350203783247\n",
      "85.07091159732568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# MLPClassifier implementieren\n",
    "mlp_1 = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)\n",
    "mlp_3 = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)\n",
    "\n",
    "# Cross-Validation\n",
    "#mlp_cv_scores = cross_val_score(mlp, X_train_scaled, y_train, cv=5)\n",
    "#print(f\"MLPClassifier Cross-Validation Accuracy: {mlp_cv_scores.mean():.4f} ¬± {mlp_cv_scores.std():.4f}\")\n",
    "\n",
    "mlp_1.fit(X_train_scaled_1, y_train_1)\n",
    "mlp_3.fit(X_train_scaled_3, y_train_3)\n",
    "\n",
    "# Vorhersagen und Bewerten des MLPClassifiers\n",
    "mlp_predictions_1 = mlp_1.predict(X_test_scaled_1)\n",
    "print(sklearn.metrics.root_mean_squared_error(y_test_1, mlp_predictions_1))\n",
    "\n",
    "mlp_predictions_3 = mlp_3.predict(X_test_scaled_3)\n",
    "print(sklearn.metrics.root_mean_squared_error(y_test_3, mlp_predictions_3))\n",
    "\n",
    "#print(\"MLP Classifier Accuracy:\", accuracy_score(y_val, mlp_predictions))\n",
    "#print(\"MLP Classifier Classification Report:\\n\", classification_report(y_val, mlp_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.18227515158303\n",
      "hello\n",
      "85.0174099817208\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random Forest implementieren\n",
    "rf_1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_3 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Cross-Validation\n",
    "#rf_cv_scores = cross_val_score(rf, X_train_scaled, y_train, cv=5)\n",
    "#print(f\"RandomForest Cross-Validation Accuracy: {rf_cv_scores.mean():.4f} ¬± {rf_cv_scores.std():.4f}\")\n",
    "\n",
    "# Random Forest Classifier trainieren\n",
    "rf_1.fit(X_train_scaled_1, y_train_1)\n",
    "rf_3.fit(X_train_scaled_3, y_train_3)\n",
    "\n",
    "# Vorhersagen und Bewerten des Random Forest Classifiers\n",
    "rf_predictions_1 = rf_1.predict(X_test_scaled_1)\n",
    "print(sklearn.metrics.root_mean_squared_error(y_test_1, rf_predictions_1))\n",
    "print(\"hello\")\n",
    "rf_predictions_3 = rf_3.predict(X_test_scaled_3)\n",
    "print(sklearn.metrics.root_mean_squared_error(y_test_3, rf_predictions_3))\n",
    "#print(\"Random Forest Classifier Accuracy:\", accuracy_score(y_val, rf_predictions))\n",
    "#print(\"Random Forest Classifier Classification Report:\\n\", classification_report(y_val, rf_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.71626200451759\n",
      "82.46296138267168\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# AdaBoost implementieren\n",
    "ada_1 = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "ada_3 = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Cross-Validation\n",
    "#ada_cv_scores = cross_val_score(ada, X_train_scaled, y_train, cv=5)\n",
    "#print(f\"AdaBoost Cross-Validation Accuracy: {ada_cv_scores.mean():.4f} ¬± {ada_cv_scores.std():.4f}\")\n",
    "\n",
    "# AdaBoost Classifier trainieren\n",
    "ada_1.fit(X_train_scaled_1, y_train_1)\n",
    "ada_3.fit(X_train_scaled_3, y_train_3)\n",
    "\n",
    "# Vorhersagen und Bewerten des AdaBoost Classifiers\n",
    "ada_predictions_1 = ada_1.predict(X_test_scaled_1)\n",
    "print(sklearn.metrics.root_mean_squared_error(y_test_1, ada_predictions_1))\n",
    "\n",
    "ada_predictions_3 = ada_3.predict(X_test_scaled_3)\n",
    "print(sklearn.metrics.root_mean_squared_error(y_test_3, ada_predictions_3))\n",
    "\n",
    "#print(\"AdaBoost Classifier Accuracy:\", accuracy_score(y_val, ada_predictions))\n",
    "#print(\"AdaBoost Classifier Classification Report:\\n\", classification_report(y_val, ada_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neighbors/_classification.py:238: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/neighbors/_classification.py:238: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.04463533933225\n",
      "84.84244220907364\n"
     ]
    }
   ],
   "source": [
    "# k-Nearest Neighbors implementieren\n",
    "knn_1 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_3 = KNeighborsClassifier(n_neighbors=5)\n",
    "#trainieren des k-Nearest Neighbors\n",
    "knn_1.fit(X_train_scaled_1, y_train_1)\n",
    "knn_3.fit(X_train_scaled_3, y_train_3)\n",
    "\n",
    "#vohersagen und Bewerten des k-Nearest Neighbors\n",
    "knn_predictions_1 = knn_1.predict(X_test_scaled_1)\n",
    "print(sklearn.metrics.root_mean_squared_error(y_test_1, knn_predictions_1))\n",
    "\n",
    "knn_predictions_3 = knn_3.predict(X_test_scaled_3)\n",
    "print(sklearn.metrics.root_mean_squared_error(y_test_3, knn_predictions_3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter tuning\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
