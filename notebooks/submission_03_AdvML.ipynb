{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T07:24:30.179033Z",
     "start_time": "2024-06-01T07:24:29.822562Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T07:24:30.475593Z",
     "start_time": "2024-06-01T07:24:30.182132Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Topic: EX2 - Turbofan RUL Prediction\n",
    "**Task**: Predict the remaining useful life (RUL) of turbofan engines based on given sensor data (time series data). It is a forcasting problem, where the goal is to predict the number of cycles an engine will last before it fails.\n",
    "**Data**: Turbofan engine degradation simulation data (NASA) - [Link](https://data.nasa.gov/dataset/Turbofan-Engine-Degradation-Simulation-Data-Set/vrks-gjie). See also in the topic [introduction notebook](https://github.com/nina-prog/damage-propagation-modeling/blob/2fb8c1a1102a48d7abbf04e4031807790a913a99/notebooks/Turbofan%20remaining%20useful%20life%20Prediction.ipynb).\n",
    "\n",
    "**Subtasks**:\n",
    "1. Perform a deep **exploratory data analysis (EDA)** on the given data.\n",
    "2. Implement a more efficient **sliding window method** for time series data analysis. -> üéØ **Focus on this task**\n",
    "3. Apply **traditional machine learning methods** (SOTA) to predict the remaining useful life. Includes data preparation, feature extraction, feature selection, model selection, and model parameter optimization.\n",
    "4. Create **neural network models** to predict the remaining useful life. Includes different architectures like Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), or Attention Models. Note: You can search for SOTA research papers and reproduce current state-of-the-art models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports + Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T07:24:36.606678Z",
     "start_time": "2024-06-01T07:24:30.478640Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# third-party libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import torch \n",
    "\n",
    "import time\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T10:30:15.422245Z",
     "start_time": "2024-06-01T10:30:14.786034Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# source code\n",
    "from src.data_loading import load_data, load_config\n",
    "from src.data_splitting import train_val_split_by_group\n",
    "from src.nn_utils import create_sliding_window, create_sliding_window_test\n",
    "from src.rolling_window_creator import calculate_RUL\n",
    "from src.data_processing import apply_padding_on_train_data_and_test_data, drop_samples_with_clipped_values, extract_peaks_from_sensor_signal\n",
    "from src.nn_util.nn_models.ligthning.cnnModel1 import CNNModel1 as CNNModel\n",
    "from src.nn_util.datamodule.lightning.turbofanDatamodule import TurbofanDatamodule\n",
    "from src.data_cleaning import clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T07:24:38.419907Z",
     "start_time": "2024-06-01T07:24:38.140434Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"Set2\")\n",
    "sns.set(rc={\"figure.dpi\":100, 'savefig.dpi':200})\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T07:24:38.707157Z",
     "start_time": "2024-06-01T07:24:38.421963Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T07:24:38.989181Z",
     "start_time": "2024-06-01T07:24:38.708452Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Johannes\\\\PycharmProjects\\\\damage-propagation-modeling'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure to execute this cell only once for one kernel session, before running any other cell below.\n",
    "os.chdir(\"../\") # set working directory to root of project\n",
    "os.getcwd() # check current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T07:24:39.254858Z",
     "start_time": "2024-06-01T07:24:38.989181Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PATH_TO_CONFIG = \"configs/config.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load Config + Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T07:24:39.535343Z",
     "start_time": "2024-06-01T07:24:39.256863Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = load_config(PATH_TO_CONFIG) # config is dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T10:41:49.162744Z",
     "start_time": "2024-06-01T10:41:47.880440Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-01 12:41:48 [\u001B[34msrc.data_loading:43\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Loading data set 2...\u001B[0m\n",
      "2024-06-01 12:41:49 [\u001B[34msrc.data_loading:72\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Loaded raw data for dataset 2.\u001B[0m\n",
      "2024-06-01 12:41:49 [\u001B[34msrc.data_loading:73\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Train Data: (53759, 26)\u001B[0m\n",
      "2024-06-01 12:41:49 [\u001B[34msrc.data_loading:74\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Test Data: (33991, 26)\u001B[0m\n",
      "2024-06-01 12:41:49 [\u001B[34msrc.data_loading:75\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Test RUL Data: (259, 1)\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "dataset_num = 2\n",
    "train_data, test_data, test_RUL_data = load_data(config_path=PATH_TO_CONFIG, dataset_num=dataset_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Create Neural Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline:\n",
    "1.\tData Cleaning\n",
    "2.\tOptional: Padding\n",
    "3.\tCreate sliding windows\n",
    "4.\tSplit train data in validation and train data\n",
    "5.\tDrop some samples with the clipped value\n",
    "6.\tScale the Data\n",
    "7.\tFind the best hyperparameters\n",
    "8.\tCreate Model with found hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of selected hyperparameters:\n",
    "*\tWindow size: We selected a window size of 30 due to some experiments with other window sizes. Furthermore, the window size is also used in the paper from Mitici [1] which shows good results with a CNN architecture.\n",
    "*\tClipping value: The clipping value of 125 has been selected because it has proven useful and is used in paper [1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "1.\tMihaela Mitici, Ingeborg de Pater, Anne Barros, Zhiguo Zeng, ‚ÄúDynamic predictive maintenance for multiple components using data-driven probabilistic RUL prognostics: The case of turbofan engines‚Äù, Reliability Engineering & System Safety, Volume 234, 2023, https://doi.org/10.1016/j.ress.2023.109199.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T10:41:49.945244Z",
     "start_time": "2024-06-01T10:41:49.420233Z"
    }
   },
   "outputs": [],
   "source": [
    "# some hyperparameters\n",
    "time_column = 'Cycle'\n",
    "group_column = 'UnitNumber'\n",
    "\n",
    "window_size = 30\n",
    "clip_value = 125\n",
    "test_size = 0.1\n",
    "apply_data_cleaning = True\n",
    "# If activated, adds for every sensor a new column with the commutative sum of the peaks\n",
    "apply_peaks_generation = False\n",
    "\n",
    "# Apply scaler. The order in the list represents the order in which they are applied\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "robust_scaler = RobustScaler()\n",
    "scaler = [std_scaler, minmax_scaler, robust_scaler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of each step:\n",
    "- Optional:  Extraction of peaks\n",
    "    - Adds for each sensor measure a column with the sum over all the peaks from the first cycle till the current cycle\n",
    "    - This additional feature made no significant difference, so it is deactivated \n",
    "- Data Cleaning\n",
    "    - The outlier detection and replacement method has been deactivated. \n",
    "    - The removal of columns based on the correlation of a single value has been deactivated because the neural model makes the feature selection.\n",
    "    - Features with a unique single value will be removed\n",
    "- Padding:\n",
    "    -\tOnly applied for the datasets with a sample in test or train data smaller than the window size.\n",
    "    -\tThe padding length is exactly the difference between the window size and the timesteps of the sample with the fewest timesteps\n",
    "    -\tThe padding is applied on all the time series\n",
    "- Create sliding window\n",
    "    -\tThe sliding window approach for the NN techniques differs from the previous approach.\n",
    "        Now, we do not have any aggregation but we keep the data as it is in windows so that the NN model can extract its own features\n",
    "- Split train data in validation and train data\n",
    "    -\tSplitting training and validation sets based on the UnitNumber\n",
    "- Drop some samples with the clipped value:\n",
    "    -\tTo make the data more evenly distributed, in this step some of the samples with the clipping value as RUL are removed \n",
    "    -\tTherefore, the median of the frequency of other RUL values is computed and the number of samples with the clipping value is a multiple of the median. \n",
    "    -\tWe selected two to not drop too many samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T10:42:10.138317Z",
     "start_time": "2024-06-01T10:41:49.948062Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-01 12:41:54 [\u001B[34msrc.data_cleaning:134\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Cleaning train and test data...\u001B[0m\n",
      "2024-06-01 12:41:54 [\u001B[34msrc.data_cleaning:136\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Formatting column types...\u001B[0m\n",
      "2024-06-01 12:41:54 [\u001B[34msrc.data_cleaning:69\u001B[0m] [DEBUG\u001B[0m] >>>> Found 0 categorical columns: []\u001B[0m\n",
      "2024-06-01 12:41:54 [\u001B[34msrc.data_cleaning:69\u001B[0m] [DEBUG\u001B[0m] >>>> Found 0 categorical columns: []\u001B[0m\n",
      "2024-06-01 12:41:54 [\u001B[34msrc.data_cleaning:141\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Handling duplicates...\u001B[0m\n",
      "2024-06-01 12:41:54 [\u001B[34msrc.data_cleaning:146\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Removing outliers...\u001B[0m\n",
      "2024-06-01 12:41:54 [\u001B[34msrc.outlier_detection:150\u001B[0m] [DEBUG\u001B[0m] >>>> Removing outliers using method: None ...\u001B[0m\n",
      "2024-06-01 12:41:54 [\u001B[34msrc.outlier_detection:162\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001B[0m\n",
      "2024-06-01 12:41:54 [\u001B[34msrc.outlier_detection:150\u001B[0m] [DEBUG\u001B[0m] >>>> Removing outliers using method: None ...\u001B[0m\n",
      "2024-06-01 12:41:54 [\u001B[34msrc.outlier_detection:162\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001B[0m\n",
      "2024-06-01 12:41:54 [\u001B[34msrc.data_cleaning:150\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Filter features based train data...\u001B[0m\n",
      "2024-06-01 12:41:54 [\u001B[34msrc.data_cleaning:26\u001B[0m] [DEBUG\u001B[0m] >>>> Found 0 features with missing values above the threshold of 0.1.\u001B[0m\n",
      "2024-06-01 12:41:54 [\u001B[34msrc.data_cleaning:46\u001B[0m] [DEBUG\u001B[0m] >>>> Found 0 features with only a single unique value: []\u001B[0m\n",
      "2024-06-01 12:41:54 [\u001B[34msrc.data_cleaning:103\u001B[0m] [DEBUG\u001B[0m] >>>> Found 0 uncorrelated features with a correlation threshold of 0.0: []\u001B[0m\n",
      "2024-06-01 12:41:54 [\u001B[34msrc.data_cleaning:162\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Dropping features based on missing values, single unique values, and no target correlation...\u001B[0m\n",
      "2024-06-01 12:41:54 [\u001B[34msrc.data_cleaning:172\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Data cleaning completed.\u001B[0m\n",
      "2024-06-01 12:41:54 [\u001B[34msrc.data_cleaning:173\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Original train DataFrame shape: (53759, 47), Resulting train DataFrame shape: (53759, 47)\u001B[0m\n",
      "2024-06-01 12:41:54 [\u001B[34msrc.data_cleaning:174\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Original test DataFrame shape: (33991, 47), Resulting test DataFrame shape: (33991, 47)\u001B[0m\n",
      "2024-06-01 12:41:54 [\u001B[34msrc.data_processing:38\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The minimum number of cycles of a motor before the padding is 21.\u001B[0m\n",
      "2024-06-01 12:41:54 [\u001B[34msrc.data_processing:44\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The padding value is 9.\u001B[0m\n",
      "2024-06-01 12:41:56 [\u001B[34msrc.data_processing:53\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The minimum number of cycles of a motor with the padding is 21.\u001B[0m\n",
      "2024-06-01 12:41:56 [\u001B[34msrc.data_splitting:47\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Train set contains 234 different engines --> in total 50633\u001B[0m\n",
      "2024-06-01 12:41:56 [\u001B[34msrc.data_splitting:48\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>>  Test set contains 26 different engines --> in total 5466\u001B[0m\n",
      "2024-06-01 12:42:10 [\u001B[34msrc.data_processing:111\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The median of the frequency of each RUL value in the data is 234.0.\u001B[0m\n",
      "2024-06-01 12:42:10 [\u001B[34msrc.data_processing:112\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The the frequency of the clipped RUL value in the data is 14895.\u001B[0m\n",
      "2024-06-01 12:42:10 [\u001B[34msrc.data_processing:118\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The the frequency of the clipped RUL value after dropping is 468.\u001B[0m\n",
      "2024-06-01 12:42:10 [\u001B[34msrc.data_processing:126\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The number of samples in the data has dropped from 43847 to 29420.\u001B[0m\n",
      "2024-06-01 12:42:10 [\u001B[34msrc.data_processing:111\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The median of the frequency of each RUL value in the data is 26.0.\u001B[0m\n",
      "2024-06-01 12:42:10 [\u001B[34msrc.data_processing:112\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The the frequency of the clipped RUL value in the data is 1488.\u001B[0m\n",
      "2024-06-01 12:42:10 [\u001B[34msrc.data_processing:118\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The the frequency of the clipped RUL value after dropping is 52.\u001B[0m\n",
      "2024-06-01 12:42:10 [\u001B[34msrc.data_processing:126\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The number of samples in the data has dropped from 4712 to 3276.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "if apply_peaks_generation:\n",
    "    train_data = extract_peaks_from_sensor_signal(train_data)\n",
    "    test_data = extract_peaks_from_sensor_signal(test_data)\n",
    "\n",
    "\n",
    "if apply_data_cleaning:\n",
    "    train_data, test_data = clean_data(train_data, test_data, method=None, ignore_columns=['UnitNumber', 'Cycle'], threshold_missing=0.1, threshold_corr=0.0, contamination=0.05)\n",
    "    \n",
    "# Add column RUL to train_data\n",
    "train_data = calculate_RUL(train_data, time_column, group_column, clip_value)\n",
    "\n",
    "train_data, test_data = apply_padding_on_train_data_and_test_data(train_data=train_data, test_data=test_data, window_size=window_size)\n",
    "\n",
    "train, val = train_val_split_by_group(train_data, test_size=test_size, random_state=12)\n",
    "\n",
    "X_train, y_train = create_sliding_window(train, window_size=window_size)  #, drop_columns=['UnitNumber', 'Cycle, 'RUL'])\n",
    "X_val, y_val = create_sliding_window(val, window_size=window_size)  #, drop_columns=['UnitNumber', 'Cycle', 'RUL'])\n",
    "X_test, _ = create_sliding_window_test(test_data, column_RUL=False, drop_columns=['UnitNumber'])\n",
    "y_test = test_RUL_data.values\n",
    "\n",
    "X_train, y_train = drop_samples_with_clipped_values(X_train, y_train, clip_value)\n",
    "X_val, y_val = drop_samples_with_clipped_values(X_val, y_val, clip_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the data\n",
    "*\tThe applied scalers are the StandardScaler, the MinMaxScaler, and the RobustScaler \n",
    "*\tThese three scalers have been selected because the training has been most robust with them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T10:42:18.661544Z",
     "start_time": "2024-06-01T10:42:10.141321Z"
    }
   },
   "outputs": [],
   "source": [
    "# Note: Do not normalize the cycle value! That is why we start with one\n",
    "for single_scaler in scaler:\n",
    "    for i in range(1, X_train.shape[-1]):\n",
    "        X_train[:, :, i] = single_scaler.fit_transform(X_train[:, :, i])\n",
    "        X_val[:, :, i] = single_scaler.transform(X_val[:, :, i])\n",
    "        X_test[:, :, i] = single_scaler.transform(X_test[:, :, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change data types of arrays to float32 and swap axes if necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T10:42:19.084653Z",
     "start_time": "2024-06-01T10:42:18.662535Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29420, 30, 46)\n",
      "(29420, 46, 30)\n",
      "(3276, 30, 46)\n",
      "(3276, 46, 30)\n",
      "(259, 30, 46)\n",
      "(259, 46, 30)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "X_train = np.swapaxes(X_train, 1, 2)\n",
    "X_train = np.array(X_train, dtype=np.float32)\n",
    "y_train = np.array(y_train, dtype=np.float32)\n",
    "print(X_train.shape)\n",
    "\n",
    "print(X_val.shape)\n",
    "X_val = np.swapaxes(X_val, 1, 2)\n",
    "X_val = np.array(X_val, dtype=np.float32)\n",
    "y_val = np.array(y_val, dtype=np.float32)\n",
    "print(X_val.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "X_test = np.swapaxes(X_test, 1, 2)\n",
    "X_test = np.array(X_test, dtype=np.float32)\n",
    "y_test = np.array(y_test, dtype=np.float32)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save processed test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T10:42:19.458559Z",
     "start_time": "2024-06-01T10:42:19.086168Z"
    }
   },
   "outputs": [],
   "source": [
    "save_test_data = False\n",
    "if save_test_data:\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    np.save(f\"{config['paths']['processed_data_dir']}ex2_preprocessed_X_test_from_dataset_{dataset_num}_for_CNNModel1_{timestamp}.npy\", X_test)\n",
    "    np.save(f\"{config['paths']['processed_data_dir']}ex2_preprocessed_y_test_from_dataset_{dataset_num}_for_CNNModel1_{timestamp}.npy\", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture\n",
    "*\tThe architecture of the first CNN model (‚ÄúExampleCNNModel‚Äù) is a minimalistic approach with only two convolutional layers and some fully connected layers \n",
    "*\tThe second CNN model uses more convolutional layers and one fully connected layers more\n",
    "*\tMore convolutional layers are used to be more like the architecture from the paper from Mitici [1]\n",
    "*\tBoth architectures use only 1D convolutional layers as is done in the paper [1]\n",
    "*\tBoth use dropout to enable generalization and prevent overfitting\n",
    "*\tAdam is used as an optimizer and the mean squared error as a loss function\n",
    "*\tBecause the possible targets are higher or equal to one in the second CNN the max function with one is applied on the output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter search\n",
    "*\tThe best hyperparameters are found with Bayesian Optimization\n",
    "*\tFor each dataset a new set of hyperparameters has been searched\n",
    "*\tThe search has been done on the SCC JupyterHub and to parallelize the computation for each data set a separate Notebook has been created\n",
    "*\tThe notebooks are stored in the ‚Äúnotebooks/cnn_hyperparameter_search‚Äù folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: By writing 'tensorboard --logdir=lightning_logs/' in the console the runs get visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T10:42:19.851389Z",
     "start_time": "2024-06-01T10:42:19.461561Z"
    }
   },
   "outputs": [],
   "source": [
    "hyper_params = [{'batch_size': 114.84809532072403, 'beta_1': 0.9586517323123119, 'beta_2': 0.9558431375026947, 'dropout': 0.021025382021542985, 'learning_rate_init': 0.01}, \n",
    "                {'batch_size': 127.48322018921996, 'beta_1': 0.8003393739374182, 'beta_2': 0.9058535052032789, 'dropout': 0.2584373840086995, 'learning_rate_init': 0.0015861602059778223},\n",
    "                {'batch_size': 92.4798215637139, 'beta_1': 0.9635139876762263, 'beta_2': 0.9432583039935667, 'dropout': 0.2119494320551308, 'learning_rate_init': 0.0004461791916105841}, \n",
    "                {'batch_size': 153.588222351065, 'beta_1': 0.9644278054982097, 'beta_2': 0.926610728635691, 'dropout': 0.02279168671841337, 'learning_rate_init': 0.007943006245227067},\n",
    "                ]\n",
    "\n",
    "seeds = [21, 21, 21, 21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T10:42:20.274608Z",
     "start_time": "2024-06-01T10:42:19.852408Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 21\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(seeds[dataset_num-1])\n",
    "\n",
    "# Select hyperparameters of trainer!\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\")\n",
    "trainer = Trainer(min_epochs=1, max_epochs=150, callbacks=[checkpoint_callback], deterministic=True)\n",
    "datamodule = TurbofanDatamodule(batch_size=int(hyper_params[dataset_num-1]['batch_size']))\n",
    "datamodule.set_train_dataset(X_train, y_train)\n",
    "datamodule.set_val_dataset(X_val, y_val)\n",
    "datamodule.set_predict_dataset(X_test)\n",
    "datamodule.set_test_dataset(X_test, y_test[:, 0])\n",
    "model = CNNModel(lr=hyper_params[dataset_num-1]['learning_rate_init'], beta_1=hyper_params[dataset_num-1]['beta_1'], beta_2=hyper_params[dataset_num-1]['beta_2'], window_size=window_size, features=X_train.shape[1], dropout_rate=hyper_params[dataset_num-1]['dropout'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T10:44:51.948418Z",
     "start_time": "2024-06-01T10:42:20.275726Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name        | Type    | Params\n",
      "----------------------------------------\n",
      "0 | loss        | MSELoss | 0     \n",
      "1 | dropout     | Dropout | 0     \n",
      "2 | layer1_conv | Conv1d  | 9.2 K \n",
      "3 | layer2_conv | Conv1d  | 8.0 K \n",
      "4 | layer3_conv | Conv1d  | 8.0 K \n",
      "5 | layer4_conv | Conv1d  | 8.0 K \n",
      "6 | fc1         | Linear  | 153 K \n",
      "7 | fc2         | Linear  | 8.3 K \n",
      "8 | fc3         | Linear  | 65    \n",
      "----------------------------------------\n",
      "195 K     Trainable params\n",
      "0         Non-trainable params\n",
      "195 K     Total params\n",
      "0.782     Total estimated model params size (MB)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# For visualization write 'tensorboard --logdir=lightning_logs/' in console\n",
    "\n",
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T10:44:52.848685Z",
     "start_time": "2024-06-01T10:44:51.950931Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at C:\\Users\\Johannes\\PycharmProjects\\damage-propagation-modeling\\lightning_logs\\version_3\\checkpoints\\epoch=22-step=5336.ckpt\n",
      "Loaded model weights from the checkpoint at C:\\Users\\Johannes\\PycharmProjects\\damage-propagation-modeling\\lightning_logs\\version_3\\checkpoints\\epoch=22-step=5336.ckpt\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "pred = trainer.test(model, datamodule=datamodule, ckpt_path=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pred"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores on all testsets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T07:45:29.666572Z",
     "start_time": "2024-06-01T07:45:24.488957Z"
    }
   },
   "outputs": [],
   "source": [
    "all_test_data = []\n",
    "paths = [\n",
    "    ('data/processed/ex2_preprocessed_X_test_from_dataset_1_for_CNNModel1_20240531-232248.npy', 'data/processed/ex2_preprocessed_y_test_from_dataset_1_for_CNNModel1_20240531-232248.npy'),\n",
    "    ('data/processed/ex2_preprocessed_X_test_from_dataset_2_for_CNNModel1_20240531-233230.npy', 'data/processed/ex2_preprocessed_y_test_from_dataset_2_for_CNNModel1_20240531-233230.npy'),\n",
    "    ('data/processed/ex2_preprocessed_X_test_from_dataset_3_for_CNNModel1_20240531-232732.npy', 'data/processed/ex2_preprocessed_y_test_from_dataset_3_for_CNNModel1_20240531-232732.npy'),\n",
    "    ('data/processed/ex2_preprocessed_X_test_from_dataset_4_for_CNNModel1_20240531-234033.npy', 'data/processed/ex2_preprocessed_y_test_from_dataset_4_for_CNNModel1_20240531-234033.npy'),\n",
    "]\n",
    "for i in range(len(paths)):\n",
    "    X_temp = np.load(paths[i][0])\n",
    "    y_temp = np.load(paths[i][1])\n",
    "    all_test_data.append((X_temp, y_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T21:39:29.132747Z",
     "start_time": "2024-05-31T21:39:28.454281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE score on dataset FD001 is 16.927452087402344.\n"
     ]
    }
   ],
   "source": [
    "dataset_num_temp = 1\n",
    "model = CNNModel(lr=hyper_params[dataset_num_temp-1]['learning_rate_init'], beta_1=hyper_params[dataset_num_temp-1]['beta_1'], beta_2=hyper_params[dataset_num_temp-1]['beta_2'], window_size=window_size, features=all_test_data[dataset_num_temp-1][0].shape[1], dropout_rate=hyper_params[dataset_num_temp-1]['dropout'])\n",
    "checkpoint = torch.load(\"models/cnn_dataset_1.ckpt\")\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "pred = model(torch.tensor(all_test_data[dataset_num_temp-1][0])).detach().numpy()\n",
    "rmse_cnn_1 = root_mean_squared_error(pred, torch.tensor(all_test_data[dataset_num_temp-1][1]))\n",
    "print(f'The RMSE score on dataset FD00{dataset_num_temp} is {rmse_cnn_1}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T07:46:46.173005Z",
     "start_time": "2024-06-01T07:46:45.518541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE score on dataset FD002 is 34.540584564208984.\n"
     ]
    }
   ],
   "source": [
    "dataset_num_temp = 2\n",
    "model = CNNModel(lr=hyper_params[dataset_num_temp-1]['learning_rate_init'], beta_1=hyper_params[dataset_num_temp-1]['beta_1'], beta_2=hyper_params[dataset_num_temp-1]['beta_2'], window_size=window_size, features=all_test_data[dataset_num_temp-1][0].shape[1], dropout_rate=hyper_params[dataset_num_temp-1]['dropout'])\n",
    "checkpoint = torch.load(\"models/cnn_dataset_2.ckpt\")\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "pred = model(torch.tensor(all_test_data[dataset_num_temp-1][0])).detach().numpy()\n",
    "rmse_cnn_2 = root_mean_squared_error(pred, torch.tensor(all_test_data[dataset_num_temp-1][1]))\n",
    "print(f'The RMSE score on dataset FD00{dataset_num_temp} is {rmse_cnn_2}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T21:39:29.726834Z",
     "start_time": "2024-05-31T21:39:29.134252Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE score on dataset FD003 is 19.158700942993164.\n"
     ]
    }
   ],
   "source": [
    "dataset_num_temp = 3\n",
    "model = CNNModel(lr=hyper_params[dataset_num_temp-1]['learning_rate_init'], beta_1=hyper_params[dataset_num_temp-1]['beta_1'], beta_2=hyper_params[dataset_num_temp-1]['beta_2'], window_size=window_size, features=all_test_data[dataset_num_temp-1][0].shape[1], dropout_rate=hyper_params[dataset_num_temp-1]['dropout'])\n",
    "checkpoint = torch.load(\"models/cnn_dataset_3.ckpt\")\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "pred = model(torch.tensor(all_test_data[dataset_num_temp-1][0])).detach().numpy()\n",
    "rmse_cnn_3 = root_mean_squared_error(pred, torch.tensor(all_test_data[dataset_num_temp-1][1]))\n",
    "print(f'The RMSE score on dataset FD00{dataset_num_temp} is {rmse_cnn_3}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T08:06:00.795996Z",
     "start_time": "2024-06-01T08:06:00.141163Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE score on dataset FD004 is 39.244022369384766.\n"
     ]
    }
   ],
   "source": [
    "dataset_num_temp = 4\n",
    "model = CNNModel(lr=hyper_params[dataset_num_temp-1]['learning_rate_init'], beta_1=hyper_params[dataset_num_temp-1]['beta_1'], beta_2=hyper_params[dataset_num_temp-1]['beta_2'], window_size=window_size, features=all_test_data[dataset_num_temp-1][0].shape[1], dropout_rate=hyper_params[dataset_num_temp-1]['dropout'])\n",
    "checkpoint = torch.load(\"models/cnn_dataset_4.ckpt\")\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "pred = model(torch.tensor(all_test_data[dataset_num_temp-1][0])).detach().numpy()\n",
    "rmse_cnn_4 = root_mean_squared_error(pred, torch.tensor(all_test_data[dataset_num_temp-1][1]))\n",
    "print(f'The RMSE score on dataset FD00{dataset_num_temp} is {rmse_cnn_4}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!!!! Transformer Approach down below !!!!!!\n",
    "We thought to add the Transformer approach as well because we spend a lot of time to figure out different techniques.\n",
    "The transformer approach has a similar performance as the CNN approach for dataset 1 and 3. Nevertheless, it is not our best attempt but worth mentioning.\n",
    "\n",
    "The complete notebooks with the cell outputs is in \"Transformer_pipeline.ipynb\". Due to time and computing power limitations, we did run the experiments only on the cluster and copied the code in this file.\n",
    "\n",
    "!!! Go to \"Transformer_pipeline.ipynb\" for more details!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic: EX2 - Turbofan RUL Prediction with Transformer\n",
    "**Task**: Predict the remaining useful life (RUL) of turbofan engines based on given sensor data (time series data). It is a regression problem.\n",
    "**Data**: Turbofan engine degradation simulation data (NASA) - [Link](https://data.nasa.gov/dataset/Turbofan-Engine-Degradation-Simulation-Data-Set/vrks-gjie). See also in the topic [introduction notebook](https://github.com/nina-prog/damage-propagation-modeling/blob/2fb8c1a1102a48d7abbf04e4031807790a913a99/notebooks/Turbofan%20remaining%20useful%20life%20Prediction.ipynb).\n",
    "\n",
    "**Subtasks**:\n",
    "4. Create **neural network models** to predict the remaining useful life. Includes different architectures like Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), or Attention Models. Note: You can search for SOTA research papers and reproduce current state-of-the-art models.\n",
    "\n",
    "\n",
    "## Transformer Description\n",
    "Next to the previous CNN Architecture, we also had a look at Transformer Models.\n",
    "The Transformer Model consists of a projection layer, an Transformer Encoder Block and a output layer at the end. <br>\n",
    "The Transformer Models in general are bigger than the CNN Models with 1 to 10 million parameters depending on the projection size and window length, because at the end the (projection size x window_size) is flattened and inputed into the output layer.\n",
    "\n",
    "### Results\n",
    "The Transformer Model showed impressive results on the first and third dataset with an RMSE of 18.6 and 21.8. The models are stored in the models folder in pt format. However our Transformer Model was not able to find a fit for dataset 2 and 4. Having in mind the good results of the CNN approach, we did not further investigate the Transformer Model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports + Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## download important libraries\n",
    "!pip install colorlog\n",
    "!pip install ray\n",
    "!pip install ax-platform botorch\n",
    "# third-party libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import List, Union\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from scipy import stats\n",
    "from scipy.stats import multivariate_normal, zscore\n",
    "from scipy.stats._mstats_basic import winsorize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source code\n",
    "os.chdir(\"../\") # set working directory to root of project\n",
    "\n",
    "# import own libraries\n",
    "from src.utils import load_data, load_config, train_val_split_by_group\n",
    "from src.rolling_window_creator import RollingWindowDatasetCreator, calculate_RUL\n",
    "from src.data_cleaning import identify_missing_values, identify_single_unique_features, format_dtype, clean_data\n",
    "import src.nn_utils as nu\n",
    "import src.transformer_fred as tff\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load config + Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_CONFIG = \"configs/config.yaml\"\n",
    "config = load_config(PATH_TO_CONFIG) # config is dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## define data set you want to use in dataset_num\n",
    "dataset_num = 1\n",
    "train_data, test_data, test_RUL_data = load_data(config_path=PATH_TO_CONFIG, dataset_num=dataset_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## overview over test dataset\n",
    "# count unit numbers in test set\n",
    "print(f\"Number of unique unit numbers in test set: {test_data['UnitNumber'].nunique()}\")\n",
    "# count min number of cycles in test set for each unit number --> window size must be in the range of these values, for example a window size of 10 would be too large if there is a unit number with only 10 cycles\n",
    "print(\"Min number of cycles in test set for a unit number: \", test_data.groupby(\"UnitNumber\")[\"Cycle\"].count().min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Test Data Cleaning Functionality and its impact on Rolling Window Creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "## Finding: The Transformer Model works better, if it contains all variable.\n",
    "##          Even those that have no clear correlation to RUL.\n",
    "##          The Transformer Model can still use the contextual information given in complex time series\n",
    "\n",
    "# clean data (with outlier removal, where no samples are dropped but the outliers are replaced, method='winsorize')\n",
    "cleaned_train, cleaned_test = clean_data(train_data, test_data, method='winsorize', ignore_columns=['UnitNumber', 'Cycle'], threshold_missing=0.1, threshold_corr=0.0, contamination=0.05)\n",
    "\n",
    "# calculate RUL for test dataset\n",
    "cleaned_train_data = calculate_RUL(cleaned_train, time_column= \"Cycle\", group_column= \"UnitNumber\")\n",
    "cleaned_test_data = nu.calculate_RUL_test(cleaned_test, test_RUL_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "## Finding: The minimun window length in the test datasets are\n",
    "##          significantly smaller than in the train datasets\n",
    "##          --> one explanation for sometimes poorer performance on test dataset\n",
    "group_sizes = test_data.groupby('UnitNumber').size()\n",
    "\n",
    "# Calculate min, max, and mean of the group sizes\n",
    "min_size = group_sizes.min()\n",
    "max_size = group_sizes.max()\n",
    "mean_size = group_sizes.mean()\n",
    "sd_size = group_sizes.std()\n",
    "\n",
    "print(f\"Min group size: {min_size}\")\n",
    "print(f\"Max group size: {max_size}\")\n",
    "print(f\"Mean group size: {mean_size}\")\n",
    "print(f\"Sd group size: {sd_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameter search with Ray Tune\n",
    "The hyper parameter search was done with Ray Tune on the cluster.\n",
    "However we were not fully satisfied by the library and many tests and finetuning was also\n",
    "done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "## Finding: The window length is a import hyperparameter.\n",
    "##          The Transformer Model needs bigger window size than the CNN models.\n",
    "##          A good window size is around 80 instead of 30.\n",
    "##          We have the feeling that an even bigger window size is better (140+) but it is limited \n",
    "##          due to the test dataset and its limited length.\n",
    "##          Another aspect is the model size: only one layer is suitable due to size issues and also the \n",
    "##          combination of window size * project dim should not be too big\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from ray import tune, train\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune import CLIReporter\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Import your necessary functions and modules (assuming nu module and TransformerModel are defined)\n",
    "# from your_module import TransformerModel, TurbofanDataset, scale_data, create_sliding_window, train_val_split_by_group, count_parameters\n",
    "\n",
    "# Define training function\n",
    "def train_model(config, checkpoint_dir=None):\n",
    "    window_size = config[\"window_size\"]\n",
    "    project_dim = config[\"project_dim\"]\n",
    "    num_heads = config[\"num_heads\"]\n",
    "    \n",
    "    # Assume your data preparation functions are defined and work as shown in your example\n",
    "    train_data, val_data = nu.scale_data(cleaned_train_data, cleaned_test_data)\n",
    "    X_train, y_train = nu.create_sliding_window(train_data, window_size=window_size)\n",
    "    X_val, y_val = nu.create_sliding_window(val_data, window_size=window_size, typ = \"test\")\n",
    "    y_train = np.clip(y_train, a_min=None, a_max=130)\n",
    "    y_train, X_train = nu.cut_high_RUL(y_train, X_train, 140, delete = 0.3)\n",
    "    \n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = tff.TurbofanDataset(X_train, y_train)\n",
    "    val_dataset = tff.TurbofanDataset(X_val, y_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = tff.TransformerModel(feature_size=X_train.shape[2], num_heads=num_heads, num_layers=config[\"num_layers\"], project_dim=project_dim, window_size=window_size, dropout = config[\"dropout\"]).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    scheduler = StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "    \n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            targets = targets.view(-1, 1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                targets = targets.view(-1, 1)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                ## preprocessing\n",
    "                outputs = torch.round(outputs)\n",
    "                # Set minimal value to 1\n",
    "                min_value = 1\n",
    "                outputs = torch.where(outputs < min_value, torch.tensor(min_value), outputs)\n",
    "\n",
    "                loss = criterion(outputs, targets)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "        val_loss = running_loss / len(val_loader.dataset)\n",
    "        \n",
    "        train.report({\"val_loss\":val_loss, \"train_loss\":train_loss})\n",
    "        scheduler.step()\n",
    "\n",
    "# Define search space and Ray Tune configuration\n",
    "search_space = {\n",
    "    \"window_size\": tune.choice([50, 60, 70, 80, 90, 100]),\n",
    "    \"project_dim\": tune.choice([16*3*2, 16*3*3]),\n",
    "    \"num_heads\": tune.choice([8, 12]),\n",
    "    \"num_layers\": 1,\n",
    "    \"batch_size\": 128,\n",
    "    \"dropout\": tune.choice([0.12, 0.18]),\n",
    "    \"num_epochs\": tune.choice([5, 8, 11, 13, 17, 19, 22])  # Reduced for quicker tuning\n",
    "}\n",
    "\n",
    "# Use ASHAScheduler for efficient hyperparameter search\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    max_t=25,\n",
    "    grace_period=5,\n",
    "    reduction_factor=2\n",
    ")\n",
    "\n",
    "# Configure the reporter\n",
    "reporter = CLIReporter(\n",
    "    metric_columns=[\"val_loss\", \"train_loss\", \"training_iteration\"]\n",
    ")\n",
    "\n",
    "# Run hyperparameter search\n",
    "result = tune.run(\n",
    "    train_model,\n",
    "    resources_per_trial={\"cpu\": 1, \"gpu\": 1},\n",
    "    config=search_space,\n",
    "    num_samples=50,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter\n",
    ")\n",
    "\n",
    "# Get the best trial\n",
    "best_trial = result.get_best_trial(\"val_loss\", \"min\", \"last\")\n",
    "print(\"Best trial config: {}\".format(best_trial.config))\n",
    "print(\"Best trial final validation loss: {}\".format(best_trial.last_result[\"val_loss\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the best model again from scratch and save the best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "## Data specifics\n",
    "window_size = best_trial.config[\"window_size\"]\n",
    "train_data, test_data = nu.scale_data(cleaned_train_data, cleaned_test_data)\n",
    "\n",
    "## the validation split is done in away that one UnitNumber is either in Train or val but not in both\n",
    "#train, val = train_val_split_by_group(train_data)\n",
    "\n",
    "X_train, y_train = nu.create_sliding_window(train_data, window_size = window_size)\n",
    "\n",
    "# Set values to maximum of the 130 according to the papers because in the early stage, everything looks the same\n",
    "y_train = np.clip(y_train, a_min=None, a_max=130)\n",
    "# now we have a overrepresentation of high RUL values in the train data --> randomly delete some of them\n",
    "y_train, X_train = nu.cut_high_RUL(y_train, X_train, 130, delete = 0.3)\n",
    "\n",
    "#X_val, y_val = nu.create_sliding_window(val, window_size = window_size)\n",
    "\n",
    "#test_data = nu.scale_data(cleaned_test_data)\n",
    "X_test, y_test = nu.create_sliding_window(test_data, typ = \"test\", window_size = window_size)\n",
    "\n",
    "##################################\n",
    "## Model specifics\n",
    "seq_len, batch_size, feature_size = X_train.shape[1], best_trial.config[\"batch_size\"], X_train.shape[2]\n",
    "num_heads, num_layers, project_dim  = best_trial.config[\"num_heads\"], best_trial.config[\"num_layers\"], best_trial.config[\"project_dim\"]\n",
    "num_epochs = 40\n",
    "learning_rate = 0.0001\n",
    "\n",
    "print(seq_len)\n",
    "# Create dataset and dataloaders\n",
    "train_dataset = tff.TurbofanDataset(X_train, y_train)\n",
    "test_dataset = tff.TurbofanDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "# Initialize model, criterion, optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = tff.TransformerModel(feature_size, num_heads, num_layers, project_dim = project_dim, window_size = seq_len, dropout = best_trial.config[\"dropout\"]).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "## scheduler plays no role more in the later experiements\n",
    "scheduler = StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "\n",
    "print(f\"The model has in total {tff.count_parameters(model)} parameters!!\")\n",
    "\n",
    "prev_acc = 1200\n",
    "# Training loop save the best model on the fly\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = tff.train_model(model, train_loader, criterion, optimizer, device)\n",
    "    #val_loss = evaluate_model(model, val_loader, criterion, device)\n",
    "    test_loss = tff.evaluate_model(model, test_loader, criterion, device)\n",
    "    scheduler.step()\n",
    "\n",
    "    if prev_acc > test_loss:\n",
    "        save_path = f\"tranM_df_{dataset_num}_{int(test_loss)}.pth\"\n",
    "        print(f\"new best RMSE: {test_loss}\")\n",
    "        prev_acc = test_loss\n",
    "        #torch.save(model, save_path)\n",
    "\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train_L: {train_loss:.2f}, Test_L: {test_loss:.2f}, Test_RMSE: {np.sqrt(test_loss):.2f} \")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "## Finding: good config for df1 and df3:\n",
    "window_size = 80\n",
    "seq_len, batch_size, feature_size = X_train.shape[1], 128, X_train.shape[2]\n",
    "num_heads, num_layers, project_dim  = 8, 1, 120\n",
    "num_epochs = 100\n",
    "learning_rate = 0.0001\n",
    "drop_out = 0.18\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load best Transformer Models for DF1 and DF3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load best models for dataset 1 and 3\n",
    "PATH_DF1 = \"models/tranM_df_1_351.pth\"\n",
    "PATH_DF3= \"models/tranM_df_3_476.pth\"\n",
    "loaded_model_1 = torch.load(PATH_DF1)\n",
    "loaded_model_3 = torch.load(PATH_DF3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
