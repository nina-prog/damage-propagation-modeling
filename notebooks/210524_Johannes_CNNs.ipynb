{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T18:00:22.990987Z",
     "start_time": "2024-05-31T18:00:22.815492Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T18:00:23.192448Z",
     "start_time": "2024-05-31T18:00:23.004308Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Topic: EX2 - Turbofan RUL Prediction\n",
    "**Task**: Predict the remaining useful life (RUL) of turbofan engines based on given sensor data (time series data). It is a forcasting problem, where the goal is to predict the number of cycles an engine will last before it fails.\n",
    "**Data**: Turbofan engine degradation simulation data (NASA) - [Link](https://data.nasa.gov/dataset/Turbofan-Engine-Degradation-Simulation-Data-Set/vrks-gjie). See also in the topic [introduction notebook](https://github.com/nina-prog/damage-propagation-modeling/blob/2fb8c1a1102a48d7abbf04e4031807790a913a99/notebooks/Turbofan%20remaining%20useful%20life%20Prediction.ipynb).\n",
    "\n",
    "**Subtasks**:\n",
    "1. Perform a deep **exploratory data analysis (EDA)** on the given data.\n",
    "2. Implement a more efficient **sliding window method** for time series data analysis. -> üéØ **Focus on this task**\n",
    "3. Apply **traditional machine learning methods** (SOTA) to predict the remaining useful life. Includes data preparation, feature extraction, feature selection, model selection, and model parameter optimization.\n",
    "4. Create **neural network models** to predict the remaining useful life. Includes different architectures like Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), or Attention Models. Note: You can search for SOTA research papers and reproduce current state-of-the-art models.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports + Settings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# third-party libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import torch \n",
    "\n",
    "import time\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T18:00:29.219989Z",
     "start_time": "2024-05-31T18:00:23.526704Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "# source code\n",
    "from src.data_loading import load_data, load_config\n",
    "from src.data_splitting import train_val_split_by_group\n",
    "from src.nn_utils import create_sliding_window, create_sliding_window_test\n",
    "from src.rolling_window_creator import calculate_RUL\n",
    "from src.data_processing import apply_padding_on_train_data_and_test_data, drop_samples_with_clipped_values, extract_peaks_from_sensor_signal\n",
    "from src.nn_util.nn_models.ligthning.cnnModel1 import CNNModel1 as CNNModel\n",
    "from src.nn_util.datamodule.lightning.turbofanDatamodule import TurbofanDatamodule\n",
    "from src.data_cleaning import clean_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T18:00:30.082816Z",
     "start_time": "2024-05-31T18:00:29.219989Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "# settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"Set2\")\n",
    "sns.set(rc={\"figure.dpi\":100, 'savefig.dpi':200})\n",
    "sns.set_context('notebook')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T18:00:30.362408Z",
     "start_time": "2024-05-31T18:00:30.084475Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "np.random.seed(42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T18:00:30.650499Z",
     "start_time": "2024-05-31T18:00:30.364230Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Paths"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Make sure to execute this cell only once for one kernel session, before running any other cell below.\n",
    "os.chdir(\"../\") # set working directory to root of project\n",
    "os.getcwd() # check current working directory"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T18:00:30.951099Z",
     "start_time": "2024-05-31T18:00:30.653638Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Johannes\\\\PycharmProjects\\\\damage-propagation-modeling'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "PATH_TO_CONFIG = \"configs/config.yaml\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T18:00:31.264664Z",
     "start_time": "2024-05-31T18:00:30.953189Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Config + Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "config = load_config(PATH_TO_CONFIG) # config is dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T18:00:31.530381Z",
     "start_time": "2024-05-31T18:00:31.266949Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "dataset_num = 3\n",
    "train_data, test_data, test_RUL_data = load_data(config_path=PATH_TO_CONFIG, dataset_num=dataset_num)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T18:00:32.184294Z",
     "start_time": "2024-05-31T18:00:31.532152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-31 20:00:31 [\u001B[34msrc.data_loading:43\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Loading data set 3...\u001B[0m\n",
      "2024-05-31 20:00:32 [\u001B[34msrc.data_loading:72\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Loaded raw data for dataset 3.\u001B[0m\n",
      "2024-05-31 20:00:32 [\u001B[34msrc.data_loading:73\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Train Data: (24720, 26)\u001B[0m\n",
      "2024-05-31 20:00:32 [\u001B[34msrc.data_loading:74\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Test Data: (16596, 26)\u001B[0m\n",
      "2024-05-31 20:00:32 [\u001B[34msrc.data_loading:75\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Test RUL Data: (100, 1)\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": "# Create Neural Regression Models",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Pipeline:\n",
    "1.\tData Cleaning\n",
    "2.\tOptional: Padding\n",
    "3.\tCreate sliding windows\n",
    "4.\tSplit train data in validation and train data\n",
    "5.\tDrop some samples with the clipped value\n",
    "6.\tScale the Data\n",
    "7.\tFind the best hyperparameters\n",
    "8.\tCreate Model with found hyperparameters"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Explanation of selected hyperparameters:\n",
    "*\tWindow size: We selected a window size of 30 due to some experiments with other window sizes. Furthermore, the window size is also used in the paper from Mitici [1] which shows good results with a CNN architecture.\n",
    "*\tClipping value: The clipping value of 125 has been selected because it has proven useful and is used in paper [1] "
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "References:\n",
    "1.\tMihaela Mitici, Ingeborg de Pater, Anne Barros, Zhiguo Zeng, ‚ÄúDynamic predictive maintenance for multiple components using data-driven probabilistic RUL prognostics: The case of turbofan engines‚Äù, Reliability Engineering & System Safety, Volume 234, 2023, https://doi.org/10.1016/j.ress.2023.109199.\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T18:00:32.466141Z",
     "start_time": "2024-05-31T18:00:32.186872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# some hyperparameters\n",
    "time_column = 'Cycle'\n",
    "group_column = 'UnitNumber'\n",
    "\n",
    "window_size = 30\n",
    "clip_value = 125\n",
    "test_size = 0.1\n",
    "apply_data_cleaning = True\n",
    "# If activated, adds for every sensor a new column with the commutative sum of the peaks\n",
    "apply_peaks_generation = False\n",
    "\n",
    "# Apply scaler. The order in the list represents the order in which they are applied\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "robust_scaler = RobustScaler()\n",
    "scaler = [std_scaler, minmax_scaler, robust_scaler]\n",
    "\n",
    "# The model_type can be 'CNN_2D' or 'CNN_1D' and defines what type of convolutional layers has been used.\n",
    "model_type = 'CNN_1D' "
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Explanation of each step:\n",
    "- Data Cleaning\n",
    "    - The outlier detection and replacement method has been deactivated. \n",
    "    - The removal of columns based on the correlation of a single value has been deactivated because the neural model makes the feature selection.\n",
    "    - Features with a unique single value will be removed\n",
    "- Padding:\n",
    "    -\tOnly applied for the datasets with a sample in test or train data smaller than the window size.\n",
    "    -\tThe padding length is exactly the difference between the window size and the timesteps of the sample with the fewest timesteps\n",
    "    -\tThe padding is applied on all the time series\n",
    "- Create sliding window\n",
    "    -\tTODO: Explanation Frederik (falls du noch eine hinzuf√ºgen m√∂chtest)\n",
    "- Split train data in validation and train data\n",
    "    -\tSplitting training and validation sets based on the UnitNumber\n",
    "- Drop some samples with the clipped value:\n",
    "    -\tTo make the data more evenly distributed, in this step some of the samples with the clipping value as RUL are removed \n",
    "    -\tTherefore, the median of the frequency of other RUL values is computed and the number of samples with the clipping value is a multiple of the median. \n",
    "    -\tWe selected two to not drop to many samples\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T18:00:40.774844Z",
     "start_time": "2024-05-31T18:00:32.467142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if apply_peaks_generation:\n",
    "    train_data = extract_peaks_from_sensor_signal(train_data)\n",
    "    test_data = extract_peaks_from_sensor_signal(test_data)\n",
    "    \n",
    "if apply_data_cleaning:\n",
    "    train_data, test_data = clean_data(train_data, test_data, method=None, ignore_columns=['UnitNumber', 'Cycle'], threshold_missing=0.1, threshold_corr=0.0, contamination=0.05)\n",
    "    \n",
    "# Add column RUL to train_data\n",
    "train_data = calculate_RUL(train_data, time_column, group_column, clip_value)\n",
    "\n",
    "train_data, test_data = apply_padding_on_train_data_and_test_data(train_data=train_data, test_data=test_data, window_size=window_size)\n",
    "\n",
    "train, val = train_val_split_by_group(train_data, test_size=test_size, random_state=12)\n",
    "\n",
    "X_train, y_train = create_sliding_window(train, window_size=window_size)  #, drop_columns=['UnitNumber', 'Cycle, 'RUL'])\n",
    "X_val, y_val = create_sliding_window(val, window_size=window_size)  #, drop_columns=['UnitNumber', 'Cycle', 'RUL'])\n",
    "X_test, _ = create_sliding_window_test(test_data, column_RUL=False, drop_columns=['UnitNumber'])\n",
    "y_test = test_RUL_data.values\n",
    "\n",
    "X_train, y_train = drop_samples_with_clipped_values(X_train, y_train, clip_value)\n",
    "X_val, y_val = drop_samples_with_clipped_values(X_val, y_val, clip_value)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-31 20:00:32 [\u001B[34msrc.data_cleaning:134\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Cleaning train and test data...\u001B[0m\n",
      "2024-05-31 20:00:32 [\u001B[34msrc.data_cleaning:136\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Formatting column types...\u001B[0m\n",
      "2024-05-31 20:00:32 [\u001B[34msrc.data_cleaning:69\u001B[0m] [DEBUG\u001B[0m] >>>> Found 0 categorical columns: []\u001B[0m\n",
      "2024-05-31 20:00:32 [\u001B[34msrc.data_cleaning:69\u001B[0m] [DEBUG\u001B[0m] >>>> Found 0 categorical columns: []\u001B[0m\n",
      "2024-05-31 20:00:32 [\u001B[34msrc.data_cleaning:141\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Handling duplicates...\u001B[0m\n",
      "2024-05-31 20:00:32 [\u001B[34msrc.data_cleaning:146\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Removing outliers...\u001B[0m\n",
      "2024-05-31 20:00:32 [\u001B[34msrc.outlier_detection:150\u001B[0m] [DEBUG\u001B[0m] >>>> Removing outliers using method: None ...\u001B[0m\n",
      "2024-05-31 20:00:32 [\u001B[34msrc.outlier_detection:162\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001B[0m\n",
      "2024-05-31 20:00:32 [\u001B[34msrc.outlier_detection:150\u001B[0m] [DEBUG\u001B[0m] >>>> Removing outliers using method: None ...\u001B[0m\n",
      "2024-05-31 20:00:32 [\u001B[34msrc.outlier_detection:162\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001B[0m\n",
      "2024-05-31 20:00:32 [\u001B[34msrc.data_cleaning:150\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Filter features based train data...\u001B[0m\n",
      "2024-05-31 20:00:32 [\u001B[34msrc.data_cleaning:26\u001B[0m] [DEBUG\u001B[0m] >>>> Found 0 features with missing values above the threshold of 0.1.\u001B[0m\n",
      "2024-05-31 20:00:32 [\u001B[34msrc.data_cleaning:46\u001B[0m] [DEBUG\u001B[0m] >>>> Found 6 features with only a single unique value: ['Operation Setting 3', 'Sensor Measure 1', 'Sensor Measure 5', 'Sensor Measure 16', 'Sensor Measure 18', 'Sensor Measure 19']\u001B[0m\n",
      "2024-05-31 20:00:32 [\u001B[34msrc.data_cleaning:103\u001B[0m] [DEBUG\u001B[0m] >>>> Found 0 uncorrelated features with a correlation threshold of 0.0: []\u001B[0m\n",
      "2024-05-31 20:00:32 [\u001B[34msrc.data_cleaning:162\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Dropping features based on missing values, single unique values, and no target correlation...\u001B[0m\n",
      "2024-05-31 20:00:32 [\u001B[34msrc.data_cleaning:172\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Data cleaning completed.\u001B[0m\n",
      "2024-05-31 20:00:32 [\u001B[34msrc.data_cleaning:173\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Original train DataFrame shape: (24720, 20), Resulting train DataFrame shape: (24720, 20)\u001B[0m\n",
      "2024-05-31 20:00:32 [\u001B[34msrc.data_cleaning:174\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Original test DataFrame shape: (16596, 20), Resulting test DataFrame shape: (16596, 20)\u001B[0m\n",
      "2024-05-31 20:00:32 [\u001B[34msrc.data_processing:38\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The minimum number of cycles of a motor before the padding is 38.\u001B[0m\n",
      "2024-05-31 20:00:32 [\u001B[34msrc.data_processing:44\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The padding value is 0.\u001B[0m\n",
      "2024-05-31 20:00:33 [\u001B[34msrc.data_processing:53\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The minimum number of cycles of a motor with the padding is 38.\u001B[0m\n",
      "2024-05-31 20:00:33 [\u001B[34msrc.data_splitting:47\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Train set contains 90 different engines --> in total 22432\u001B[0m\n",
      "2024-05-31 20:00:33 [\u001B[34msrc.data_splitting:48\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>>  Test set contains 10 different engines --> in total 2288\u001B[0m\n",
      "2024-05-31 20:00:40 [\u001B[34msrc.data_processing:111\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The median of the frequency of each RUL value in the data is 90.0.\u001B[0m\n",
      "2024-05-31 20:00:40 [\u001B[34msrc.data_processing:112\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The the frequency of the clipped RUL value in the data is 8677.\u001B[0m\n",
      "2024-05-31 20:00:40 [\u001B[34msrc.data_processing:118\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The the frequency of the clipped RUL value after dropping is 180.\u001B[0m\n",
      "2024-05-31 20:00:40 [\u001B[34msrc.data_processing:126\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The number of samples in the data has dropped from 19822 to 11325.\u001B[0m\n",
      "2024-05-31 20:00:40 [\u001B[34msrc.data_processing:111\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The median of the frequency of each RUL value in the data is 10.0.\u001B[0m\n",
      "2024-05-31 20:00:40 [\u001B[34msrc.data_processing:112\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The the frequency of the clipped RUL value in the data is 758.\u001B[0m\n",
      "2024-05-31 20:00:40 [\u001B[34msrc.data_processing:118\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The the frequency of the clipped RUL value after dropping is 20.\u001B[0m\n",
      "2024-05-31 20:00:40 [\u001B[34msrc.data_processing:126\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The number of samples in the data has dropped from 1998 to 1260.\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Scale the data\n",
    "*\tThe applied scalers are the StandardScaler, the MinMaxScaler, and the RobustScaler \n",
    "*\tThese three scalers have been selected because the training has been most robust with them\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T18:00:42.487104Z",
     "start_time": "2024-05-31T18:00:40.776854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train_shape = X_train.shape\n",
    "X_val_shape = X_val.shape\n",
    "X_test_shape = X_test.shape\n",
    "\n",
    "# Note: Do not normalize the cycle value! That is why we start with one\n",
    "for single_scaler in scaler:\n",
    "    for i in range(1, X_train.shape[-1]):\n",
    "        X_train[:, :, i] = single_scaler.fit_transform(X_train[:, :, i])\n",
    "        X_val[:, :, i] = single_scaler.transform(X_val[:, :, i])\n",
    "        X_test[:, :, i] = single_scaler.transform(X_test[:, :, i])"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Change data types of arrays to float32 and swap axes if necessary:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T18:00:42.771490Z",
     "start_time": "2024-05-31T18:00:42.489569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_type = 'CNN_1D'\n",
    "\n",
    "print(X_train.shape)\n",
    "if model_type == 'CNN_1D':\n",
    "    X_train = np.swapaxes(X_train, 1, 2)\n",
    "elif model_type == 'CNN_2D':\n",
    "    X_train = np.swapaxes(X_train, 1, 2)\n",
    "    X_train = X_train[:, np.newaxis, :, :]\n",
    "X_train = np.array(X_train, dtype=np.float32)\n",
    "y_train = np.array(y_train, dtype=np.float32)\n",
    "print(X_train.shape)\n",
    "\n",
    "print(X_val.shape)\n",
    "if model_type == 'CNN_1D':\n",
    "    X_val = np.swapaxes(X_val, 1, 2)\n",
    "elif model_type == 'CNN_2D':\n",
    "    X_val = np.swapaxes(X_val, 1, 2)\n",
    "    X_val = X_val[:, np.newaxis, :, :]\n",
    "X_val = np.array(X_val, dtype=np.float32)\n",
    "y_val = np.array(y_val, dtype=np.float32)\n",
    "print(X_val.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "if model_type == 'CNN_1D':\n",
    "    X_test = np.swapaxes(X_test, 1, 2)\n",
    "elif model_type == 'CNN_2D':\n",
    "    X_test = np.swapaxes(X_test, 1, 2)\n",
    "    X_test = X_test[:, np.newaxis, :, :]\n",
    "X_test = np.array(X_test, dtype=np.float32)\n",
    "y_test = np.array(y_test, dtype=np.float32)\n",
    "print(X_test.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11325, 30, 19)\n",
      "(11325, 19, 30)\n",
      "(1260, 30, 19)\n",
      "(1260, 19, 30)\n",
      "(100, 30, 19)\n",
      "(100, 19, 30)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CNN"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Architecture\n",
    "*\tThe architecture of the first CNN model (‚ÄúExampleCNNModel‚Äù) is a minimalistic approach with only two convolutional layers and some fully connected layers \n",
    "*\tThe second CNN model uses more convolutional layers and one fully connected layers more\n",
    "*\tMore convolutional layers are used to be more like the architecture from the paper from Mitici [1]\n",
    "*\tBoth architectures use only 1D convolutional layers as is done in the paper [1]\n",
    "*\tBoth use dropout to enable generalization and prevent overfitting\n",
    "*\tAdam is used as an optimizer and the mean squared error as a loss function\n",
    "*\tBecause the possible targets are higher or equal to one in the second CNN the max function with one is applied on the output.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Hyperparameter search\n",
    "*\tThe best hyperparameters are found with Bayesian Optimization\n",
    "*\tFor each dataset a new set of hyperparameters has been searched\n",
    "*\tThe search has been done on the SCC JupyterHub and to parallelize the computation for each data set a separate Notebook has been created\n",
    "*\tThe notebooks are stored in the ‚Äúnotebooks/cnn_hyperparameter_search‚Äù folder\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T18:44:37.077523Z",
     "start_time": "2024-05-31T18:44:36.419558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hyper_params = [{}, \n",
    "                {}, \n",
    "                {'batch_size': 92.4798215637139, 'beta_1': 0.9635139876762263, 'beta_2': 0.9432583039935667, 'dropout': 0.2119494320551308, 'learning_rate_init': 0.0004461791916105841}, \n",
    "                {},\n",
    "                ]\n",
    "\n",
    "seeds = [21, 21, 21, 21]"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T18:30:46.039075Z",
     "start_time": "2024-05-31T18:30:45.356210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pl.seed_everything(seeds[dataset_num-1])\n",
    "\n",
    "# Select hyperparameters of trainer!\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\")\n",
    "trainer = Trainer(min_epochs=1, max_epochs=150, callbacks=[checkpoint_callback], deterministic=True)\n",
    "datamodule = TurbofanDatamodule(batch_size=int(hyper_params[dataset_num-1]['batch_size']))\n",
    "datamodule.set_train_dataset(X_train, y_train)\n",
    "datamodule.set_val_dataset(X_val, y_val)\n",
    "datamodule.set_predict_dataset(X_test)\n",
    "datamodule.set_test_dataset(X_test, y_test[:, 0])\n",
    "model = CNNModel(lr=hyper_params[dataset_num-1]['learning_rate_init'], beta_1=hyper_params[dataset_num-1]['beta_1'], beta_2=hyper_params[dataset_num-1]['beta_2'], window_size=window_size, features=X_train.shape[1], dropout_rate=hyper_params[dataset_num-1]['dropout'])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 18\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T18:39:09.584405Z",
     "start_time": "2024-05-31T18:30:47.464694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "# For visualization write 'tensorboard --logdir=lightning_logs/' in console\n",
    "\n",
    "trainer.fit(model, datamodule=datamodule)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name        | Type    | Params\n",
      "----------------------------------------\n",
      "0 | loss        | MSELoss | 0     \n",
      "1 | dropout     | Dropout | 0     \n",
      "2 | layer1_conv | Conv1d  | 3.8 K \n",
      "3 | layer2_conv | Conv1d  | 8.0 K \n",
      "4 | layer3_conv | Conv1d  | 8.0 K \n",
      "5 | layer4_conv | Conv1d  | 8.0 K \n",
      "6 | fc1         | Linear  | 153 K \n",
      "7 | fc2         | Linear  | 8.3 K \n",
      "8 | fc3         | Linear  | 65    \n",
      "----------------------------------------\n",
      "190 K     Trainable params\n",
      "0         Non-trainable params\n",
      "190 K     Total params\n",
      "0.760     Total estimated model params size (MB)\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T18:43:33.748767Z",
     "start_time": "2024-05-31T18:43:32.513104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "pred = trainer.test(model, datamodule=datamodule, ckpt_path=\"best\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at C:\\Users\\Johannes\\PycharmProjects\\damage-propagation-modeling\\lightning_logs\\version_4\\checkpoints\\epoch=64-step=8060.ckpt\n",
      "Loaded model weights from the checkpoint at C:\\Users\\Johannes\\PycharmProjects\\damage-propagation-modeling\\lightning_logs\\version_4\\checkpoints\\epoch=64-step=8060.ckpt\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T18:43:36.630187Z",
     "start_time": "2024-05-31T18:43:35.873817Z"
    }
   },
   "cell_type": "code",
   "source": "pred",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 382.58831787109375}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T18:43:06.686302Z",
     "start_time": "2024-05-31T18:43:06.046634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = CNNModel(lr=hyper_params[dataset_num-1]['learning_rate_init'], beta_1=hyper_params[dataset_num-1]['beta_1'], beta_2=hyper_params[dataset_num-1]['beta_2'], window_size=window_size, features=X_train.shape[1], dropout_rate=hyper_params[dataset_num-1]['dropout'])\n",
    "checkpoint = torch.load(\"models/cnn_dataset_3.ckpt\")\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "pred = model(torch.tensor(X_test)).detach().numpy()\n",
    "mse_cnn_3 = root_mean_squared_error(pred, torch.tensor(y_test))\n",
    "print(mse_cnn_3)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.1587"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
