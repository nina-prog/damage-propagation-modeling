{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T18:00:22.990987Z",
     "start_time": "2024-05-31T18:00:22.815492Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T18:00:23.192448Z",
     "start_time": "2024-05-31T18:00:23.004308Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Topic: EX2 - Turbofan RUL Prediction\n",
    "**Task**: Predict the remaining useful life (RUL) of turbofan engines based on given sensor data (time series data). It is a forcasting problem, where the goal is to predict the number of cycles an engine will last before it fails.\n",
    "**Data**: Turbofan engine degradation simulation data (NASA) - [Link](https://data.nasa.gov/dataset/Turbofan-Engine-Degradation-Simulation-Data-Set/vrks-gjie). See also in the topic [introduction notebook](https://github.com/nina-prog/damage-propagation-modeling/blob/2fb8c1a1102a48d7abbf04e4031807790a913a99/notebooks/Turbofan%20remaining%20useful%20life%20Prediction.ipynb).\n",
    "\n",
    "**Subtasks**:\n",
    "1. Perform a deep **exploratory data analysis (EDA)** on the given data.\n",
    "2. Implement a more efficient **sliding window method** for time series data analysis. -> üéØ **Focus on this task**\n",
    "3. Apply **traditional machine learning methods** (SOTA) to predict the remaining useful life. Includes data preparation, feature extraction, feature selection, model selection, and model parameter optimization.\n",
    "4. Create **neural network models** to predict the remaining useful life. Includes different architectures like Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), or Attention Models. Note: You can search for SOTA research papers and reproduce current state-of-the-art models.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports + Settings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# third-party libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import torch \n",
    "\n",
    "import time\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T18:00:29.219989Z",
     "start_time": "2024-05-31T18:00:23.526704Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "# source code\n",
    "from src.data_loading import load_data, load_config\n",
    "from src.data_splitting import train_val_split_by_group\n",
    "from src.nn_utils import create_sliding_window, create_sliding_window_test\n",
    "from src.rolling_window_creator import calculate_RUL\n",
    "from src.data_processing import apply_padding_on_train_data_and_test_data, drop_samples_with_clipped_values\n",
    "from src.nn_util.nn_models.ligthning.cnnModel1 import CNNModel1 as CNNModel\n",
    "from src.nn_util.datamodule.lightning.turbofanDatamodule import TurbofanDatamodule\n",
    "from src.data_cleaning import clean_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T18:00:30.082816Z",
     "start_time": "2024-05-31T18:00:29.219989Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "# settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"Set2\")\n",
    "sns.set(rc={\"figure.dpi\":100, 'savefig.dpi':200})\n",
    "sns.set_context('notebook')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T18:00:30.362408Z",
     "start_time": "2024-05-31T18:00:30.084475Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "np.random.seed(42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T18:00:30.650499Z",
     "start_time": "2024-05-31T18:00:30.364230Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Paths"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Make sure to execute this cell only once for one kernel session, before running any other cell below.\n",
    "os.chdir(\"../\") # set working directory to root of project\n",
    "os.getcwd() # check current working directory"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T18:00:30.951099Z",
     "start_time": "2024-05-31T18:00:30.653638Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Johannes\\\\PycharmProjects\\\\damage-propagation-modeling'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "PATH_TO_CONFIG = \"configs/config.yaml\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T18:00:31.264664Z",
     "start_time": "2024-05-31T18:00:30.953189Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Config + Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "config = load_config(PATH_TO_CONFIG) # config is dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T18:00:31.530381Z",
     "start_time": "2024-05-31T18:00:31.266949Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "dataset_num = 4\n",
    "train_data, test_data, test_RUL_data = load_data(config_path=PATH_TO_CONFIG, dataset_num=dataset_num)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T21:40:03.316404Z",
     "start_time": "2024-05-31T21:40:01.583137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-31 23:40:02 [\u001B[34msrc.data_loading:43\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Loading data set 4...\u001B[0m\n",
      "2024-05-31 23:40:03 [\u001B[34msrc.data_loading:72\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Loaded raw data for dataset 4.\u001B[0m\n",
      "2024-05-31 23:40:03 [\u001B[34msrc.data_loading:73\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Train Data: (61249, 26)\u001B[0m\n",
      "2024-05-31 23:40:03 [\u001B[34msrc.data_loading:74\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Test Data: (41214, 26)\u001B[0m\n",
      "2024-05-31 23:40:03 [\u001B[34msrc.data_loading:75\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Test RUL Data: (248, 1)\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 114
  },
  {
   "cell_type": "markdown",
   "source": "# Create Neural Regression Models",
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Pipeline:\n",
    "1.\tData Cleaning\n",
    "2.\tOptional: Padding\n",
    "3.\tCreate sliding windows\n",
    "4.\tSplit train data in validation and train data\n",
    "5.\tDrop some samples with the clipped value\n",
    "6.\tScale the Data\n",
    "7.\tFind the best hyperparameters\n",
    "8.\tCreate Model with found hyperparameters"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Explanation of selected hyperparameters:\n",
    "*\tWindow size: We selected a window size of 30 due to some experiments with other window sizes. Furthermore, the window size is also used in the paper from Mitici [1] which shows good results with a CNN architecture.\n",
    "*\tClipping value: The clipping value of 125 has been selected because it has proven useful and is used in paper [1] "
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "References:\n",
    "1.\tMihaela Mitici, Ingeborg de Pater, Anne Barros, Zhiguo Zeng, ‚ÄúDynamic predictive maintenance for multiple components using data-driven probabilistic RUL prognostics: The case of turbofan engines‚Äù, Reliability Engineering & System Safety, Volume 234, 2023, https://doi.org/10.1016/j.ress.2023.109199.\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T21:40:04.014310Z",
     "start_time": "2024-05-31T21:40:03.318492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# some hyperparameters\n",
    "time_column = 'Cycle'\n",
    "group_column = 'UnitNumber'\n",
    "\n",
    "window_size = 30\n",
    "clip_value = 125\n",
    "test_size = 0.1\n",
    "apply_data_cleaning = True\n",
    "# If activated, adds for every sensor a new column with the commutative sum of the peaks\n",
    "apply_peaks_generation = False\n",
    "\n",
    "# Apply scaler. The order in the list represents the order in which they are applied\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "robust_scaler = RobustScaler()\n",
    "scaler = [std_scaler, minmax_scaler, robust_scaler]"
   ],
   "outputs": [],
   "execution_count": 115
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Explanation of each step:\n",
    "- Data Cleaning\n",
    "    - The outlier detection and replacement method has been deactivated. \n",
    "    - The removal of columns based on the correlation of a single value has been deactivated because the neural model makes the feature selection.\n",
    "    - Features with a unique single value will be removed\n",
    "- Padding:\n",
    "    -\tOnly applied for the datasets with a sample in test or train data smaller than the window size.\n",
    "    -\tThe padding length is exactly the difference between the window size and the timesteps of the sample with the fewest timesteps\n",
    "    -\tThe padding is applied on all the time series\n",
    "- Create sliding window\n",
    "    -\tTODO: Explanation Frederik (falls du noch eine hinzuf√ºgen m√∂chtest)\n",
    "- Split train data in validation and train data\n",
    "    -\tSplitting training and validation sets based on the UnitNumber\n",
    "- Drop some samples with the clipped value:\n",
    "    -\tTo make the data more evenly distributed, in this step some of the samples with the clipping value as RUL are removed \n",
    "    -\tTherefore, the median of the frequency of other RUL values is computed and the number of samples with the clipping value is a multiple of the median. \n",
    "    -\tWe selected two to not drop to many samples\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T21:40:27.867421Z",
     "start_time": "2024-05-31T21:40:04.014310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if apply_data_cleaning:\n",
    "    train_data, test_data = clean_data(train_data, test_data, method=None, ignore_columns=['UnitNumber', 'Cycle'], threshold_missing=0.1, threshold_corr=0.0, contamination=0.05)\n",
    "    \n",
    "# Add column RUL to train_data\n",
    "train_data = calculate_RUL(train_data, time_column, group_column, clip_value)\n",
    "\n",
    "train_data, test_data = apply_padding_on_train_data_and_test_data(train_data=train_data, test_data=test_data, window_size=window_size)\n",
    "\n",
    "train, val = train_val_split_by_group(train_data, test_size=test_size, random_state=12)\n",
    "\n",
    "X_train, y_train = create_sliding_window(train, window_size=window_size)  #, drop_columns=['UnitNumber', 'Cycle, 'RUL'])\n",
    "X_val, y_val = create_sliding_window(val, window_size=window_size)  #, drop_columns=['UnitNumber', 'Cycle', 'RUL'])\n",
    "X_test, _ = create_sliding_window_test(test_data, column_RUL=False, drop_columns=['UnitNumber'])\n",
    "y_test = test_RUL_data.values\n",
    "\n",
    "X_train, y_train = drop_samples_with_clipped_values(X_train, y_train, clip_value)\n",
    "X_val, y_val = drop_samples_with_clipped_values(X_val, y_val, clip_value)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-31 23:40:04 [\u001B[34msrc.data_cleaning:134\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Cleaning train and test data...\u001B[0m\n",
      "2024-05-31 23:40:04 [\u001B[34msrc.data_cleaning:136\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Formatting column types...\u001B[0m\n",
      "2024-05-31 23:40:04 [\u001B[34msrc.data_cleaning:69\u001B[0m] [DEBUG\u001B[0m] >>>> Found 0 categorical columns: []\u001B[0m\n",
      "2024-05-31 23:40:04 [\u001B[34msrc.data_cleaning:69\u001B[0m] [DEBUG\u001B[0m] >>>> Found 0 categorical columns: []\u001B[0m\n",
      "2024-05-31 23:40:04 [\u001B[34msrc.data_cleaning:141\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Handling duplicates...\u001B[0m\n",
      "2024-05-31 23:40:04 [\u001B[34msrc.data_cleaning:146\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Removing outliers...\u001B[0m\n",
      "2024-05-31 23:40:04 [\u001B[34msrc.outlier_detection:150\u001B[0m] [DEBUG\u001B[0m] >>>> Removing outliers using method: None ...\u001B[0m\n",
      "2024-05-31 23:40:04 [\u001B[34msrc.outlier_detection:162\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001B[0m\n",
      "2024-05-31 23:40:04 [\u001B[34msrc.outlier_detection:150\u001B[0m] [DEBUG\u001B[0m] >>>> Removing outliers using method: None ...\u001B[0m\n",
      "2024-05-31 23:40:04 [\u001B[34msrc.outlier_detection:162\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001B[0m\n",
      "2024-05-31 23:40:04 [\u001B[34msrc.data_cleaning:150\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Filter features based train data...\u001B[0m\n",
      "2024-05-31 23:40:04 [\u001B[34msrc.data_cleaning:26\u001B[0m] [DEBUG\u001B[0m] >>>> Found 0 features with missing values above the threshold of 0.1.\u001B[0m\n",
      "2024-05-31 23:40:05 [\u001B[34msrc.data_cleaning:46\u001B[0m] [DEBUG\u001B[0m] >>>> Found 0 features with only a single unique value: []\u001B[0m\n",
      "2024-05-31 23:40:05 [\u001B[34msrc.data_cleaning:103\u001B[0m] [DEBUG\u001B[0m] >>>> Found 0 uncorrelated features with a correlation threshold of 0.0: []\u001B[0m\n",
      "2024-05-31 23:40:05 [\u001B[34msrc.data_cleaning:162\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Dropping features based on missing values, single unique values, and no target correlation...\u001B[0m\n",
      "2024-05-31 23:40:05 [\u001B[34msrc.data_cleaning:172\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Data cleaning completed.\u001B[0m\n",
      "2024-05-31 23:40:05 [\u001B[34msrc.data_cleaning:173\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Original train DataFrame shape: (61249, 26), Resulting train DataFrame shape: (61249, 26)\u001B[0m\n",
      "2024-05-31 23:40:05 [\u001B[34msrc.data_cleaning:174\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Original test DataFrame shape: (41214, 26), Resulting test DataFrame shape: (41214, 26)\u001B[0m\n",
      "2024-05-31 23:40:05 [\u001B[34msrc.data_processing:38\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The minimum number of cycles of a motor before the padding is 19.\u001B[0m\n",
      "2024-05-31 23:40:05 [\u001B[34msrc.data_processing:44\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The padding value is 11.\u001B[0m\n",
      "2024-05-31 23:40:07 [\u001B[34msrc.data_processing:53\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The minimum number of cycles of a motor with the padding is 19.\u001B[0m\n",
      "2024-05-31 23:40:07 [\u001B[34msrc.data_splitting:47\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Train set contains 224 different engines --> in total 57320\u001B[0m\n",
      "2024-05-31 23:40:07 [\u001B[34msrc.data_splitting:48\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>>  Test set contains 25 different engines --> in total 6668\u001B[0m\n",
      "2024-05-31 23:40:27 [\u001B[34msrc.data_processing:111\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The median of the frequency of each RUL value in the data is 224.0.\u001B[0m\n",
      "2024-05-31 23:40:27 [\u001B[34msrc.data_processing:112\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The the frequency of the clipped RUL value in the data is 23084.\u001B[0m\n",
      "2024-05-31 23:40:27 [\u001B[34msrc.data_processing:118\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The the frequency of the clipped RUL value after dropping is 448.\u001B[0m\n",
      "2024-05-31 23:40:27 [\u001B[34msrc.data_processing:126\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The number of samples in the data has dropped from 50824 to 28188.\u001B[0m\n",
      "2024-05-31 23:40:27 [\u001B[34msrc.data_processing:111\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The median of the frequency of each RUL value in the data is 25.0.\u001B[0m\n",
      "2024-05-31 23:40:27 [\u001B[34msrc.data_processing:112\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The the frequency of the clipped RUL value in the data is 2843.\u001B[0m\n",
      "2024-05-31 23:40:27 [\u001B[34msrc.data_processing:118\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The the frequency of the clipped RUL value after dropping is 50.\u001B[0m\n",
      "2024-05-31 23:40:27 [\u001B[34msrc.data_processing:126\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The number of samples in the data has dropped from 5943 to 3150.\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Scale the data\n",
    "*\tThe applied scalers are the StandardScaler, the MinMaxScaler, and the RobustScaler \n",
    "*\tThese three scalers have been selected because the training has been most robust with them\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T21:40:32.922353Z",
     "start_time": "2024-05-31T21:40:27.871287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Note: Do not normalize the cycle value! That is why we start with one\n",
    "for single_scaler in scaler:\n",
    "    for i in range(1, X_train.shape[-1]):\n",
    "        X_train[:, :, i] = single_scaler.fit_transform(X_train[:, :, i])\n",
    "        X_val[:, :, i] = single_scaler.transform(X_val[:, :, i])\n",
    "        X_test[:, :, i] = single_scaler.transform(X_test[:, :, i])"
   ],
   "outputs": [],
   "execution_count": 117
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Change data types of arrays to float32 and swap axes if necessary:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T21:40:33.435343Z",
     "start_time": "2024-05-31T21:40:32.922353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(X_train.shape)\n",
    "X_train = np.swapaxes(X_train, 1, 2)\n",
    "X_train = np.array(X_train, dtype=np.float32)\n",
    "y_train = np.array(y_train, dtype=np.float32)\n",
    "print(X_train.shape)\n",
    "\n",
    "print(X_val.shape)\n",
    "X_val = np.swapaxes(X_val, 1, 2)\n",
    "X_val = np.array(X_val, dtype=np.float32)\n",
    "y_val = np.array(y_val, dtype=np.float32)\n",
    "print(X_val.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "X_test = np.swapaxes(X_test, 1, 2)\n",
    "X_test = np.array(X_test, dtype=np.float32)\n",
    "y_test = np.array(y_test, dtype=np.float32)\n",
    "print(X_test.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28188, 30, 25)\n",
      "(28188, 25, 30)\n",
      "(3150, 30, 25)\n",
      "(3150, 25, 30)\n",
      "(248, 30, 25)\n",
      "(248, 25, 30)\n"
     ]
    }
   ],
   "execution_count": 118
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Save processed test data"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T21:40:33.968428Z",
     "start_time": "2024-05-31T21:40:33.435343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "save_test_data = False\n",
    "if save_test_data:\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    np.save(f\"{config['paths']['processed_data_dir']}ex2_preprocessed_X_test_from_dataset_{dataset_num}_for_CNNModel1_{timestamp}.npy\", X_test)\n",
    "    np.save(f\"{config['paths']['processed_data_dir']}ex2_preprocessed_y_test_from_dataset_{dataset_num}_for_CNNModel1_{timestamp}.npy\", y_test)"
   ],
   "outputs": [],
   "execution_count": 119
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CNN"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Architecture\n",
    "*\tThe architecture of the first CNN model (‚ÄúExampleCNNModel‚Äù) is a minimalistic approach with only two convolutional layers and some fully connected layers \n",
    "*\tThe second CNN model uses more convolutional layers and one fully connected layers more\n",
    "*\tMore convolutional layers are used to be more like the architecture from the paper from Mitici [1]\n",
    "*\tBoth architectures use only 1D convolutional layers as is done in the paper [1]\n",
    "*\tBoth use dropout to enable generalization and prevent overfitting\n",
    "*\tAdam is used as an optimizer and the mean squared error as a loss function\n",
    "*\tBecause the possible targets are higher or equal to one in the second CNN the max function with one is applied on the output.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Hyperparameter search\n",
    "*\tThe best hyperparameters are found with Bayesian Optimization\n",
    "*\tFor each dataset a new set of hyperparameters has been searched\n",
    "*\tThe search has been done on the SCC JupyterHub and to parallelize the computation for each data set a separate Notebook has been created\n",
    "*\tThe notebooks are stored in the ‚Äúnotebooks/cnn_hyperparameter_search‚Äù folder\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T21:32:31.818849Z",
     "start_time": "2024-05-31T21:32:31.019149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hyper_params = [{'batch_size': 114.84809532072403, 'beta_1': 0.9586517323123119, 'beta_2': 0.9558431375026947, 'dropout': 0.021025382021542985, 'learning_rate_init': 0.01}, \n",
    "                {}, \n",
    "                {'batch_size': 92.4798215637139, 'beta_1': 0.9635139876762263, 'beta_2': 0.9432583039935667, 'dropout': 0.2119494320551308, 'learning_rate_init': 0.0004461791916105841}, \n",
    "                {},\n",
    "                ]\n",
    "\n",
    "seeds = [21, 21, 21, 21]"
   ],
   "outputs": [],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T20:43:15.407462Z",
     "start_time": "2024-05-31T20:43:14.811399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pl.seed_everything(seeds[dataset_num-1])\n",
    "\n",
    "# Select hyperparameters of trainer!\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\")\n",
    "trainer = Trainer(min_epochs=1, max_epochs=150, callbacks=[checkpoint_callback], deterministic=True)\n",
    "datamodule = TurbofanDatamodule(batch_size=int(hyper_params[dataset_num-1]['batch_size']))\n",
    "datamodule.set_train_dataset(X_train, y_train)\n",
    "datamodule.set_val_dataset(X_val, y_val)\n",
    "datamodule.set_predict_dataset(X_test)\n",
    "datamodule.set_test_dataset(X_test, y_test[:, 0])\n",
    "model = CNNModel(lr=hyper_params[dataset_num-1]['learning_rate_init'], beta_1=hyper_params[dataset_num-1]['beta_1'], beta_2=hyper_params[dataset_num-1]['beta_2'], window_size=window_size, features=X_train.shape[1], dropout_rate=hyper_params[dataset_num-1]['dropout'])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 21\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T20:52:39.194871Z",
     "start_time": "2024-05-31T20:43:15.409656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "# For visualization write 'tensorboard --logdir=lightning_logs/' in console\n",
    "\n",
    "trainer.fit(model, datamodule=datamodule)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name        | Type    | Params\n",
      "----------------------------------------\n",
      "0 | loss        | MSELoss | 0     \n",
      "1 | dropout     | Dropout | 0     \n",
      "2 | layer1_conv | Conv1d  | 3.6 K \n",
      "3 | layer2_conv | Conv1d  | 8.0 K \n",
      "4 | layer3_conv | Conv1d  | 8.0 K \n",
      "5 | layer4_conv | Conv1d  | 8.0 K \n",
      "6 | fc1         | Linear  | 153 K \n",
      "7 | fc2         | Linear  | 8.3 K \n",
      "8 | fc3         | Linear  | 65    \n",
      "----------------------------------------\n",
      "189 K     Trainable params\n",
      "0         Non-trainable params\n",
      "189 K     Total params\n",
      "0.759     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=150` reached.\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T20:52:40.966525Z",
     "start_time": "2024-05-31T20:52:39.196186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "pred = trainer.test(model, datamodule=datamodule, ckpt_path=\"best\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at C:\\Users\\Johannes\\PycharmProjects\\damage-propagation-modeling\\lightning_logs\\version_0\\checkpoints\\epoch=125-step=12474.ckpt\n",
      "Loaded model weights from the checkpoint at C:\\Users\\Johannes\\PycharmProjects\\damage-propagation-modeling\\lightning_logs\\version_0\\checkpoints\\epoch=125-step=12474.ckpt\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T20:52:55.847674Z",
     "start_time": "2024-05-31T20:52:55.185801Z"
    }
   },
   "cell_type": "code",
   "source": "pred",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 286.5386657714844}]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Scores on all testsets "
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T21:39:28.451998Z",
     "start_time": "2024-05-31T21:39:27.855696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_test_data = []\n",
    "paths = [\n",
    "    ('data/processed/ex2_preprocessed_X_test_from_dataset_1_for_CNNModel1_20240531-232248.npy', 'data/processed/ex2_preprocessed_y_test_from_dataset_1_for_CNNModel1_20240531-232248.npy'),\n",
    "    ('data/processed/ex2_preprocessed_X_test_from_dataset_2_for_CNNModel1_20240531-233230.npy', 'data/processed/ex2_preprocessed_y_test_from_dataset_2_for_CNNModel1_20240531-233230.npy'),\n",
    "    ('data/processed/ex2_preprocessed_X_test_from_dataset_3_for_CNNModel1_20240531-232732.npy', 'data/processed/ex2_preprocessed_y_test_from_dataset_3_for_CNNModel1_20240531-232732.npy'),\n",
    "    ('data/processed/ex2_preprocessed_X_test_from_dataset_4_for_CNNModel1_20240531-234033.npy', 'data/processed/ex2_preprocessed_y_test_from_dataset_4_for_CNNModel1_20240531-234033.npy'),\n",
    "]\n",
    "for i in range(len(paths)):\n",
    "    X_temp = np.load(paths[i][0])\n",
    "    y_temp = np.load(paths[i][1])\n",
    "    all_test_data.append((X_temp, y_temp))"
   ],
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T21:39:29.132747Z",
     "start_time": "2024-05-31T21:39:28.454281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_num_temp = 1\n",
    "model = CNNModel(lr=hyper_params[dataset_num_temp-1]['learning_rate_init'], beta_1=hyper_params[dataset_num_temp-1]['beta_1'], beta_2=hyper_params[dataset_num_temp-1]['beta_2'], window_size=window_size, features=all_test_data[dataset_num_temp-1][0].shape[1], dropout_rate=hyper_params[dataset_num_temp-1]['dropout'])\n",
    "checkpoint = torch.load(\"models/cnn_dataset_1.ckpt\")\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "pred = model(torch.tensor(all_test_data[dataset_num_temp-1][0])).detach().numpy()\n",
    "rmse_cnn_1 = root_mean_squared_error(pred, torch.tensor(all_test_data[dataset_num_temp-1][1]))\n",
    "print(f'The RMSE score on dataset FD00{dataset_num_temp} is {rmse_cnn_1}.')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE score on dataset FD001 is 16.927452087402344.\n"
     ]
    }
   ],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T21:39:29.726834Z",
     "start_time": "2024-05-31T21:39:29.134252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_num_temp = 3\n",
    "model = CNNModel(lr=hyper_params[dataset_num_temp-1]['learning_rate_init'], beta_1=hyper_params[dataset_num_temp-1]['beta_1'], beta_2=hyper_params[dataset_num_temp-1]['beta_2'], window_size=window_size, features=all_test_data[dataset_num_temp-1][0].shape[1], dropout_rate=hyper_params[dataset_num_temp-1]['dropout'])\n",
    "checkpoint = torch.load(\"models/cnn_dataset_3.ckpt\")\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "pred = model(torch.tensor(all_test_data[dataset_num_temp-1][0])).detach().numpy()\n",
    "rmse_cnn_3 = root_mean_squared_error(pred, torch.tensor(all_test_data[dataset_num_temp-1][1]))\n",
    "print(f'The RMSE score on dataset FD00{dataset_num_temp} is {rmse_cnn_3}.')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE score on dataset FD003 is 19.158700942993164.\n"
     ]
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
