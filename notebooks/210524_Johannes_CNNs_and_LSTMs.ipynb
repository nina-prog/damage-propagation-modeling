{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-30T14:59:40.484406Z",
     "start_time": "2024-05-30T14:59:40.300966Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-30T14:59:40.673396Z",
     "start_time": "2024-05-30T14:59:40.486920Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Topic: EX2 - Turbofan RUL Prediction\n",
    "**Task**: Predict the remaining useful life (RUL) of turbofan engines based on given sensor data (time series data). It is a forcasting problem, where the goal is to predict the number of cycles an engine will last before it fails.\n",
    "**Data**: Turbofan engine degradation simulation data (NASA) - [Link](https://data.nasa.gov/dataset/Turbofan-Engine-Degradation-Simulation-Data-Set/vrks-gjie). See also in the topic [introduction notebook](https://github.com/nina-prog/damage-propagation-modeling/blob/2fb8c1a1102a48d7abbf04e4031807790a913a99/notebooks/Turbofan%20remaining%20useful%20life%20Prediction.ipynb).\n",
    "\n",
    "**Subtasks**:\n",
    "1. Perform a deep **exploratory data analysis (EDA)** on the given data.\n",
    "2. Implement a more efficient **sliding window method** for time series data analysis. -> ğŸ¯ **Focus on this task**\n",
    "3. Apply **traditional machine learning methods** (SOTA) to predict the remaining useful life. Includes data preparation, feature extraction, feature selection, model selection, and model parameter optimization.\n",
    "4. Create **neural network models** to predict the remaining useful life. Includes different architectures like Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), or Attention Models. Note: You can search for SOTA research papers and reproduce current state-of-the-art models.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports + Settings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# third-party libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import time\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-30T14:59:45.684683Z",
     "start_time": "2024-05-30T14:59:40.993491Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "# source code\n",
    "from src.utils import load_data, load_config, train_val_split_by_group\n",
    "from src.nn_utils import scale_data, create_sliding_window\n",
    "from src.nn_util.nn_models.ligthning.cnnModel2 import CNNModel2 as CNNModel\n",
    "from src.nn_util.nn_models.ligthning.cnnModel1 import CNNModel1 as CNNModel1\n",
    "from src.nn_util.nn_models.ligthning.exampleLSTMModel import ExampleLSTMModel as LSTMModel\n",
    "from src.nn_util.datamodule.lightning.turbofanDatamodule import TurbofanDatamodule\n",
    "from src.data_cleaning import clean_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-30T14:59:46.710323Z",
     "start_time": "2024-05-30T14:59:45.686796Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "# settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"Set2\")\n",
    "sns.set(rc={\"figure.dpi\":100, 'savefig.dpi':200})\n",
    "sns.set_context('notebook')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-30T14:59:46.943051Z",
     "start_time": "2024-05-30T14:59:46.712331Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "np.random.seed(42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-30T14:59:47.160052Z",
     "start_time": "2024-05-30T14:59:46.944758Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Paths"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Make sure to execute this cell only once for one kernel session, before running any other cell below.\n",
    "os.chdir(\"../\") # set working directory to root of project\n",
    "os.getcwd() # check current working directory"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-30T14:59:47.391365Z",
     "start_time": "2024-05-30T14:59:47.162054Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Johannes\\\\PycharmProjects\\\\damage-propagation-modeling'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "PATH_TO_CONFIG = \"configs/config.yaml\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-30T14:59:47.621388Z",
     "start_time": "2024-05-30T14:59:47.393366Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Config + Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "config = load_config(PATH_TO_CONFIG) # config is dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-30T14:59:47.839618Z",
     "start_time": "2024-05-30T14:59:47.623389Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": "train_data, test_data, test_RUL_data = load_data(config_path=PATH_TO_CONFIG, dataset_num=1)",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-30T15:34:54.977206Z",
     "start_time": "2024-05-30T15:34:54.088657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-30 17:34:54 [\u001B[34msrc.utils:60\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Loading data set 1...\u001B[0m\n",
      "2024-05-30 17:34:54 [\u001B[34msrc.utils:89\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Loaded raw data for dataset 1.\u001B[0m\n",
      "2024-05-30 17:34:54 [\u001B[34msrc.utils:90\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Train Data: (20631, 26)\u001B[0m\n",
      "2024-05-30 17:34:54 [\u001B[34msrc.utils:91\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Test Data: (13096, 26)\u001B[0m\n",
      "2024-05-30 17:34:54 [\u001B[34msrc.utils:92\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Test RUL Data: (100, 1)\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ğŸ“ << Subtask X: TOPIC >>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[TEMPLATE]\n",
    "\n",
    "Findings:\n",
    "* Interpretation of plots\n",
    "* or other key take aways from previous code"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# [TEMPLATE] - save processed data (as pickle)\n",
    "df = pd.DataFrame()\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "df.to_pickle(f\"{config['paths']['processed_data_dir']}ex2_topic_{timestamp}.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-30T15:19:43.570780Z",
     "start_time": "2024-05-30T15:19:42.970476Z"
    }
   },
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "source": [
    "# [TEMPLATE] - save data predictions (as csv)\n",
    "df = pd.DataFrame()\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "df.to_csv(f\"{config['paths']['prediction_dir']}ex2_topic_{timestamp}.csv\", sep=',', decimal='.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-30T15:19:44.146497Z",
     "start_time": "2024-05-30T15:19:43.573786Z"
    }
   },
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 900x600 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [TEMPLATE] - save plot results (as png)\n",
    "fig = plt.figure(figsize=(9, 6))\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "fig.savefig(f\"{config['paths']['plot_dir']}ex2_topic_{timestamp}.png\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:39.485116500Z",
     "start_time": "2024-05-09T15:31:39.320350800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:39.522027500Z",
     "start_time": "2024-05-09T15:31:39.474058600Z"
    }
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T15:19:46.874807Z",
     "start_time": "2024-05-30T15:19:46.355659Z"
    }
   },
   "cell_type": "code",
   "source": "window_size = 30",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T15:19:47.484755Z",
     "start_time": "2024-05-30T15:19:46.878062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setting the seed\n",
    "pl.seed_everything(21)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T15:19:47.932630Z",
     "start_time": "2024-05-30T15:19:47.486907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#scoring: Dict[str, make_scorer] = {\n",
    "#    'mae': make_scorer(mean_absolute_error),\n",
    "#    'mse': make_scorer(mean_squared_error),\n",
    "#    'r2': make_scorer(r2_score)\n",
    "#}"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T15:34:58.931346Z",
     "start_time": "2024-05-30T15:34:58.297114Z"
    }
   },
   "cell_type": "code",
   "source": "cleaned_train, cleaned_test = clean_data(train_data, test_data, method=None, ignore_columns=['UnitNumber', 'Cycle'], threshold_missing=0.1, threshold_corr=0.0, contamination=0.05)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-30 17:34:58 [\u001B[34msrc.data_cleaning:134\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Cleaning train and test data...\u001B[0m\n",
      "2024-05-30 17:34:58 [\u001B[34msrc.data_cleaning:136\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Formatting column types...\u001B[0m\n",
      "2024-05-30 17:34:58 [\u001B[34msrc.data_cleaning:69\u001B[0m] [DEBUG\u001B[0m] >>>> Found 0 categorical columns: []\u001B[0m\n",
      "2024-05-30 17:34:58 [\u001B[34msrc.data_cleaning:69\u001B[0m] [DEBUG\u001B[0m] >>>> Found 0 categorical columns: []\u001B[0m\n",
      "2024-05-30 17:34:58 [\u001B[34msrc.data_cleaning:141\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Handling duplicates...\u001B[0m\n",
      "2024-05-30 17:34:58 [\u001B[34msrc.data_cleaning:146\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Removing outliers...\u001B[0m\n",
      "2024-05-30 17:34:58 [\u001B[34msrc.outlier_detection:150\u001B[0m] [DEBUG\u001B[0m] >>>> Removing outliers using method: None ...\u001B[0m\n",
      "2024-05-30 17:34:58 [\u001B[34msrc.outlier_detection:162\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001B[0m\n",
      "2024-05-30 17:34:58 [\u001B[34msrc.outlier_detection:150\u001B[0m] [DEBUG\u001B[0m] >>>> Removing outliers using method: None ...\u001B[0m\n",
      "2024-05-30 17:34:58 [\u001B[34msrc.outlier_detection:162\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001B[0m\n",
      "2024-05-30 17:34:58 [\u001B[34msrc.data_cleaning:150\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Filter features based train data...\u001B[0m\n",
      "2024-05-30 17:34:58 [\u001B[34msrc.data_cleaning:26\u001B[0m] [DEBUG\u001B[0m] >>>> Found 0 features with missing values above the threshold of 0.1.\u001B[0m\n",
      "2024-05-30 17:34:58 [\u001B[34msrc.data_cleaning:46\u001B[0m] [DEBUG\u001B[0m] >>>> Found 7 features with only a single unique value: ['Operation Setting 3', 'Sensor Measure 1', 'Sensor Measure 5', 'Sensor Measure 10', 'Sensor Measure 16', 'Sensor Measure 18', 'Sensor Measure 19']\u001B[0m\n",
      "2024-05-30 17:34:58 [\u001B[34msrc.data_cleaning:103\u001B[0m] [DEBUG\u001B[0m] >>>> Found 0 uncorrelated features with a correlation threshold of 0.0: []\u001B[0m\n",
      "2024-05-30 17:34:58 [\u001B[34msrc.data_cleaning:162\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Dropping features based on missing values, single unique values, and no target correlation...\u001B[0m\n",
      "2024-05-30 17:34:58 [\u001B[34msrc.data_cleaning:172\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Data cleaning completed.\u001B[0m\n",
      "2024-05-30 17:34:58 [\u001B[34msrc.data_cleaning:173\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Original train DataFrame shape: (20631, 19), Resulting train DataFrame shape: (20631, 19)\u001B[0m\n",
      "2024-05-30 17:34:58 [\u001B[34msrc.data_cleaning:174\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Original test DataFrame shape: (13096, 19), Resulting test DataFrame shape: (13096, 19)\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T15:34:59.462995Z",
     "start_time": "2024-05-30T15:34:58.934342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "apply_data_cleaning = True\n",
    "if apply_data_cleaning:\n",
    "    train_data = cleaned_train\n",
    "    test_data = cleaned_test"
   ],
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T15:35:01.261600Z",
     "start_time": "2024-05-30T15:34:59.463994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scipy.signal import find_peaks\n",
    "\n",
    "def extract_peaks_from_sensor_signal(dataframe, sensor_measure_columns_names):\n",
    "    motor_ids = dataframe['UnitNumber'].unique()\n",
    "    # TODO: Rename\n",
    "    dataframes_motor_id = []\n",
    "    for motor_id in motor_ids:\n",
    "        dataframe_motor_id = pd.DataFrame(dataframe[dataframe['UnitNumber'] == motor_id])\n",
    "        for sensor_measure_column_name in sensor_measure_columns_names:\n",
    "            values = dataframe[dataframe['UnitNumber'] == motor_id][sensor_measure_column_name].values\n",
    "            peaks = find_peaks(values)\n",
    "            count_of_peaks = np.zeros(values.shape)\n",
    "            count_of_peaks[peaks[0]] = 1\n",
    "            count_of_peaks = np.cumsum(count_of_peaks)\n",
    "            dataframe_motor_id[sensor_measure_column_name + ' Peaks'] = count_of_peaks\n",
    "        dataframes_motor_id.append(dataframe_motor_id)\n",
    "    return pd.concat(dataframes_motor_id)\n",
    "\n",
    "sensor_measure_columns_names = list(filter(lambda x: x.startswith('Sensor'), train_data.keys()))\n",
    "train_data_peaks = extract_peaks_from_sensor_signal(train_data, sensor_measure_columns_names)\n",
    "test_data_peaks = extract_peaks_from_sensor_signal(test_data, sensor_measure_columns_names)"
   ],
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T15:35:01.791544Z",
     "start_time": "2024-05-30T15:35:01.264602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "apply_peaks_generation = False\n",
    "if apply_data_cleaning:\n",
    "    train_data = train_data_peaks\n",
    "    test_data = test_data_peaks"
   ],
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T15:35:02.293956Z",
     "start_time": "2024-05-30T15:35:01.793553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add RUL to train datasets\n",
    "from src.rolling_window_creator import calculate_RUL\n",
    "\n",
    "time_column = 'Cycle'\n",
    "group_column = 'UnitNumber'\n",
    "\n",
    "train_data = calculate_RUL(train_data, time_column, group_column)"
   ],
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T15:35:03.064616Z",
     "start_time": "2024-05-30T15:35:02.296951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Padding of train_data and test_data\n",
    "min_train = min([values.shape[0] for values in train_data.groupby(\"UnitNumber\").indices.values()])\n",
    "min_test = min([values.shape[0] for values in test_data.groupby(\"UnitNumber\").indices.values()])\n",
    "print(min(min_train, min_test))\n",
    "\n",
    "padding = 0\n",
    "if window_size > min(min_train, min_test):\n",
    "    padding = window_size - min(min_train, min_test)\n",
    "print(padding)\n",
    "\n",
    "# Padding of train_data\n",
    "for engine_number in train_data[\"UnitNumber\"].unique():\n",
    "    padding_data_frame = pd.DataFrame(np.zeros(shape=(padding, len(train_data.columns))), columns=train_data.columns)\n",
    "    padding_data_frame[\"UnitNumber\"] = engine_number\n",
    "    padding_data_frame[\"Cycle\"] = -1\n",
    "    train_data = pd.concat([train_data, padding_data_frame])\n",
    "    \n",
    "# Padding of test_data\n",
    "for engine_number in test_data[\"UnitNumber\"].unique():\n",
    "    padding_data_frame = pd.DataFrame(np.zeros(shape=(padding, len(test_data.columns))), columns=test_data.columns)\n",
    "    padding_data_frame[\"UnitNumber\"] = engine_number\n",
    "    padding_data_frame[\"Cycle\"] = -1\n",
    "    test_data = pd.concat([test_data, padding_data_frame])\n",
    "\n",
    "# Sort to assure the correct processing\n",
    "train_data.sort_values(['UnitNumber', 'Cycle'], inplace=True, ascending=True)\n",
    "test_data.sort_values(['UnitNumber', 'Cycle'], inplace=True, ascending=True)\n",
    "\n",
    "min_train = min([values.shape[0] for values in train_data.groupby(\"UnitNumber\").indices.values()])\n",
    "min_test = min([values.shape[0] for values in test_data.groupby(\"UnitNumber\").indices.values()])\n",
    "print(min(min_train, min_test))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "0\n",
      "31\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T15:35:03.748496Z",
     "start_time": "2024-05-30T15:35:03.065734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_list = []\n",
    "for engine_number in test_data[\"UnitNumber\"].unique():\n",
    "    rows_of_engine = test_data[\"UnitNumber\"] == engine_number\n",
    "    engine_dataframe = test_data[rows_of_engine].tail(window_size)\n",
    "    #engine_dataframe = engine_dataframe.drop(columns=[\"UnitNumber\", \"Cycle\"])\n",
    "    engine_dataframe = engine_dataframe.drop(columns=[\"UnitNumber\"])\n",
    "    test_list.append(engine_dataframe.values)\n",
    "    \n",
    "X_test = np.array(test_list)\n",
    "y_test = test_RUL_data.values"
   ],
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T15:35:04.202539Z",
     "start_time": "2024-05-30T15:35:03.750495Z"
    }
   },
   "cell_type": "code",
   "source": "train, val = train_val_split_by_group(train_data, test_size=0.2, random_state=12)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-30 17:35:04 [\u001B[34msrc.utils:131\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Train set contains 80 different engines --> in total 16528\u001B[0m\n",
      "2024-05-30 17:35:04 [\u001B[34msrc.utils:132\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>>  Test set contains 20 different engines --> in total 4103\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T15:35:09.617960Z",
     "start_time": "2024-05-30T15:35:04.203541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train, y_train = create_sliding_window(train, window_size=window_size, drop_columns=['UnitNumber', 'RUL'])\n",
    "X_val, y_val = create_sliding_window(val, window_size=window_size, drop_columns=['UnitNumber', 'RUL'])"
   ],
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T15:35:10.073524Z",
     "start_time": "2024-05-30T15:35:09.620202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "select_part_of_data_1 = True\n",
    "\n",
    "size_compared_to_other_values = 2\n",
    "if select_part_of_data_1:\n",
    "    # Drop some samples with clip value \n",
    "    clip_value = 140\n",
    "    count_train_value = []\n",
    "    count_val_value = []\n",
    "    for i in range(1, clip_value + 1):\n",
    "        count_train_value.append(np.sum(y_train == i))\n",
    "        count_val_value.append(np.sum(y_val == i))\n",
    "        \n",
    "    print(count_train_value)\n",
    "    print(count_val_value)\n",
    "    \n",
    "    indices_train_clip_value = np.arange(y_train.shape[0])[y_train == clip_value]\n",
    "    indices_val_clip_value = np.arange(y_val.shape[0])[y_val == clip_value]\n",
    "    \n",
    "    np.random.seed(63)\n",
    "    np.random.shuffle(indices_train_clip_value)\n",
    "    np.random.shuffle(indices_val_clip_value)\n",
    "    \n",
    "    median_train = np.median(count_train_value[:-1])\n",
    "    median_val = np.median(count_val_value[:-1])\n",
    "    \n",
    "    indices_train_clip_value = indices_val_clip_value[:int(median_train) * size_compared_to_other_values]\n",
    "    indices_val_clip_value = indices_val_clip_value[:int(median_val) * size_compared_to_other_values]\n",
    "    \n",
    "    print(indices_train_clip_value.shape)\n",
    "    print(indices_val_clip_value.shape)\n",
    "    \n",
    "    new_indices_train = np.arange(y_train.shape[0])[y_train != clip_value]\n",
    "    new_indices_val = np.arange(y_val.shape[0])[y_val != clip_value]\n",
    "    new_indices_train = np.concatenate((new_indices_train, indices_train_clip_value))\n",
    "    new_indices_val = np.concatenate((new_indices_val, indices_val_clip_value))\n",
    "    print(new_indices_train.shape)\n",
    "    print(new_indices_val.shape)\n",
    "    \n",
    "    np.random.shuffle(new_indices_train)\n",
    "    np.random.shuffle(new_indices_val)\n",
    "    \n",
    "    # Get selected samples\n",
    "    select_num_train_samples = 80 * 4\n",
    "    select_num_val_samples = 20 * 4\n",
    "    X_train = X_train[new_indices_train]\n",
    "    y_train = y_train[new_indices_train]\n",
    "    X_val = X_val[new_indices_val]\n",
    "    y_val = y_val[new_indices_val]\n",
    "\n",
    "select_part_of_data_2 = False\n",
    "    \n",
    "keep_from_motor = 20\n",
    "if select_part_of_data_2:\n",
    "    # Select from each UnitNumber some samples\n",
    "    indicis_for_train = []\n",
    "    indicis_for_val = []\n",
    "    for motor_id in train_data['UnitNumber'].unique():\n",
    "        if motor_id in X_train[:, 0, 0]:\n",
    "            indices_with_motor_id = np.arange(X_train.shape[0])[X_train[:, 0, 0] == motor_id] \n",
    "            indicis_for_train.append(indices_with_motor_id[:keep_from_motor])\n",
    "        else:\n",
    "            indices_with_motor_id = np.arange(X_val.shape[0])[X_val[:, 0, 0] == motor_id] \n",
    "            indicis_for_val.append(indices_with_motor_id[:keep_from_motor])\n",
    "        \n",
    "    indicis_for_train = np.concatenate(indicis_for_train)\n",
    "    indicis_for_val = np.concatenate(indicis_for_val)\n",
    "    \n",
    "    # Select new indices and remove UnitNumber\n",
    "    X_train = X_train[indicis_for_train][:, :, 1:]\n",
    "    y_train = y_train[indicis_for_train]\n",
    "    #X_val = X_val[indicis_for_val][:, :, 1:]\n",
    "    X_val = X_val[:, :, 1:]\n",
    "    #y_val = y_val[indicis_for_val]"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 79, 79, 79, 79, 79, 79, 79, 78, 78, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 74, 74, 74, 74, 74, 74, 73, 72, 72, 70, 70, 67, 67, 67, 67, 67, 66, 66, 65, 64, 64, 3358]\n",
      "[20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 19, 19, 19, 19, 18, 17, 17, 17, 17, 17, 17, 17, 17, 16, 16, 16, 16, 16, 793]\n",
      "(160,)\n",
      "(40,)\n",
      "(11010,)\n",
      "(2770,)\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T15:35:12.745102Z",
     "start_time": "2024-05-30T15:35:10.075521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "X_train_shape = X_train.shape\n",
    "X_val_shape = X_val.shape\n",
    "X_test_shape = X_test.shape\n",
    "\n",
    "# Note: Do not normalize the cycle value! That is why we start with one\n",
    "for i in range(1, X_train.shape[-1]):\n",
    "    minmax_scaler = MinMaxScaler()\n",
    "    robust_scaler = RobustScaler()\n",
    "    std_scaler = StandardScaler()\n",
    "    \n",
    "    # Scale the data with StandardScaler\n",
    "    X_train[:, :, i] = std_scaler.fit_transform(X_train[:, :, i])\n",
    "    X_val[:, :, i] = std_scaler.transform(X_val[:, :, i])\n",
    "    X_test[:, :, i] = std_scaler.transform(X_test[:, :, i])\n",
    "    \n",
    "    # Scale the data wit MinMaxScaler\n",
    "    X_train[:, :, i] = minmax_scaler.fit_transform(X_train[:, :, i])\n",
    "    X_val[:, :, i] = minmax_scaler.transform(X_val[:, :, i])\n",
    "    X_test[:, :, i] = minmax_scaler.transform(X_test[:, :, i])\n",
    "    \n",
    "    # Scale the data wit RobustScaler\n",
    "    X_train[:, :, i] = robust_scaler.fit_transform(X_train[:, :, i])\n",
    "    X_val[:, :, i] = robust_scaler.transform(X_val[:, :, i])\n",
    "    X_test[:, :, i] = robust_scaler.transform(X_test[:, :, i])"
   ],
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Change types of arrays and swap axes:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T15:35:13.166133Z",
     "start_time": "2024-05-30T15:35:12.746168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_CNN = True\n",
    "\n",
    "print(X_train.shape)\n",
    "if train_CNN:\n",
    "    X_train = np.swapaxes(X_train, 1, 2)\n",
    "    #X_train = X_train[:, np.newaxis, :, :]\n",
    "X_train = np.array(X_train, dtype=np.float32)\n",
    "y_train = np.array(y_train, dtype=np.float32)\n",
    "print(X_train.shape)\n",
    "\n",
    "print(X_val.shape)\n",
    "if train_CNN:\n",
    "    X_val = np.swapaxes(X_val, 1, 2)\n",
    "    #X_val = X_val[:, np.newaxis, :, :]\n",
    "X_val = np.array(X_val, dtype=np.float32)\n",
    "y_val = np.array(y_val, dtype=np.float32)\n",
    "print(X_val.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "if train_CNN:\n",
    "    X_test = np.swapaxes(X_test, 1, 2)\n",
    "    #X_test = X_test[:, np.newaxis, :, :]\n",
    "X_test = np.array(X_test, dtype=np.float32)\n",
    "y_test = np.array(y_test, dtype=np.float32)\n",
    "print(X_test.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11010, 30, 33)\n",
      "(11010, 33, 30)\n",
      "(2770, 30, 33)\n",
      "(2770, 33, 30)\n",
      "(100, 30, 33)\n",
      "(100, 33, 30)\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Only CNN"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Inspiration: Paper Dynamic predictive maintenance for multiple components using data-driven\n",
    "probabilistic RUL prognostics: The case of turbofan engines"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T15:35:26.112427Z",
     "start_time": "2024-05-30T15:35:25.474600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Select hyperparameters of trainer!\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\")\n",
    "trainer = Trainer(min_epochs=1, max_epochs=150, callbacks=[checkpoint_callback], deterministic=True)\n",
    "datamodule = TurbofanDatamodule(batch_size=128)\n",
    "datamodule.set_train_dataset(X_train, y_train)\n",
    "datamodule.set_val_dataset(X_val, y_val)\n",
    "datamodule.set_predict_dataset(X_test)\n",
    "datamodule.set_test_dataset(X_test, y_test[:, 0])\n",
    "model = CNNModel(lr=0.001, window_size=window_size, features=33, dropout_rate=0.2)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T15:39:52.153299Z",
     "start_time": "2024-05-30T15:35:26.685114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "# For visualization write 'tensorboard --logdir=lightning_logs/' in console\n",
    "\n",
    "trainer.fit(model, datamodule=datamodule)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name        | Type    | Params\n",
      "----------------------------------------\n",
      "0 | loss        | MSELoss | 0     \n",
      "1 | dropout     | Dropout | 0     \n",
      "2 | layer1_conv | Conv1d  | 6.6 K \n",
      "3 | layer2_conv | Conv1d  | 8.0 K \n",
      "4 | layer3_conv | Conv1d  | 8.0 K \n",
      "5 | layer4_conv | Conv1d  | 8.0 K \n",
      "6 | fc1         | Linear  | 153 K \n",
      "7 | fc2         | Linear  | 8.3 K \n",
      "8 | fc3         | Linear  | 65    \n",
      "----------------------------------------\n",
      "192 K     Trainable params\n",
      "0         Non-trainable params\n",
      "192 K     Total params\n",
      "0.771     Total estimated model params size (MB)\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T15:39:55.904172Z",
     "start_time": "2024-05-30T15:39:54.777782Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.validate(model, datamodule=datamodule, ckpt_path='best')",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at C:\\Users\\Johannes\\PycharmProjects\\damage-propagation-modeling\\lightning_logs\\version_149\\checkpoints\\epoch=45-step=4002.ckpt\n",
      "Loaded model weights from the checkpoint at C:\\Users\\Johannes\\PycharmProjects\\damage-propagation-modeling\\lightning_logs\\version_149\\checkpoints\\epoch=45-step=4002.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "accb568d43234b6caa9d1c8731c014aa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001B[1m \u001B[0m\u001B[1m     Validate metric     \u001B[0m\u001B[1m \u001B[0mâ”ƒ\u001B[1m \u001B[0m\u001B[1m      DataLoader 0       \u001B[0m\u001B[1m \u001B[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001B[36m \u001B[0m\u001B[36m        val_loss         \u001B[0m\u001B[36m \u001B[0mâ”‚\u001B[35m \u001B[0m\u001B[35m   180.86964416503906    \u001B[0m\u001B[35m \u001B[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\">      Validate metric      </span>â”ƒ<span style=\"font-weight: bold\">       DataLoader 0        </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">    180.86964416503906     </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 180.86964416503906}]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T15:40:03.582968Z",
     "start_time": "2024-05-30T15:40:02.876834Z"
    }
   },
   "cell_type": "code",
   "source": "pred = trainer.test(model, datamodule=datamodule, ckpt_path='best')",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at C:\\Users\\Johannes\\PycharmProjects\\damage-propagation-modeling\\lightning_logs\\version_149\\checkpoints\\epoch=45-step=4002.ckpt\n",
      "Loaded model weights from the checkpoint at C:\\Users\\Johannes\\PycharmProjects\\damage-propagation-modeling\\lightning_logs\\version_149\\checkpoints\\epoch=45-step=4002.ckpt\n",
      "C:\\Users\\Johannes\\anaconda3\\envs\\PSDA\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a647daf3c0447e3b2a42f677e42d783"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001B[1m \u001B[0m\u001B[1m       Test metric       \u001B[0m\u001B[1m \u001B[0mâ”ƒ\u001B[1m \u001B[0m\u001B[1m      DataLoader 0       \u001B[0m\u001B[1m \u001B[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001B[36m \u001B[0m\u001B[36m        test_loss        \u001B[0m\u001B[36m \u001B[0mâ”‚\u001B[35m \u001B[0m\u001B[35m    267.6355285644531    \u001B[0m\u001B[35m \u001B[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\">        Test metric        </span>â”ƒ<span style=\"font-weight: bold\">       DataLoader 0        </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">     267.6355285644531     </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T15:05:58.458154Z",
     "start_time": "2024-05-30T15:05:56.723959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "from lightning import LightningModule\n",
    "import torch \n",
    "model_from_checkpoint = LightningModule.load_from_checkpoint(\"C:/Users/Johannes/PycharmProjects/damage-propagation-modeling/lightning_logs/version_134/checkpoints/epoch=114-step=26105.ckpt\")\n",
    "# disable randomness, dropout, etc...\n",
    "model_from_checkpoint.eval()\n",
    "# predict with the model\n",
    "pred = model_from_checkpoint(torch.tensor(X_test)).detach().numpy()\n",
    "root_mean_squared_error(pred, torch.tensor(y_test))"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/Johannes/PycharmProjects/damage-propagation-modeling/lightning_logs/version_134/checkpoints/epoch=114-step=26105.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlightning\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LightningModule\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m \n\u001B[1;32m----> 4\u001B[0m model_from_checkpoint \u001B[38;5;241m=\u001B[39m \u001B[43mLightningModule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_from_checkpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mC:/Users/Johannes/PycharmProjects/damage-propagation-modeling/lightning_logs/version_134/checkpoints/epoch=114-step=26105.ckpt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# disable randomness, dropout, etc...\u001B[39;00m\n\u001B[0;32m      6\u001B[0m model_from_checkpoint\u001B[38;5;241m.\u001B[39meval()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\PSDA\\lib\\site-packages\\lightning\\pytorch\\utilities\\model_helpers.py:125\u001B[0m, in \u001B[0;36m_restricted_classmethod_impl.__get__.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    120\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m instance \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_scripting:\n\u001B[0;32m    121\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[0;32m    122\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe classmethod `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmethod\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` cannot be called on an instance.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    123\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Please call it on the class type and make sure the return value is used.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    124\u001B[0m     )\n\u001B[1;32m--> 125\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmethod(\u001B[38;5;28mcls\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\PSDA\\lib\\site-packages\\lightning\\pytorch\\core\\module.py:1581\u001B[0m, in \u001B[0;36mLightningModule.load_from_checkpoint\u001B[1;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001B[0m\n\u001B[0;32m   1492\u001B[0m \u001B[38;5;129m@_restricted_classmethod\u001B[39m\n\u001B[0;32m   1493\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_from_checkpoint\u001B[39m(\n\u001B[0;32m   1494\u001B[0m     \u001B[38;5;28mcls\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1499\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m   1500\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Self:\n\u001B[0;32m   1501\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001B[39;00m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;124;03m    passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1579\u001B[0m \n\u001B[0;32m   1580\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1581\u001B[0m     loaded \u001B[38;5;241m=\u001B[39m _load_from_checkpoint(\n\u001B[0;32m   1582\u001B[0m         \u001B[38;5;28mcls\u001B[39m,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[0;32m   1583\u001B[0m         checkpoint_path,\n\u001B[0;32m   1584\u001B[0m         map_location,\n\u001B[0;32m   1585\u001B[0m         hparams_file,\n\u001B[0;32m   1586\u001B[0m         strict,\n\u001B[0;32m   1587\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   1588\u001B[0m     )\n\u001B[0;32m   1589\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(Self, loaded)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\PSDA\\lib\\site-packages\\lightning\\pytorch\\core\\saving.py:63\u001B[0m, in \u001B[0;36m_load_from_checkpoint\u001B[1;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001B[0m\n\u001B[0;32m     61\u001B[0m map_location \u001B[38;5;241m=\u001B[39m map_location \u001B[38;5;129;01mor\u001B[39;00m _default_map_location\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m pl_legacy_patch():\n\u001B[1;32m---> 63\u001B[0m     checkpoint \u001B[38;5;241m=\u001B[39m \u001B[43mpl_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcheckpoint_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmap_location\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;66;03m# convert legacy checkpoints to the new format\u001B[39;00m\n\u001B[0;32m     66\u001B[0m checkpoint \u001B[38;5;241m=\u001B[39m _pl_migrate_checkpoint(\n\u001B[0;32m     67\u001B[0m     checkpoint, checkpoint_path\u001B[38;5;241m=\u001B[39m(checkpoint_path \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(checkpoint_path, (\u001B[38;5;28mstr\u001B[39m, Path)) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m     68\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\PSDA\\lib\\site-packages\\lightning\\fabric\\utilities\\cloud_io.py:56\u001B[0m, in \u001B[0;36m_load\u001B[1;34m(path_or_url, map_location)\u001B[0m\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mhub\u001B[38;5;241m.\u001B[39mload_state_dict_from_url(\n\u001B[0;32m     52\u001B[0m         \u001B[38;5;28mstr\u001B[39m(path_or_url),\n\u001B[0;32m     53\u001B[0m         map_location\u001B[38;5;241m=\u001B[39mmap_location,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[0;32m     54\u001B[0m     )\n\u001B[0;32m     55\u001B[0m fs \u001B[38;5;241m=\u001B[39m get_filesystem(path_or_url)\n\u001B[1;32m---> 56\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mfs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath_or_url\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mload(f, map_location\u001B[38;5;241m=\u001B[39mmap_location)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\PSDA\\lib\\site-packages\\fsspec\\spec.py:1298\u001B[0m, in \u001B[0;36mAbstractFileSystem.open\u001B[1;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001B[0m\n\u001B[0;32m   1296\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1297\u001B[0m     ac \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mautocommit\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_intrans)\n\u001B[1;32m-> 1298\u001B[0m     f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_open(\n\u001B[0;32m   1299\u001B[0m         path,\n\u001B[0;32m   1300\u001B[0m         mode\u001B[38;5;241m=\u001B[39mmode,\n\u001B[0;32m   1301\u001B[0m         block_size\u001B[38;5;241m=\u001B[39mblock_size,\n\u001B[0;32m   1302\u001B[0m         autocommit\u001B[38;5;241m=\u001B[39mac,\n\u001B[0;32m   1303\u001B[0m         cache_options\u001B[38;5;241m=\u001B[39mcache_options,\n\u001B[0;32m   1304\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   1305\u001B[0m     )\n\u001B[0;32m   1306\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m compression \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1307\u001B[0m         \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mfsspec\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompression\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m compr\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\PSDA\\lib\\site-packages\\fsspec\\implementations\\local.py:191\u001B[0m, in \u001B[0;36mLocalFileSystem._open\u001B[1;34m(self, path, mode, block_size, **kwargs)\u001B[0m\n\u001B[0;32m    189\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_mkdir \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m    190\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmakedirs(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_parent(path), exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m--> 191\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m LocalFileOpener(path, mode, fs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\PSDA\\lib\\site-packages\\fsspec\\implementations\\local.py:355\u001B[0m, in \u001B[0;36mLocalFileOpener.__init__\u001B[1;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001B[0m\n\u001B[0;32m    353\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompression \u001B[38;5;241m=\u001B[39m get_compression(path, compression)\n\u001B[0;32m    354\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblocksize \u001B[38;5;241m=\u001B[39m io\u001B[38;5;241m.\u001B[39mDEFAULT_BUFFER_SIZE\n\u001B[1;32m--> 355\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\PSDA\\lib\\site-packages\\fsspec\\implementations\\local.py:360\u001B[0m, in \u001B[0;36mLocalFileOpener._open\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    358\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf\u001B[38;5;241m.\u001B[39mclosed:\n\u001B[0;32m    359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mautocommit \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode:\n\u001B[1;32m--> 360\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    361\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompression:\n\u001B[0;32m    362\u001B[0m             compress \u001B[38;5;241m=\u001B[39m compr[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompression]\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'C:/Users/Johannes/PycharmProjects/damage-propagation-modeling/lightning_logs/version_134/checkpoints/epoch=114-step=26105.ckpt'"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T15:40:08.754392Z",
     "start_time": "2024-05-30T15:40:08.133264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pred = model(torch.tensor(X_test)).detach().numpy()\n",
    "model.eval()\n",
    "root_mean_squared_error(pred, torch.tensor(y_test))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.74496"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T15:06:25.960289Z",
     "start_time": "2024-05-30T15:06:25.359986Z"
    }
   },
   "cell_type": "code",
   "source": "pred",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 51.862972,  67.89349 ,  26.498564,  95.374695,  79.14195 ,\n",
       "        99.26876 ,  70.10647 ,  60.206066,  78.893234,  75.16445 ,\n",
       "        68.97512 ,  75.68833 , 106.93179 ,  31.458738, 111.88981 ,\n",
       "        70.50948 , 121.09294 , 133.38309 ,  88.324196,  93.30068 ,\n",
       "        20.652811, 116.759964,  92.665634,  16.95975 , 100.89441 ,\n",
       "        16.584661, 142.51463 , 109.31307 , 110.87555 ,  78.90963 ,\n",
       "        85.24842 ,  45.71385 ,  86.61725 ,  63.96631 , 103.03752 ,\n",
       "       113.956795, 127.93344 ,  37.64642 ,   9.226481,  43.823063,\n",
       "       112.50242 ,  74.93342 , 115.161705,  78.06849 , 146.9397  ,\n",
       "         9.05889 , 125.63947 ,  41.578754, 106.79327 ,  13.959208,\n",
       "        63.320995, 141.03683 ,  81.06794 , 112.00667 ,  65.58863 ,\n",
       "        62.390923,  57.31184 ,  38.89475 ,  36.391994,  92.64628 ,\n",
       "       134.31812 ,  64.032   ,  80.11501 ,  34.75124 ,  94.39489 ,\n",
       "        83.86167 ,  88.42881 , 127.99012 , 112.436226, 118.27959 ,\n",
       "        51.29933 ,  80.9912  , 141.13812 , 146.47522 ,  49.28496 ,\n",
       "        11.749313, 104.11753 ,  23.058588,  33.81395 , 131.55167 ,\n",
       "        21.847551,  11.284406,  91.10576 ,  81.15719 ,  70.78307 ,\n",
       "        18.305836,  93.13181 , 162.69672 ,  47.337345,  64.771324,\n",
       "       113.64936 ,  19.879753,  90.09495 ,  13.075133, 113.97542 ,\n",
       "        94.58347 , 129.0021  ,  11.363343,  25.780993,  20.930113],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T15:06:26.955901Z",
     "start_time": "2024-05-30T15:06:26.341986Z"
    }
   },
   "cell_type": "code",
   "source": "y_test[:, 0]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 44.,  51.,  27., 120., 101.,  99.,  71.,  55.,  55.,  66.,  77.,\n",
       "       115., 115.,  31., 108.,  56., 136., 132.,  85.,  56.,  18., 119.,\n",
       "        78.,   9.,  58.,  11.,  88., 144., 124.,  89.,  79.,  55.,  71.,\n",
       "        65.,  87., 137., 145.,  22.,   8.,  41., 131., 115., 128.,  69.,\n",
       "       111.,   7., 137.,  55., 135.,  11.,  78., 120.,  87.,  87.,  55.,\n",
       "        93.,  88.,  40.,  49., 128., 129.,  58., 117.,  28., 115.,  87.,\n",
       "        92., 103., 100.,  63.,  35.,  45.,  99., 117.,  45.,  27.,  86.,\n",
       "        20.,  18., 133.,  15.,   6., 145., 104.,  56.,  25.,  68., 144.,\n",
       "        41.,  51.,  81.,  14.,  67.,  10., 127., 113., 123.,  17.,   8.,\n",
       "        28.], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Only LSTM"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T19:43:34.804117Z",
     "start_time": "2024-05-28T19:43:33.945387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Select hyperparameters of trainer!\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\")\n",
    "trainer = Trainer(min_epochs=1, max_epochs=150, callbacks=[checkpoint_callback])\n",
    "datamodule = TurbofanDatamodule()\n",
    "datamodule.set_train_dataset(X_train, y_train)\n",
    "datamodule.set_val_dataset(X_val, y_val)\n",
    "datamodule.set_predict_dataset(X_test)\n",
    "datamodule.set_test_dataset(X_test, y_test[:, 0])\n",
    "model = LSTMModel(lr=0.00001, window_size=window_size, features=25, dropout_rate=0.05)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T19:49:40.026340Z",
     "start_time": "2024-05-28T19:43:34.804648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "trainer.fit(model, datamodule=datamodule)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name        | Type    | Params\n",
      "----------------------------------------\n",
      "0 | loss        | MSELoss | 0     \n",
      "1 | dropout     | Dropout | 0     \n",
      "2 | lstm_layer1 | LSTM    | 79.4 K\n",
      "3 | lstm_layer2 | LSTM    | 49.7 K\n",
      "4 | lstm_layer3 | LSTM    | 5.2 K \n",
      "5 | fc1         | Linear  | 20.5 K\n",
      "6 | fc2         | Linear  | 4.2 K \n",
      "7 | fc3         | Linear  | 65    \n",
      "----------------------------------------\n",
      "159 K     Trainable params\n",
      "0         Non-trainable params\n",
      "159 K     Total params\n",
      "0.636     Total estimated model params size (MB)\n"
     ]
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T19:49:43.159635Z",
     "start_time": "2024-05-28T19:49:42.506294Z"
    }
   },
   "cell_type": "code",
   "source": "pred = trainer.test(model, datamodule=datamodule, ckpt_path='best')",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at C:\\Users\\Johannes\\PycharmProjects\\damage-propagation-modeling\\lightning_logs\\version_128\\checkpoints\\epoch=20-step=9534.ckpt\n",
      "Loaded model weights from the checkpoint at C:\\Users\\Johannes\\PycharmProjects\\damage-propagation-modeling\\lightning_logs\\version_128\\checkpoints\\epoch=20-step=9534.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8f3418aa0775433e8daeb5d932ae94ae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001B[1m \u001B[0m\u001B[1m       Test metric       \u001B[0m\u001B[1m \u001B[0mâ”ƒ\u001B[1m \u001B[0m\u001B[1m      DataLoader 0       \u001B[0m\u001B[1m \u001B[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001B[36m \u001B[0m\u001B[36m        test_loss        \u001B[0m\u001B[36m \u001B[0mâ”‚\u001B[35m \u001B[0m\u001B[35m    3046.253173828125    \u001B[0m\u001B[35m \u001B[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\">        Test metric        </span>â”ƒ<span style=\"font-weight: bold\">       DataLoader 0        </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">     3046.253173828125     </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T19:49:46.872669Z",
     "start_time": "2024-05-28T19:49:46.252201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "import torch \n",
    "pred = model(torch.tensor(X_test)).detach().numpy()\n",
    "root_mean_squared_error(pred, torch.tensor(y_test))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.869015"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
