{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-09T15:31:37.900316500Z",
          "start_time": "2024-05-09T15:31:37.830560800Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5F5XXw_dG30W",
        "outputId": "fa5c361f-7ce1-4c60-9ec0-12b984af133f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-09T15:31:37.980322700Z",
          "start_time": "2024-05-09T15:31:37.900316500Z"
        },
        "id": "c3wYfrYSG30Y"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "DthkG3zAG30Y"
      },
      "source": [
        "# Topic: EX2 - Turbofan RUL Prediction\n",
        "**Task**: Predict the remaining useful life (RUL) of turbofan engines based on given sensor data (time series data). It is a forcasting problem, where the goal is to predict the number of cycles an engine will last before it fails.\n",
        "**Data**: Turbofan engine degradation simulation data (NASA) - [Link](https://data.nasa.gov/dataset/Turbofan-Engine-Degradation-Simulation-Data-Set/vrks-gjie). See also in the topic [introduction notebook](https://github.com/nina-prog/damage-propagation-modeling/blob/2fb8c1a1102a48d7abbf04e4031807790a913a99/notebooks/Turbofan%20remaining%20useful%20life%20Prediction.ipynb).\n",
        "\n",
        "**Subtasks**:\n",
        "1. Perform a deep **exploratory data analysis (EDA)** on the given data.\n",
        "2. Implement a more efficient **sliding window method** for time series data analysis. -> üéØ **Focus on this task**\n",
        "3. Apply **traditional machine learning methods** (SOTA) to predict the remaining useful life. Includes data preparation, feature extraction, feature selection, model selection, and model parameter optimization.\n",
        "4. Create **neural network models** to predict the remaining useful life. Includes different architectures like Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), or Attention Models. Note: You can search for SOTA research papers and reproduce current state-of-the-art models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "QtIxFnhBG30Z"
      },
      "source": [
        "# Imports + Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-09T15:31:38.193478100Z",
          "start_time": "2024-05-09T15:31:37.980322700Z"
        },
        "id": "Dq9s8mxyG30Z"
      },
      "outputs": [],
      "source": [
        "#imports aller Classifier\n",
        "\n",
        "# third-party libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "#sklearn models\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, AdaBoostRegressor, RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.linear_model import ridge_regression, LogisticRegression, Lasso, LinearRegression\n",
        "# sklearn tools\n",
        "from sklearn.metrics import mean_squared_error, root_mean_squared_error\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "#tsfresh\n",
        "from tsfresh.feature_extraction import feature_calculators, MinimalFCParameters, EfficientFCParameters\n",
        "#xgboost\n",
        "from xgboost import XGBRegressor\n",
        "# Bayesion Optimizer\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-09T15:31:38.304026600Z",
          "start_time": "2024-05-09T15:31:38.200491900Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "DLvHCR4AG30a",
        "outputId": "ede887e4-4573-49a1-d9b8-28b378347776"
      },
      "outputs": [],
      "source": [
        "# source code\n",
        "from src.data_loading import load_data, load_config\n",
        "from src.data_splitting import train_val_split_by_group\n",
        "from src.data_cleaning import clean_data, format_dtype\n",
        "from src.rolling_window_creator import calculate_RUL, RollingWindowDatasetCreator\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-09T15:31:38.400456500Z",
          "start_time": "2024-05-09T15:31:38.304026600Z"
        },
        "id": "DwZsljLeG30a"
      },
      "outputs": [],
      "source": [
        "# settings\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_palette(\"Set2\")\n",
        "sns.set(rc={\"figure.dpi\":100, 'savefig.dpi':200})\n",
        "sns.set_context('notebook')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-09T15:31:38.492590100Z",
          "start_time": "2024-05-09T15:31:38.400456500Z"
        },
        "id": "rMUHqB3XG30b"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "_yJmC8GlG30b"
      },
      "source": [
        "# Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "Mz1y0iQ1G30b",
        "outputId": "c2b8c271-67d1-460a-8209-09dce73c8103"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/niklasquendt/Documents/Uni/PSDA/UÃàbung 2/damage-propagation-modeling'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Make sure to execute this cell only once for one kernel session, before running any other cell below.\n",
        "os.chdir(\"/Users/niklasquendt/Documents/Uni/PSDA/√úbung 2/damage-propagation-modeling\") # set working directory to root of project\n",
        "os.getcwd() # check current working directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-09T15:31:38.652221700Z",
          "start_time": "2024-05-09T15:31:38.560578800Z"
        },
        "id": "mgdYuLBxG30c"
      },
      "outputs": [],
      "source": [
        "PATH_TO_CONFIG = \"configs/config.yaml\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "FM9WDvx3G30c"
      },
      "source": [
        "# Load Config + Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-09T15:31:38.732034900Z",
          "start_time": "2024-05-09T15:31:38.652221700Z"
        },
        "id": "n_wsaNOuG30c"
      },
      "outputs": [],
      "source": [
        "config = load_config(PATH_TO_CONFIG) # config is dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-09T15:31:39.154853100Z",
          "start_time": "2024-05-09T15:31:38.740553100Z"
        },
        "id": "dYsb-EXNG30d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 18:53:26 [\u001b[34msrc.data_loading:43\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loading data set 1...\u001b[0m\n",
            "2024-06-01 18:53:26 [\u001b[34msrc.data_loading:72\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loaded raw data for dataset 1.\u001b[0m\n",
            "2024-06-01 18:53:26 [\u001b[34msrc.data_loading:73\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Train Data: (20631, 26)\u001b[0m\n",
            "2024-06-01 18:53:26 [\u001b[34msrc.data_loading:74\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test Data: (13096, 26)\u001b[0m\n",
            "2024-06-01 18:53:26 [\u001b[34msrc.data_loading:75\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test RUL Data: (100, 1)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#Load Dataset 1\n",
        "train_data_1, test_data_1, test_RUL_data_1 = load_data(config_path=PATH_TO_CONFIG, dataset_num=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "fJGsJAdCIYIw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 18:53:31 [\u001b[34msrc.data_loading:43\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loading data set 2...\u001b[0m\n",
            "2024-06-01 18:53:31 [\u001b[34msrc.data_loading:72\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loaded raw data for dataset 2.\u001b[0m\n",
            "2024-06-01 18:53:31 [\u001b[34msrc.data_loading:73\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Train Data: (53759, 26)\u001b[0m\n",
            "2024-06-01 18:53:31 [\u001b[34msrc.data_loading:74\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test Data: (33991, 26)\u001b[0m\n",
            "2024-06-01 18:53:31 [\u001b[34msrc.data_loading:75\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test RUL Data: (259, 1)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#Load Dataset 2\n",
        "train_data_2, test_data_2, test_RUL_data_2 = load_data(config_path=PATH_TO_CONFIG, dataset_num=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "ThMLwpDXIYn2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 18:53:32 [\u001b[34msrc.data_loading:43\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loading data set 3...\u001b[0m\n",
            "2024-06-01 18:53:32 [\u001b[34msrc.data_loading:72\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loaded raw data for dataset 3.\u001b[0m\n",
            "2024-06-01 18:53:32 [\u001b[34msrc.data_loading:73\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Train Data: (24720, 26)\u001b[0m\n",
            "2024-06-01 18:53:32 [\u001b[34msrc.data_loading:74\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test Data: (16596, 26)\u001b[0m\n",
            "2024-06-01 18:53:32 [\u001b[34msrc.data_loading:75\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test RUL Data: (100, 1)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#Load Dataset 3\n",
        "train_data_3, test_data_3, test_RUL_data_3 = load_data(config_path=PATH_TO_CONFIG, dataset_num=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "xItee_5VIY6m"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 18:53:34 [\u001b[34msrc.data_loading:43\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loading data set 4...\u001b[0m\n",
            "2024-06-01 18:53:34 [\u001b[34msrc.data_loading:72\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loaded raw data for dataset 4.\u001b[0m\n",
            "2024-06-01 18:53:34 [\u001b[34msrc.data_loading:73\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Train Data: (61249, 26)\u001b[0m\n",
            "2024-06-01 18:53:34 [\u001b[34msrc.data_loading:74\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test Data: (41214, 26)\u001b[0m\n",
            "2024-06-01 18:53:34 [\u001b[34msrc.data_loading:75\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test RUL Data: (248, 1)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#Load Dataset 4\n",
        "train_data_4, test_data_4, test_RUL_data_4 = load_data(config_path=PATH_TO_CONFIG, dataset_num=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "4VTJOiDVG30d"
      },
      "source": [
        "# üìç << Subtask 3: Traditional ML >>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0IylsBbJ5TL"
      },
      "source": [
        "# Best Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFxeTQYvJ74I"
      },
      "outputs": [],
      "source": [
        "# Dataset 1\n",
        "# On Dataset 1 our best Training model was the RandomForest Regressor\n",
        "# the parameter are the following:\n",
        "# n_estimators=296\n",
        "# max_features=4\n",
        "# random_state = 17\n",
        "\n",
        "\n",
        "\n",
        "# please find the complete pipeline and how we achieved this score below\n",
        "# RMSE : 18.1911"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMWZuNAEKelF"
      },
      "outputs": [],
      "source": [
        "# Dataset 2\n",
        "# On Dataset 2 our best Training model was the XGBoost Regressor\n",
        "# the parameter are the following:\n",
        "# eta=0.02803\n",
        "# gamma=0.8998\n",
        "# max_depth=6\n",
        "\n",
        "\n",
        "# please find the complete pipeline and how we achieved this score below\n",
        "# RMSE on the Train-data: 34.11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nb3XPLlbKfC0"
      },
      "outputs": [],
      "source": [
        "# Dataset 3\n",
        "# On the dataset 3 was XGBoostRegressor our best performing model.\n",
        "# eta= 0.2079\n",
        "# gamma= 1.342\n",
        "# max_depth= 2\n",
        "# It achieved an RMSE of 21.48 in this configuration\n",
        "\n",
        "# The complete pipeline is listed down below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DUqNa41KfYO"
      },
      "outputs": [],
      "source": [
        "# Dataset 4\n",
        "# For dataset 4 the RandomForestRegressor performed the best.\n",
        "# n_estimators = 264\n",
        "# max_features = 1\n",
        "# random_state = 17\n",
        "# In this case it achieved a RMSE of 39.53. When comparing the 4 datasets is by far the worst.\n",
        "\n",
        "# Again the whole pipeline and process is detailed below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTIrxsquK1H7"
      },
      "source": [
        "# Procedure\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoN6TVu9J3IX"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-09T15:31:39.522027500Z",
          "start_time": "2024-05-09T15:31:39.474058600Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "A4sMLpUfG30e",
        "outputId": "bdead425-e33b-46a0-ff55-0f2b4d2d4f9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 18:54:07 [\u001b[34msrc.data_cleaning:134\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Cleaning train and test data...\u001b[0m\n",
            "2024-06-01 18:54:07 [\u001b[34msrc.data_cleaning:136\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Formatting column types...\u001b[0m\n",
            "2024-06-01 18:54:07 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
            "2024-06-01 18:54:07 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
            "2024-06-01 18:54:07 [\u001b[34msrc.data_cleaning:141\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Handling duplicates...\u001b[0m\n",
            "2024-06-01 18:54:07 [\u001b[34msrc.data_cleaning:146\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Removing outliers...\u001b[0m\n",
            "2024-06-01 18:54:07 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
            "2024-06-01 18:54:07 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
            "2024-06-01 18:54:07 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
            "2024-06-01 18:54:07 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
            "2024-06-01 18:54:07 [\u001b[34msrc.data_cleaning:150\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Filter features based train data...\u001b[0m\n",
            "2024-06-01 18:54:07 [\u001b[34msrc.data_cleaning:26\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 features with missing values above the threshold of 0.1.\u001b[0m\n",
            "2024-06-01 18:54:07 [\u001b[34msrc.data_cleaning:46\u001b[0m] [DEBUG\u001b[0m] >>>> Found 7 features with only a single unique value: ['Operation Setting 3', 'Sensor Measure 1', 'Sensor Measure 5', 'Sensor Measure 10', 'Sensor Measure 16', 'Sensor Measure 18', 'Sensor Measure 19']\u001b[0m\n",
            "2024-06-01 18:54:07 [\u001b[34msrc.data_cleaning:103\u001b[0m] [DEBUG\u001b[0m] >>>> Found 4 uncorrelated features with a correlation threshold of 0.3: ['UnitNumber', 'Operation Setting 1', 'Operation Setting 2', 'Sensor Measure 6']\u001b[0m\n",
            "2024-06-01 18:54:07 [\u001b[34msrc.data_cleaning:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Dropping features based on missing values, single unique values, and no target correlation...\u001b[0m\n",
            "2024-06-01 18:54:07 [\u001b[34msrc.data_cleaning:172\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Data cleaning completed.\u001b[0m\n",
            "2024-06-01 18:54:07 [\u001b[34msrc.data_cleaning:173\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original train DataFrame shape: (20631, 16), Resulting train DataFrame shape: (20631, 16)\u001b[0m\n",
            "2024-06-01 18:54:07 [\u001b[34msrc.data_cleaning:174\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original test DataFrame shape: (13096, 16), Resulting test DataFrame shape: (13096, 16)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Default Data cleaning\n",
        "cleaned_train_1, cleaned_test_1 = clean_data(train_data_1, test_data_1, method=None, ignore_columns=['UnitNumber', 'Cycle'], threshold_missing=0.1, threshold_corr=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "KX9QRSvzLZoR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 18:54:09 [\u001b[34msrc.data_cleaning:134\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Cleaning train and test data...\u001b[0m\n",
            "2024-06-01 18:54:09 [\u001b[34msrc.data_cleaning:136\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Formatting column types...\u001b[0m\n",
            "2024-06-01 18:54:09 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
            "2024-06-01 18:54:09 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
            "2024-06-01 18:54:09 [\u001b[34msrc.data_cleaning:141\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Handling duplicates...\u001b[0m\n",
            "2024-06-01 18:54:09 [\u001b[34msrc.data_cleaning:146\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Removing outliers...\u001b[0m\n",
            "2024-06-01 18:54:09 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
            "2024-06-01 18:54:09 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
            "2024-06-01 18:54:09 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
            "2024-06-01 18:54:09 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
            "2024-06-01 18:54:09 [\u001b[34msrc.data_cleaning:150\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Filter features based train data...\u001b[0m\n",
            "2024-06-01 18:54:09 [\u001b[34msrc.data_cleaning:26\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 features with missing values above the threshold of 0.1.\u001b[0m\n",
            "2024-06-01 18:54:09 [\u001b[34msrc.data_cleaning:46\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 features with only a single unique value: []\u001b[0m\n",
            "2024-06-01 18:54:10 [\u001b[34msrc.data_cleaning:103\u001b[0m] [DEBUG\u001b[0m] >>>> Found 25 uncorrelated features with a correlation threshold of 0.3: ['UnitNumber', 'Operation Setting 1', 'Operation Setting 2', 'Operation Setting 3', 'Sensor Measure 1', 'Sensor Measure 2', 'Sensor Measure 3', 'Sensor Measure 4', 'Sensor Measure 5', 'Sensor Measure 6', 'Sensor Measure 7', 'Sensor Measure 8', 'Sensor Measure 9', 'Sensor Measure 10', 'Sensor Measure 11', 'Sensor Measure 12', 'Sensor Measure 13', 'Sensor Measure 14', 'Sensor Measure 15', 'Sensor Measure 16', 'Sensor Measure 17', 'Sensor Measure 18', 'Sensor Measure 19', 'Sensor Measure 20', 'Sensor Measure 21']\u001b[0m\n",
            "2024-06-01 18:54:10 [\u001b[34msrc.data_cleaning:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Dropping features based on missing values, single unique values, and no target correlation...\u001b[0m\n",
            "2024-06-01 18:54:10 [\u001b[34msrc.data_cleaning:172\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Data cleaning completed.\u001b[0m\n",
            "2024-06-01 18:54:10 [\u001b[34msrc.data_cleaning:173\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original train DataFrame shape: (53759, 2), Resulting train DataFrame shape: (53759, 2)\u001b[0m\n",
            "2024-06-01 18:54:10 [\u001b[34msrc.data_cleaning:174\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original test DataFrame shape: (33991, 2), Resulting test DataFrame shape: (33991, 2)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "cleaned_train_2, cleaned_test_2 = clean_data(train_data_2, test_data_2, method=None, ignore_columns=['UnitNumber', 'Cycle'], threshold_missing=0.1, threshold_corr=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "dPX0td5_LdOs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 18:54:11 [\u001b[34msrc.data_cleaning:134\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Cleaning train and test data...\u001b[0m\n",
            "2024-06-01 18:54:11 [\u001b[34msrc.data_cleaning:136\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Formatting column types...\u001b[0m\n",
            "2024-06-01 18:54:11 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
            "2024-06-01 18:54:11 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
            "2024-06-01 18:54:11 [\u001b[34msrc.data_cleaning:141\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Handling duplicates...\u001b[0m\n",
            "2024-06-01 18:54:11 [\u001b[34msrc.data_cleaning:146\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Removing outliers...\u001b[0m\n",
            "2024-06-01 18:54:11 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
            "2024-06-01 18:54:11 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
            "2024-06-01 18:54:11 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
            "2024-06-01 18:54:11 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
            "2024-06-01 18:54:11 [\u001b[34msrc.data_cleaning:150\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Filter features based train data...\u001b[0m\n",
            "2024-06-01 18:54:11 [\u001b[34msrc.data_cleaning:26\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 features with missing values above the threshold of 0.1.\u001b[0m\n",
            "2024-06-01 18:54:11 [\u001b[34msrc.data_cleaning:46\u001b[0m] [DEBUG\u001b[0m] >>>> Found 6 features with only a single unique value: ['Operation Setting 3', 'Sensor Measure 1', 'Sensor Measure 5', 'Sensor Measure 16', 'Sensor Measure 18', 'Sensor Measure 19']\u001b[0m\n",
            "2024-06-01 18:54:11 [\u001b[34msrc.data_cleaning:103\u001b[0m] [DEBUG\u001b[0m] >>>> Found 7 uncorrelated features with a correlation threshold of 0.3: ['UnitNumber', 'Operation Setting 1', 'Operation Setting 2', 'Sensor Measure 6', 'Sensor Measure 15', 'Sensor Measure 20', 'Sensor Measure 21']\u001b[0m\n",
            "2024-06-01 18:54:11 [\u001b[34msrc.data_cleaning:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Dropping features based on missing values, single unique values, and no target correlation...\u001b[0m\n",
            "2024-06-01 18:54:11 [\u001b[34msrc.data_cleaning:172\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Data cleaning completed.\u001b[0m\n",
            "2024-06-01 18:54:11 [\u001b[34msrc.data_cleaning:173\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original train DataFrame shape: (24720, 14), Resulting train DataFrame shape: (24720, 14)\u001b[0m\n",
            "2024-06-01 18:54:11 [\u001b[34msrc.data_cleaning:174\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original test DataFrame shape: (16596, 14), Resulting test DataFrame shape: (16596, 14)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "cleaned_train_3, cleaned_test_3 = clean_data(train_data_3, test_data_3, method=None, ignore_columns=['UnitNumber', 'Cycle'], threshold_missing=0.1, threshold_corr=0.3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "_QcyyJE4LicB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 18:54:14 [\u001b[34msrc.data_cleaning:134\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Cleaning train and test data...\u001b[0m\n",
            "2024-06-01 18:54:14 [\u001b[34msrc.data_cleaning:136\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Formatting column types...\u001b[0m\n",
            "2024-06-01 18:54:14 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
            "2024-06-01 18:54:14 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
            "2024-06-01 18:54:14 [\u001b[34msrc.data_cleaning:141\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Handling duplicates...\u001b[0m\n",
            "2024-06-01 18:54:14 [\u001b[34msrc.data_cleaning:146\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Removing outliers...\u001b[0m\n",
            "2024-06-01 18:54:14 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
            "2024-06-01 18:54:14 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
            "2024-06-01 18:54:14 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
            "2024-06-01 18:54:14 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
            "2024-06-01 18:54:14 [\u001b[34msrc.data_cleaning:150\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Filter features based train data...\u001b[0m\n",
            "2024-06-01 18:54:14 [\u001b[34msrc.data_cleaning:26\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 features with missing values above the threshold of 0.1.\u001b[0m\n",
            "2024-06-01 18:54:14 [\u001b[34msrc.data_cleaning:46\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 features with only a single unique value: []\u001b[0m\n",
            "2024-06-01 18:54:14 [\u001b[34msrc.data_cleaning:103\u001b[0m] [DEBUG\u001b[0m] >>>> Found 25 uncorrelated features with a correlation threshold of 0.3: ['UnitNumber', 'Operation Setting 1', 'Operation Setting 2', 'Operation Setting 3', 'Sensor Measure 1', 'Sensor Measure 2', 'Sensor Measure 3', 'Sensor Measure 4', 'Sensor Measure 5', 'Sensor Measure 6', 'Sensor Measure 7', 'Sensor Measure 8', 'Sensor Measure 9', 'Sensor Measure 10', 'Sensor Measure 11', 'Sensor Measure 12', 'Sensor Measure 13', 'Sensor Measure 14', 'Sensor Measure 15', 'Sensor Measure 16', 'Sensor Measure 17', 'Sensor Measure 18', 'Sensor Measure 19', 'Sensor Measure 20', 'Sensor Measure 21']\u001b[0m\n",
            "2024-06-01 18:54:14 [\u001b[34msrc.data_cleaning:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Dropping features based on missing values, single unique values, and no target correlation...\u001b[0m\n",
            "2024-06-01 18:54:14 [\u001b[34msrc.data_cleaning:172\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Data cleaning completed.\u001b[0m\n",
            "2024-06-01 18:54:14 [\u001b[34msrc.data_cleaning:173\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original train DataFrame shape: (61249, 2), Resulting train DataFrame shape: (61249, 2)\u001b[0m\n",
            "2024-06-01 18:54:14 [\u001b[34msrc.data_cleaning:174\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original test DataFrame shape: (41214, 2), Resulting test DataFrame shape: (41214, 2)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "cleaned_train_4, cleaned_test_4 = clean_data(train_data_4, test_data_4, method=None, ignore_columns=['UnitNumber', 'Cycle'], threshold_missing=0.1, threshold_corr=0.3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrjaiXcaMvW8"
      },
      "source": [
        "Reasons for default data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "OEMGuCD_MMJD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 18:54:17 [\u001b[34msrc.data_cleaning:134\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Cleaning train and test data...\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.data_cleaning:136\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Formatting column types...\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.data_cleaning:141\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Handling duplicates...\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.data_cleaning:146\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Removing outliers...\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.data_cleaning:150\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Filter features based train data...\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.data_cleaning:26\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 features with missing values above the threshold of 0.1.\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.data_cleaning:46\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 features with only a single unique value: []\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.data_cleaning:103\u001b[0m] [DEBUG\u001b[0m] >>>> Found 3 uncorrelated features with a correlation threshold of 0.5: ['UnitNumber', 'Sensor Measure 9', 'Sensor Measure 14']\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.data_cleaning:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Dropping features based on missing values, single unique values, and no target correlation...\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.data_cleaning:172\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Data cleaning completed.\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.data_cleaning:173\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original train DataFrame shape: (20631, 14), Resulting train DataFrame shape: (20631, 14)\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.data_cleaning:174\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original test DataFrame shape: (13096, 14), Resulting test DataFrame shape: (13096, 14)\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.data_cleaning:134\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Cleaning train and test data...\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.data_cleaning:136\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Formatting column types...\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.data_cleaning:141\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Handling duplicates...\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.data_cleaning:146\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Removing outliers...\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.data_cleaning:150\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Filter features based train data...\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.data_cleaning:26\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 features with missing values above the threshold of 0.1.\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.data_cleaning:46\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 features with only a single unique value: []\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.data_cleaning:103\u001b[0m] [DEBUG\u001b[0m] >>>> Found 6 uncorrelated features with a correlation threshold of 0.5: ['UnitNumber', 'Cycle', 'Sensor Measure 7', 'Sensor Measure 10', 'Sensor Measure 12', 'Sensor Measure 14']\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.data_cleaning:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Dropping features based on missing values, single unique values, and no target correlation...\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.data_cleaning:172\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Data cleaning completed.\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.data_cleaning:173\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original train DataFrame shape: (24720, 10), Resulting train DataFrame shape: (24720, 10)\u001b[0m\n",
            "2024-06-01 18:54:17 [\u001b[34msrc.data_cleaning:174\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original test DataFrame shape: (16596, 10), Resulting test DataFrame shape: (16596, 10)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Variations C\n",
        "cleaned_train_1_varC, cleaned_test_1_varC = clean_data(train_data_1, test_data_1, method=None, ignore_columns=['UnitNumber', 'Cycle'], threshold_missing=0.1, threshold_corr=0.5)\n",
        "\n",
        "cleaned_train_3_varC, cleaned_test_1_varC = clean_data(train_data_3, test_data_3, method=None, ignore_columns=['UnitNumber', 'Cycle'], threshold_missing=0.1, threshold_corr=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnoV1PaxSeih"
      },
      "source": [
        "Notes: Data cleaning Variation C for dataset 1 is used instead of the default configuration to reduce clutter by not helpful sensor data. This also improves computation time in all following steps which is very important."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMN9DyQPLT4o"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXTSv4GRLWAy"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "y7pUk0YDMfUv"
      },
      "outputs": [],
      "source": [
        "# Feature Selection -- 1 -> 4\n",
        "feature_list_ds_1 = [\"c3\", \"quantile\", \"mean\", \"root_mean_square\", \"median\", \"time_reversal_asymmetry_statistic\", \"absolute_maximum\", \"maximum\", \"minimum\", \"agg_autocorrelation\", \"autocorrelation\" ]\n",
        "feature_list_ds_2 = [\"c3\", \"quantile\", \"mean\", \"median\", \"root_mean_square\", \"variance\", \"mean_abs_change\", \"standard_deviation\", \"skewness\", \"variation_coefficient\", \"last_location_of_maximum\", \"first_location_of_maximum\"]\n",
        "feature_list_ds_3 = [\"c3\", \"quantile\", \"mean\", \"root_mean_square\", \"median\", \"time_reversal_asymmetry_statistic\", \"absolute_maximum\", \"maximum\", \"minimum\", \"agg_autocorrelation\", \"autocorrelation\" ]\n",
        "feature_list_ds_4 = [\"c3\", \"quantile\", \"mean\", \"median\", \"root_mean_square\", \"variance\", \"mean_abs_change\", \"standard_deviation\", \"skewness\", \"variation_coefficient\", \"last_location_of_maximum\", \"first_location_of_maximum\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gB1RN15cTf91"
      },
      "source": [
        "The feature list we generated by evaluing most of the tsfresh FCparameters by themselves and choosing the top performing ones.\n",
        "Since the datasets 1 & 3 and 2 & 4 are similar they share the same feature list. Further explainations for this are below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_6wM_PXUJMf"
      },
      "outputs": [],
      "source": [
        "#min_timeshift, max_timeshift = 17,18\n",
        "#for ds_train, ds_test, ds_rul in [cleaned_train_1,cleaned_train_2,cleaned_train_3,cleaned_train_4],[cleaned_test_1,cleaned_test_2,cleaned_test_3,cleaned_test_4],[test_RUL_data_1,test_RUL_data_2,test_RUL_data_3,test_RUL_data_4]:\n",
        "#  for feat in EfficientFCParameters():\n",
        "#    # RollingWindow\n",
        "#    rwCreator = RollingWindowDatasetCreator(max_timeshift=max_timeshift,min_timeshift=min_timeshift,feature_extraction_mode= 'custom',feature_list=[feat])\n",
        "#    X_train, y_train, X_test, y_test = rwCreator.create_rolling_windows_datasets(train_data=ds_train, test_data=ds_test,test_RUL_data=ds_rul,)\n",
        "#    # KNeighborsRegressor\n",
        "#    knr = KNeighborsRegressor(3)\n",
        "#    knr.fit(X_train, y_train.values.ravel())\n",
        "#    rgr1 = np.sqrt(mean_squared_error(y_test, knr.predict(X_test)))\n",
        "#    # RandomForestRegressor\n",
        "#    rfr  = RandomForestRegressor(max_depth=5, n_estimators=10, max_features=1, random_state=42)\n",
        "#    rfr.fit(X_train, y_train.values.ravel())\n",
        "#    rgr2 = np.sqrt(mean_squared_error(y_test, rfr.predict(X_test)))\n",
        "#    # Lasso\n",
        "#    lr = Lasso()\n",
        "#    lr.fit(X_train, y_train.values.ravel())\n",
        "#    rgr3 = np.sqrt(mean_squared_error(y_test, lr.predict(X_test)))\n",
        "#    # XGBRegressor\n",
        "#    xgbr = XGBRegressor(n_estimators=3, max_depth=1, learning_rate=0.211) # objective='binary:logistic'\n",
        "#    xgbr.fit(X_train, y_train.values.ravel())\n",
        "#    rgr4 = np.sqrt(mean_squared_error(y_test, xgbr.predict(X_test)))\n",
        "#    # Prev Data import\n",
        "#    df_in = pd.read_pickle(\"drive/MyDrive/PSDA_cml/data/processed/ds4_tsf-feat_eff_results.pkl\")\n",
        "#    df = pd.DataFrame(data={'Feature': feat, 'Regressor Results': [f\"KNR: {rgr1}\",f\"RFR: {rgr2}\", f\"Lasso: {rgr3}\", f\"XBGr: {rgr4}\"]})\n",
        "#    df_out = pd.concat([df_in, df])\n",
        "#    df_out.to_pickle(f\"drive/MyDrive/PSDA_cml/data/processed/ds_tsf-feat_eff_results.pkl\")\n",
        "#    print({'Feature': feat, 'Regressor Results': [f\"KNR: {rgr1}\",f\"RFR: {rgr2}\", f\"Lasso: {rgr3}\", f\"XBGr: {rgr4}\"]})\n",
        "#  df = pd.read_pickle(\"drive/MyDrive/PSDA_cml/data/processed/ds1_tsf-feat_eff_results.pkl\")\n",
        "#  dict_ds = dict()\n",
        "#  for i in range(0,df.shape[0],4):\n",
        "#    mean = (float(df.values[i, 1].partition(\":\")[2]) + float(df.values[i+1, 1].partition(\":\")[2]) + float(df.values[i+2, 1].partition(\":\")[2]) + float(df.values[i+3, 1].partition(\":\")[2])) /4\n",
        "#    dict_ds[df.values[i,0]] = mean\n",
        "#  df_ds = pd.DataFrame.from_dict(data=dict_ds,orient='index',columns=['mean'])\n",
        "#  print(f\"Dataset {ds_train}: \")\n",
        "#  print(df_ds.sort_values(by='mean'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH9JRI8tW7aQ"
      },
      "source": [
        "This code loops over all datasets and all features of tsfreshs EfficientFCParameters. \\\\\n",
        "This took multiple hours and never fully finished. The longest run computed 56 features while the other computed far less. Thus we based the feature_list on the longest succesfully running tries for dataset 1 & 3 and the dataset 2 & 4.\n",
        "The two datasets were combined due to their similarity. This approach can't be advised to repeat since it each try, even unsuccessfull took ones multiple hours. The shortest was somewhat over 2 hours before the runtime reached its limit. Sadly because tsfresh was uncompatible with jupyterhub there was no real alternative. [Computation for this was done in Colab: https://colab.research.google.com/drive/1F_hpmXcxYoJT3LsvXjF65c3_lZ7ltEr6?usp=sharing ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu5JfkMsNEu4"
      },
      "source": [
        "# Windowing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "QHW-7VlKSUhU"
      },
      "outputs": [],
      "source": [
        "# Var\n",
        "min_ts = 5\n",
        "max_ts = 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "fl1gJ9fESWC8"
      },
      "outputs": [],
      "source": [
        "# Var\n",
        "min_ts_ds1_varc = 29\n",
        "max_ts_ds1_varc = 30\n",
        "\n",
        "min_ts_ds2_varc = 17\n",
        "max_ts_ds2_varc = 18\n",
        "\n",
        "min_ts_ds3_varc = 29\n",
        "max_ts_ds3_varc = 30\n",
        "\n",
        "min_ts_ds4_varc = 17\n",
        "max_ts_ds4_varc = 18"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udNp6gPwZXoQ"
      },
      "source": [
        "The best results were achieved with a window size of around 30. Because of that variation C uses a max timeshift of 30. Similar to some papers.\n",
        "Sadly datasets 2 & 4 have entries which have fewer than 20 steps. Thus for them the timeshift is reduced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnyyyMdi37qH"
      },
      "outputs": [],
      "source": [
        "#Erstellen der Datens√§tze mittels unserer Rolling Window Methode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "hES8TXiOYQa2"
      },
      "outputs": [],
      "source": [
        "rwCreator_varc = RollingWindowDatasetCreator(max_timeshift=max_ts_ds1_varc,min_timeshift=min_ts_ds1_varc,feature_extraction_mode= 'custom',feature_list=feature_list_ds_1)\n",
        "rwCreator = RollingWindowDatasetCreator(max_timeshift=max_ts,min_timeshift=min_ts,feature_extraction_mode= 'minimal')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM5jyxjbPbCL"
      },
      "source": [
        "RollingWindowCreator Variation C with the custom extraction mode is helpful for datasets 1 & 3. The results for the custom feature list for 2 & 4 were heavily dependent on the used regressor model. For them minimal seemed usually sufficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "4scobRfOYxnL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 19:02:11 [\u001b[34msrc.rolling_window_creator:134\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Creating rolling windows for train data...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rolling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:03<00:00,  5.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 19:02:15 [\u001b[34msrc.rolling_window_creator:140\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Extracting features for train data...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Feature Extraction: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [01:02<00:00,  3.12s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 19:03:26 [\u001b[34msrc.rolling_window_creator:148\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Calculating target for train data...\u001b[0m\n",
            "2024-06-01 19:03:26 [\u001b[34msrc.rolling_window_creator:134\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Creating rolling windows for test data...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rolling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:03<00:00,  6.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 19:03:29 [\u001b[34msrc.rolling_window_creator:140\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Extracting features for test data...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Feature Extraction: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 11.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 19:03:31 [\u001b[34msrc.rolling_window_creator:176\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Datasets created successfully.\u001b[0m\n",
            "2024-06-01 19:03:31 [\u001b[34msrc.rolling_window_creator:177\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Shape of X_train: (17731, 396)\u001b[0m\n",
            "2024-06-01 19:03:31 [\u001b[34msrc.rolling_window_creator:178\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Shape of y_train: (17731, 1)\u001b[0m\n",
            "2024-06-01 19:03:31 [\u001b[34msrc.rolling_window_creator:179\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Shape of X_test: (100, 264)\u001b[0m\n",
            "2024-06-01 19:03:31 [\u001b[34msrc.rolling_window_creator:180\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Shape of y_test: (100, 1)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "X_train_1_varc, y_train_1_varc, X_test_1_varc, y_test_1_varc = rwCreator_varc.create_rolling_windows_datasets(cleaned_train_1_varC,cleaned_test_1_varC,test_RUL_data_1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bfr_dedSZUjK"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SePwXGwXNj-h"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "S7PzljVdW8oS"
      },
      "outputs": [],
      "source": [
        "y_train_1_varc = y_train_1_varc.clip(upper=125)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "BL7tfVI0Nmbw"
      },
      "outputs": [],
      "source": [
        "#Scalieren der Datens√§tze\n",
        "scaler_std = StandardScaler()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "TxGQ1UwGNotE"
      },
      "outputs": [],
      "source": [
        "X_train_1_varc[2:] = scaler_std.fit_transform(X_train_1_varc[2:])\n",
        "X_test_1_varc[2:] = scaler_std.fit_transform(X_test_1_varc[2:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaZclkiyORbn"
      },
      "source": [
        "# Traditional ML Models\n",
        "\n",
        "The models we tested are the following:\n",
        "\n",
        "KNeighborsRegressor  \\\\\n",
        "SupportVectorMachineRegressor \\\\\n",
        "RandomForestRegressor  \\\\\n",
        "MultiLayerPerceptronRegressor \\\\\n",
        "AdaBoostRegressor \\\\\n",
        "GaussianNaiveBayes \\\\\n",
        "KernelRidge \\\\\n",
        "Lasso  \\\\\n",
        "LinearRegressor \\\\\n",
        "LogisiticRegressor \\\\\n",
        "GradBoostRegressor \\\\\n",
        "XGBoostRegressor \\\\\n",
        "ExtraTrees\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8Ci80IHWFho"
      },
      "outputs": [],
      "source": [
        "#for reasons of clarity we only show the models on the first Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "HpiZQvAUQhC4",
        "outputId": "aa6b5d8d-fb79-4ab9-922c-df6692bdd1ea"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'KNeighborsRegressor' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-cfb92fdd615c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# KNeighorsRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrgr\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mrgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_1_varc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_1_varc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_1_varc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_1_varc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'KNeighborsRegressor' is not defined"
          ]
        }
      ],
      "source": [
        "# KNeighorsRegressor\n",
        "rgr  = KNeighborsRegressor(3)\n",
        "rgr.fit(X_train_1_varc, y_train_1_varc.values.ravel())\n",
        "print(np.sqrt(mean_squared_error(y_test_1_varc, rgr.predict(X_test_1_varc))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "EFsvLH8nX7zN",
        "outputId": "e1cc579e-6b33-487e-c1c9-8a6b2def109e"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'SVR' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-2b22df7e6b73>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# SupportVectorMachineRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"linear\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mrgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_1_varc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_1_varc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_1_varc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_1_varc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SVR' is not defined"
          ]
        }
      ],
      "source": [
        "# SupportVectorMachineRegressor\n",
        "rgr = SVR(kernel=\"linear\", C=0.025)\n",
        "rgr.fit(X_train_1_varc, y_train_1_varc.values.ravel())\n",
        "print(np.sqrt(mean_squared_error(y_test_1_varc, rgr.predict(X_test_1_varc))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuIVCSKiZsVy"
      },
      "outputs": [],
      "source": [
        "# RandomForestRegressor\n",
        "rgr  = RandomForestRegressor(max_depth=5, n_estimators=10, max_features=1, random_state=42)\n",
        "rgr.fit(X_train_1_varc, y_train_1_varc.values.ravel())\n",
        "print(np.sqrt(mean_squared_error(y_test_1_varc, rgr.predict(X_test_1_varc))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1yBOWnuZ6E8"
      },
      "outputs": [],
      "source": [
        "# MultiLayerPerceptronRegressor\n",
        "rgr  = MLPRegressor(alpha=1, max_iter=1000, random_state=42)\n",
        "rgr.fit(X_train_1_varc, y_train_1_varc.values.ravel())\n",
        "print(np.sqrt(mean_squared_error(y_test_1_varc, rgr.predict(X_test_1_varc))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W94wA5Wjacrr"
      },
      "outputs": [],
      "source": [
        "# AdaBoostRegressor\n",
        "rgr  = AdaBoostRegressor(random_state=42)\n",
        "rgr.fit(X_train_1_varc, y_train_1_varc.values.ravel())\n",
        "print(np.sqrt(mean_squared_error(y_test_1_varc, rgr.predict(X_test_1_varc))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKz9RpniahgG"
      },
      "outputs": [],
      "source": [
        "# GaussianNaiveBayes\n",
        "rgr  = GaussianNB()\n",
        "rgr.fit(X_train_1_varc, y_train_1_varc.values.ravel())\n",
        "print(np.sqrt(mean_squared_error(y_test_1_varc, rgr.predict(X_test_1_varc))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEUzFKiWaly2"
      },
      "outputs": [],
      "source": [
        "# KernelRidgeRegressor\n",
        "rgr  = KernelRidge()\n",
        "rgr.fit(X_train_1_varc, y_train_1_varc.values.ravel())\n",
        "print(np.sqrt(mean_squared_error(y_test_1_varc, rgr.predict(X_test_1_varc))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJVHwY4Kapk4"
      },
      "outputs": [],
      "source": [
        "# Lasso\n",
        "rgr  = Lasso()\n",
        "rgr.fit(X_train_1_varc, y_train_1_varc.values.ravel())\n",
        "print(np.sqrt(mean_squared_error(y_test_1_varc, rgr.predict(X_test_1_varc))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jc8GGzi5arSv"
      },
      "outputs": [],
      "source": [
        "# LinearRegressor\n",
        "rgr  = LinearRegression()\n",
        "rgr.fit(X_train_1_varc, y_train_1_varc.values.ravel())\n",
        "print(np.sqrt(mean_squared_error(y_test_1_varc, rgr.predict(X_test_1_varc))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzS70lbGauO4"
      },
      "outputs": [],
      "source": [
        "# LogisticRegressor\n",
        "rgr  = LogisticRegression()\n",
        "rgr.fit(X_train_1_varc, y_train_1_varc.values.ravel())\n",
        "print(np.sqrt(mean_squared_error(y_test_1_varc, rgr.predict(X_test_1_varc))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hp8tYg-Caxsb"
      },
      "outputs": [],
      "source": [
        "# GradientBoostRegressor\n",
        "rgr  = GradientBoostingRegressor()\n",
        "rgr.fit(X_train_1_varc, y_train_1_varc.values.ravel())\n",
        "print(np.sqrt(mean_squared_error(y_test_1_varc, rgr.predict(X_test_1_varc))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dp1cmvlfa06e"
      },
      "outputs": [],
      "source": [
        "# XGBoostRegressor\n",
        "rgr  = XGBRegressor()\n",
        "rgr.fit(X_train_1_varc, y_train_1_varc.values.ravel())\n",
        "print(np.sqrt(mean_squared_error(y_test_1_varc, rgr.predict(X_test_1_varc))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64fuhOF2fX41"
      },
      "outputs": [],
      "source": [
        "#ExtraTreesRegressor\n",
        "#separate try for the ExtraTreesRegressor as it was the suggestion of TPOT\n",
        "# the pipeline for the ExtraTreesRegressor is different from the ones implemented above thats why we create the dataset with the rolling window again\n",
        "\n",
        "# Erstellen die Rollfenster-Datens√§tze\n",
        "X_train_1, y_train_1, X_test_1, y_test_1 = rwCreator.create_rolling_windows_datasets(\n",
        "    train_data=cleaned_train_1,\n",
        "    test_data=cleaned_test_1,\n",
        "    test_RUL_data=test_RUL_data_1,\n",
        ")\n",
        "\n",
        "# split data into training and validation\n",
        "X_train_1, X_val_1, y_train_1, y_val_1 = train_test_split(\n",
        "    X_train_1,\n",
        "    y_train_1,\n",
        "    test_size=0.2,  # 20% der Daten werden f√ºr die Validierung verwendet\n",
        "    random_state=42\n",
        ")\n",
        "# Data-Scaling\n",
        "scaler_1 = StandardScaler()\n",
        "X_train_scaled_1 = scaler_1.fit_transform(X_train_1)\n",
        "X_val_scaled_1 = scaler_1.transform(X_val_1)\n",
        "X_test_scaled_1 = scaler_1.transform(X_test_1)\n",
        "\n",
        "#ExtraTreeRegressor\n",
        "extra_trees = ExtraTreesRegressor(n_estimators=140, max_depth= 15, min_samples_leaf=5, random_state=42)\n",
        "\n",
        "# Training\n",
        "extra_trees.fit(X_train_scaled_1, y_train_1)\n",
        "\n",
        "# Vorhersagen und Bewerten des ExtraTreesRegressor\n",
        "et_predictions_1 = extra_trees.predict(X_val_scaled_1)\n",
        "print(sklearn.metrics.root_mean_squared_error(y_val_1, et_predictions_1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xw-5hLxPiSF"
      },
      "source": [
        "# Findings: Models\n",
        "\n",
        "Most notable of the first spectated models are KNeighbor, RandomForest, Lasso and GradientBoost. Except for GradientBoost all other regressors will be optimized for. Instead of GradientBoost we will instead optimize for XGBoost. \\\\\n",
        "The reasons for this decisions are that the computation time for XGB is far shorter than GradBoost and also according to some papers XGBoost can perform very well on this dataset if optimized correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ3HcLxaPqZv"
      },
      "source": [
        "### Optimization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vu_uw8J9MDp2"
      },
      "outputs": [],
      "source": [
        "# for the Optimization we also used TPOT to find the best model\n",
        "# the result of the TPOT was the ExtraTreeRegressor\n",
        "# due to very long runtime, the TPOT is in comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkOFyWGEVpll"
      },
      "outputs": [],
      "source": [
        "#tpot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tpot import TPOTRegressor\n",
        "\n",
        "# TPOT\n",
        "#tpot = TPOTRegressor(generations=3, population_size=20, cv=3, verbosity=2, random_state=42)\n",
        "\n",
        "#tpot.fit(X_train_scaled_1, y_train_1)\n",
        "\n",
        "#print(tpot.score(X_test_scaled_1, y_test_1))\n",
        "\n",
        "#tpot.export('best_model_pipeline.py')\n",
        "\n",
        "#print(tpot.fitted_pipeline_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KGGq6krfQcv"
      },
      "source": [
        "# Dataset 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "G-yJmoYSQxaJ",
        "outputId": "92917082-92f2-4dc3-f021-8528b7e525df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 19:08:34 [\u001b[34msrc.data_loading:43\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loading data set 1...\u001b[0m\n",
            "2024-06-01 19:08:34 [\u001b[34msrc.data_loading:72\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loaded raw data for dataset 1.\u001b[0m\n",
            "2024-06-01 19:08:34 [\u001b[34msrc.data_loading:73\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Train Data: (20631, 26)\u001b[0m\n",
            "2024-06-01 19:08:34 [\u001b[34msrc.data_loading:74\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test Data: (13096, 26)\u001b[0m\n",
            "2024-06-01 19:08:34 [\u001b[34msrc.data_loading:75\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test RUL Data: (100, 1)\u001b[0m\n",
            "2024-06-01 19:08:34 [\u001b[34msrc.data_cleaning:134\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Cleaning train and test data...\u001b[0m\n",
            "2024-06-01 19:08:34 [\u001b[34msrc.data_cleaning:136\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Formatting column types...\u001b[0m\n",
            "2024-06-01 19:08:34 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
            "2024-06-01 19:08:34 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
            "2024-06-01 19:08:34 [\u001b[34msrc.data_cleaning:141\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Handling duplicates...\u001b[0m\n",
            "2024-06-01 19:08:34 [\u001b[34msrc.data_cleaning:146\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Removing outliers...\u001b[0m\n",
            "2024-06-01 19:08:34 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
            "2024-06-01 19:08:34 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
            "2024-06-01 19:08:34 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
            "2024-06-01 19:08:34 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
            "2024-06-01 19:08:34 [\u001b[34msrc.data_cleaning:150\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Filter features based train data...\u001b[0m\n",
            "2024-06-01 19:08:34 [\u001b[34msrc.data_cleaning:26\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 features with missing values above the threshold of 0.1.\u001b[0m\n",
            "2024-06-01 19:08:34 [\u001b[34msrc.data_cleaning:46\u001b[0m] [DEBUG\u001b[0m] >>>> Found 7 features with only a single unique value: ['Operation Setting 3', 'Sensor Measure 1', 'Sensor Measure 5', 'Sensor Measure 10', 'Sensor Measure 16', 'Sensor Measure 18', 'Sensor Measure 19']\u001b[0m\n",
            "2024-06-01 19:08:34 [\u001b[34msrc.data_cleaning:103\u001b[0m] [DEBUG\u001b[0m] >>>> Found 6 uncorrelated features with a correlation threshold of 0.5: ['UnitNumber', 'Operation Setting 1', 'Operation Setting 2', 'Sensor Measure 6', 'Sensor Measure 9', 'Sensor Measure 14']\u001b[0m\n",
            "2024-06-01 19:08:34 [\u001b[34msrc.data_cleaning:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Dropping features based on missing values, single unique values, and no target correlation...\u001b[0m\n",
            "2024-06-01 19:08:34 [\u001b[34msrc.data_cleaning:172\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Data cleaning completed.\u001b[0m\n",
            "2024-06-01 19:08:34 [\u001b[34msrc.data_cleaning:173\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original train DataFrame shape: (20631, 14), Resulting train DataFrame shape: (20631, 14)\u001b[0m\n",
            "2024-06-01 19:08:34 [\u001b[34msrc.data_cleaning:174\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original test DataFrame shape: (13096, 14), Resulting test DataFrame shape: (13096, 14)\u001b[0m\n",
            "2024-06-01 19:08:34 [\u001b[34msrc.data_splitting:47\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Train set contains 82 different engines --> in total 16807\u001b[0m\n",
            "2024-06-01 19:08:34 [\u001b[34msrc.data_splitting:48\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>>  Test set contains 18 different engines --> in total 3824\u001b[0m\n",
            "2024-06-01 19:08:34 [\u001b[34msrc.rolling_window_creator:134\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Creating rolling windows for train data...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rolling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:03<00:00,  6.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 19:08:37 [\u001b[34msrc.rolling_window_creator:140\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Extracting features for train data...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Feature Extraction: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:55<00:00,  2.79s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 19:09:40 [\u001b[34msrc.rolling_window_creator:148\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Calculating target for train data...\u001b[0m\n",
            "2024-06-01 19:09:40 [\u001b[34msrc.rolling_window_creator:134\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Creating rolling windows for train data...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rolling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:01<00:00, 10.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 19:09:42 [\u001b[34msrc.rolling_window_creator:140\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Extracting features for train data...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Feature Extraction: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:14<00:00,  1.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 19:09:57 [\u001b[34msrc.rolling_window_creator:148\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Calculating target for train data...\u001b[0m\n",
            "2024-06-01 19:09:57 [\u001b[34msrc.rolling_window_creator:134\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Creating rolling windows for test data...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rolling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:02<00:00,  8.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 19:09:59 [\u001b[34msrc.rolling_window_creator:140\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Extracting features for test data...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Feature Extraction: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 12.74it/s]\n"
          ]
        }
      ],
      "source": [
        "## Load first Dataset\n",
        "train_data_1_opt, test_data_1_opt,test_rul_data_1_opt = load_data(config_path=PATH_TO_CONFIG, dataset_num=1)\n",
        "cleaned_train_1_opt, cleaned_test_1_opt = clean_data(train_data_1_opt, test_data_1_opt, method=None, ignore_columns=['UnitNumber', 'Cycle'], threshold_missing=0.1, threshold_corr=0.5)\n",
        "\n",
        "# Train Val Split\n",
        "cl_train_1_opt, cl_val_1_opt = train_val_split_by_group(df = cleaned_train_1_opt,group = \"UnitNumber\",test_size = 0.18,n_splits = 2,random_state = 7)\n",
        "\n",
        "# RollingWindowParameter\n",
        "min_ts_1_opt = 29\n",
        "max_ts_1_opt = 30\n",
        "feature_list_ds_1 = [\"c3\", \"quantile\", \"mean\", \"root_mean_square\", \"median\", \"time_reversal_asymmetry_statistic\", \"absolute_maximum\", \"maximum\", \"minimum\", \"agg_autocorrelation\", \"autocorrelation\" ]\n",
        "# Create RollingWindows\n",
        "rwCreator = RollingWindowDatasetCreator(max_timeshift=max_ts_1_opt,min_timeshift=min_ts_1_opt,feature_extraction_mode= 'custom',feature_list=feature_list_ds_1)\n",
        "X_train_1_opt, y_train_1_opt = rwCreator._process_data(cl_train_1_opt, 'train')\n",
        "X_val_1_opt, y_val_1_opt = rwCreator._process_data(cl_val_1_opt, 'train')\n",
        "X_test_1_opt, y_test_1_opt = rwCreator._process_data(cleaned_test_1_opt, 'test', test_rul_data_1_opt)\n",
        "\n",
        "#Data Preprocessing\n",
        "y_train_1_opt = y_train_1_opt.clip(upper=125)\n",
        "scaler = StandardScaler()\n",
        "X_train_1_opt[2:] = scaler.fit_transform(X_train_1_opt[2:])\n",
        "X_val_1_opt[2:] = scaler.fit_transform(X_val_1_opt[2:])\n",
        "X_test_1_opt[2:] = scaler.fit_transform(X_test_1_opt[2:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGZlqcWjenQi"
      },
      "outputs": [],
      "source": [
        "#knn optimization\n",
        "def hyperparameter_function_knn(neighbours):\n",
        "      \"\"\" Function for hyperparameter optimization\n",
        "      \"\"\"\n",
        "      neighbours = neighbours.round().astype(int)\n",
        "      knn_regressor = KNeighborsRegressor(n_neighbors=neighbours)\n",
        "      metric = cross_val_score(knn_regressor, X=X_train_1_opt, y=y_train_1_opt, cv=5, scoring='neg_root_mean_squared_error')\n",
        "      return metric.min()\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'neighbours': (1, 750)}\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "  f=hyperparameter_function_knn,\n",
        "  pbounds=pbounds,\n",
        "  random_state=17,\n",
        "  allow_duplicate_points= True\n",
        ")\n",
        "\n",
        "optimizer.maximize(\n",
        "  init_points=10,\n",
        "  n_iter=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "fyxarzUsewUM",
        "outputId": "057c023b-a7c0-4980-fa54-99ec145d11e3"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'KNeighborsRegressor' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-43a6e1777add>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Neigbors = 383\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Testing of Knn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mknn_regressor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m383\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mknn_regressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_1_opt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_1_opt\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_pred_1_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn_regressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_1_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'KNeighborsRegressor' is not defined"
          ]
        }
      ],
      "source": [
        "knn_regressor = KNeighborsRegressor(n_neighbors=383)\n",
        "knn_regressor.fit(X_train_1_opt,y_train_1_opt )\n",
        "y_pred_1_opt = knn_regressor.predict(X_test_1_opt)\n",
        "print(np.sqrt(mean_squared_error(y_test_1_opt, y_pred_1_opt)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaAt3MyXQ6nF"
      },
      "source": [
        "Results: 23.36\n",
        "\n",
        "Notes: KNR works very well and achieved similar values in both validation and testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4o3_Faebey9_"
      },
      "outputs": [],
      "source": [
        "#Random Forest Optimization\n",
        "def hyperparameter_function_rf(n_estimators, max_features, ):\n",
        "      \"\"\" Function for hyperparameter optimization\n",
        "      \"\"\"\n",
        "      n_estimators = n_estimators.round().astype(int)\n",
        "      max_features = max_features.round().astype(int)\n",
        "\n",
        "      rf_regressor = RandomForestRegressor(n_estimators=n_estimators,max_features=max_features,random_state=17,n_jobs=-1)\n",
        "      metric = cross_val_score(rf_regressor, X=X_train_1_opt, y=y_train_1_opt.values.ravel(), cv=5, scoring='neg_root_mean_squared_error')\n",
        "      return metric.min()\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'n_estimators': (20, 500),'max_features': (1,1) }\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "  f=hyperparameter_function_rf,\n",
        "  pbounds=pbounds,\n",
        "  random_state=17,\n",
        "  allow_duplicate_points= True\n",
        ")\n",
        "\n",
        "optimizer.maximize(\n",
        "  init_points=10,\n",
        "  n_iter=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "IkOf6uOOezWX",
        "outputId": "73da7f9d-a09e-4e0e-8ea5-ab3aa317a421"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'RandomForestRegressor' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-df87e1625f83>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# n_estimators=296 , max_features=4 -> 22.93\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrf_regressor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m296\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mrf_regressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_1_opt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_1_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_pred_1_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_regressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_1_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_1_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_1_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'RandomForestRegressor' is not defined"
          ]
        }
      ],
      "source": [
        "# n_estimators=296 , max_features=4 -> 22.93\n",
        "\n",
        "#Random Forest Regressor Testing\n",
        "rf_regressor = RandomForestRegressor(n_estimators=296,max_features=4,random_state=17)\n",
        "rf_regressor.fit(X_train_1_opt,y_train_1_opt.values.ravel())\n",
        "y_pred_1_opt = rf_regressor.predict(X_test_1_opt)\n",
        "print(np.sqrt(mean_squared_error(y_test_1_opt, y_pred_1_opt)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVq__ks8Q9Hq"
      },
      "source": [
        "Results: 18.47\n",
        "\n",
        "Notes: RFR was the best performer for the custom feature set on tsfresh\n",
        "with reaching a peak in optimization of 18.56. This is far above expectation since similar papers stopped at the low twenties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpb-0Ihpe36V"
      },
      "outputs": [],
      "source": [
        "#Lasso Optimization\n",
        "def hyperparameter_function_lasso(alpha, max_iter, ):\n",
        "      \"\"\" Function for hyperparameter optimization\n",
        "      \"\"\"\n",
        "      max_iter = max_iter.round().astype(int)\n",
        "\n",
        "      lasso_regressor = Lasso(alpha=alpha,max_iter=max_iter,random_state=17)\n",
        "      metric = cross_val_score(lasso_regressor, X=X_train_1_opt, y=y_train_1_opt.values.ravel(), cv=5, scoring='neg_root_mean_squared_error')\n",
        "      return metric.min()\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'alpha': (0.001, 1),'max_iter': (100,10000) }\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "  f=hyperparameter_function_lasso,\n",
        "  pbounds=pbounds,\n",
        "  random_state=17,\n",
        "  allow_duplicate_points= True\n",
        ")\n",
        "\n",
        "optimizer.maximize(\n",
        "  init_points=10,\n",
        "  n_iter=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ipRp6RSe4Lx"
      },
      "outputs": [],
      "source": [
        "lasso_regressor = Lasso(alpha=0.0319,max_iter=656,random_state=17)\n",
        "lasso_regressor.fit(X_train_1_opt,y_train_1_opt )\n",
        "y_pred_1_opt = lasso_regressor.predict(X_test_1_opt)\n",
        "print(np.sqrt(mean_squared_error(y_test_1_opt, y_pred_1_opt)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7YUpWmYRAC_"
      },
      "source": [
        "Results: 1620.91\n",
        "\n",
        "Notes: Despite the first test Lasso seems to be underperforming. Which might be because of the hyperparameter optimization or other factors such as the random_stare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42An6m-je4a2"
      },
      "outputs": [],
      "source": [
        "#XGBoost Optimization\n",
        "def hyperparameter_function_xgboost(eta, gamma ,max_depth,reg_lambda,reg_alpha ):\n",
        "      \"\"\" Function for hyperparameter optimization\n",
        "      \"\"\"\n",
        "      max_depth = max_depth.round().astype(int)\n",
        "\n",
        "      xgb_regressor = XGBRegressor(eta=eta,gamma=gamma,max_depth=max_depth,reg_lambda=reg_lambda, reg_alpha=reg_alpha)\n",
        "      metric = cross_val_score(xgb_regressor, X=X_train_1_opt, y=y_train_1_opt.values.ravel(), cv=5, scoring='neg_root_mean_squared_error')\n",
        "      return metric.min()\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'eta': (0, 1),'gamma': (0,2),'max_depth':(1,10),'reg_lambda':(1,1),'reg_alpha':(0,0) }\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "  f=hyperparameter_function_xgboost,\n",
        "  pbounds=pbounds,\n",
        "  random_state=17,\n",
        "  allow_duplicate_points= True\n",
        ")\n",
        "\n",
        "optimizer.maximize(\n",
        "  init_points=10,\n",
        "  n_iter=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "do9vfC3le4oH"
      },
      "outputs": [],
      "source": [
        "xgb_regressor = XGBRegressor(eta=0.09569,gamma=0.05334,max_depth=4,reg_lambda=1, reg_alpha=0)\n",
        "xgb_regressor.fit(X_train_1_opt,y_train_1_opt )\n",
        "y_pred_1_opt = xgb_regressor.predict(X_test_1_opt)\n",
        "\n",
        "print(np.sqrt(mean_squared_error(y_test_1_opt, y_pred_1_opt)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCnCOABRRCHs"
      },
      "source": [
        "Results: 19.02\n",
        "\n",
        "Notes: The XGBoost results is also very solid and achieved comparable results to the paper which featured it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyQA_R8-WGm3"
      },
      "outputs": [],
      "source": [
        "# Define the function to optimize\n",
        "def evaluate_model(n_estimators, max_depth, min_samples_split, min_samples_leaf):\n",
        "    # Make sure parameters are integer\n",
        "    n_estimators = int(n_estimators)\n",
        "    max_depth = int(max_depth)\n",
        "    min_samples_split = int(min_samples_split)\n",
        "    min_samples_leaf = int(min_samples_leaf)\n",
        "\n",
        "    # Define the model with the parameters\n",
        "    model = ExtraTreesRegressor(n_estimators=n_estimators, max_depth=max_depth,\n",
        "                                min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n",
        "                                random_state=42, n_jobs=-1)\n",
        "\n",
        "    # Fit and predict\n",
        "    model.fit(X_train_1, y_train_1)\n",
        "    pred = model.predict(X_val_1)\n",
        "\n",
        "    # Calculate RMSE\n",
        "    rmse = mean_squared_error(y_val_1, pred, squared=False)\n",
        "\n",
        "    # We want to minimize RMSE, so we return the negative value\n",
        "    return -rmse\n",
        "\n",
        "\n",
        "# Define the bounds of the parameters\n",
        "param_bounds = {\n",
        "    'n_estimators': (100, 200),\n",
        "    'max_depth': (10, 30),\n",
        "    'min_samples_split': (2, 10),\n",
        "    'min_samples_leaf': (1, 4)\n",
        "}\n",
        "# Create the BayesianOptimization object\n",
        "optimizer = BayesianOptimization(\n",
        "    f=evaluate_model,\n",
        "    pbounds=param_bounds,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "# Perform the optimization\n",
        "optimizer.maximize(init_points=5, n_iter=12)\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Beste Hyperparameter-Kombination:\", optimizer.max['params'])\n",
        "#Beste Hyperparameter-Kombination: {'max_depth': 26.648852816008436, 'min_samples_leaf': 1.6370173320348285, 'min_samples_split': 3.454599737656805, 'n_estimators': 118.34045098534338}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhcCZk9MWGY_"
      },
      "outputs": [],
      "source": [
        "#Prediction on testdata\n",
        "extra_trees = ExtraTreesRegressor(n_estimators=118, max_depth=27, min_samples_split=3, min_samples_leaf=2, random_state=42)\n",
        "# Training\n",
        "extra_trees.fit(X_train_scaled_1, y_train_1)\n",
        "test_predictions = extra_trees.predict(X_test_scaled_1)\n",
        "test_rmse = sklearn.metrics.root_mean_squared_error(y_test_1, test_predictions)\n",
        "print(f\"Test RMSE: {test_rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxOz11HOYLL0"
      },
      "source": [
        "Result:\n",
        "Test RMSE: 62.9231\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QvYXX2MfWE4"
      },
      "source": [
        "The optimization for the first dataset went well. We reach comparable results we found in papers for this dataset with traditional ML approaches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b78kYcudRq6"
      },
      "source": [
        "# Dataset 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfsB1YhuQzGC"
      },
      "outputs": [],
      "source": [
        "## Load second dataset\n",
        "train_data_2_opt, test_data_2_opt,test_rul_data_2_opt = load_data(config_path=PATH_TO_CONFIG, dataset_num=2)\n",
        "cleaned_train_2_opt, cleaned_test_2_opt = clean_data(train_data_2_opt, test_data_2_opt, method=None, ignore_columns=['UnitNumber', 'Cycle','Operation Setting 2','Operation Setting 3','Sensor Measure 2','Sensor Measure 3','Sensor Measure 4','Sensor Measure 8','Sensor Measure 9','Sensor Measure 11','Sensor Measure 15','Sensor Measure 17'], threshold_missing=0.1, threshold_corr=0.5)\n",
        "\n",
        "# Train Val Split\n",
        "cl_train_2_opt, cl_val_2_opt = train_val_split_by_group(df = cleaned_train_2_opt,group = \"UnitNumber\",test_size = 0.18,n_splits = 2,random_state = 7)\n",
        "\n",
        "## RollingWindowParameter\n",
        "min_ts_2_opt = 17\n",
        "max_ts_2_opt = 18\n",
        "feature_list_ds_2 = [\"c3\", \"quantile\", \"mean\", \"median\", \"root_mean_square\", \"variance\", \"mean_abs_change\", \"standard_deviation\", \"skewness\", \"variation_coefficient\", \"last_location_of_maximum\", \"first_location_of_maximum\"]\n",
        "\n",
        "# Create RollingWindows\n",
        "#rwCreator = RollingWindowDatasetCreator(max_timeshift=max_ts_2_opt,min_timeshift=min_ts_2_opt,feature_extraction_mode= 'custom',feature_list=feature_list_ds_3)\n",
        "rwCreator = RollingWindowDatasetCreator(max_timeshift=max_ts_2_opt,min_timeshift=min_ts_2_opt,feature_extraction_mode= 'minimal',feature_list=[\"median\"])\n",
        "X_train_2_opt, y_train_2_opt = rwCreator._process_data(cl_train_2_opt, 'train')\n",
        "X_val_2_opt, y_val_2_opt = rwCreator._process_data(cl_val_2_opt, 'train')\n",
        "X_test_2_opt, y_test_2_opt = rwCreator._process_data(cleaned_test_2_opt, 'test', test_rul_data_2_opt)\n",
        "\n",
        "# Data Preprocessing\n",
        "y_train_2_opt = y_train_2_opt.clip(upper=125)\n",
        "scaler = StandardScaler()\n",
        "X_train_2_opt[2:] = scaler.fit_transform(X_train_2_opt[2:])\n",
        "X_val_2_opt[2:] = scaler.fit_transform(X_val_2_opt[2:])\n",
        "X_test_2_opt[2:] = scaler.fit_transform(X_test_2_opt[2:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tQyle6ii2B8"
      },
      "outputs": [],
      "source": [
        "def hyperparameter_function_knn(neighbours):\n",
        "      \"\"\" Function for hyperparameter optimization\n",
        "      \"\"\"\n",
        "      neighbours = neighbours.round().astype(int)\n",
        "      knn_regressor = KNeighborsRegressor(n_neighbors=neighbours)\n",
        "      metric = cross_val_score(knn_regressor, X=X_train_2_opt, y=y_train_2_opt, cv=5, scoring='neg_root_mean_squared_error')\n",
        "      return metric.min()\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'neighbours': (1, 750)}\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "  f=hyperparameter_function_knn,\n",
        "  pbounds=pbounds,\n",
        "  random_state=17,\n",
        "  allow_duplicate_points= True\n",
        ")\n",
        "\n",
        "optimizer.maximize(\n",
        "  init_points=10,\n",
        "  n_iter=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owCMUTHoi2Sf"
      },
      "outputs": [],
      "source": [
        "knn_regressor = KNeighborsRegressor(n_neighbors=20)\n",
        "knn_regressor.fit(X_train_2_opt,y_train_2_opt )\n",
        "y_pred_2_opt = knn_regressor.predict(X_test_2_opt)\n",
        "print(np.sqrt(mean_squared_error(y_test_2_opt, y_pred_2_opt)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PL8vaR6Sn3g"
      },
      "source": [
        "Results: 38.46\n",
        "\n",
        "Notes: Still KNeighbors performs as a one of the worst (when Lasso is out of competition)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0xhQMOUi2j8"
      },
      "outputs": [],
      "source": [
        "def hyperparameter_function_rf(n_estimators, max_features, ):\n",
        "      \"\"\" Function for hyperparameter optimization\n",
        "      \"\"\"\n",
        "      n_estimators = n_estimators.round().astype(int)\n",
        "      max_features = max_features.round().astype(int)\n",
        "\n",
        "      rf_regressor = RandomForestRegressor(n_estimators=n_estimators,max_features=max_features,random_state=17,n_jobs=-1)\n",
        "      metric = cross_val_score(rf_regressor, X=X_train_2_opt, y=y_train_2_opt.values.ravel(), cv=5, scoring='neg_root_mean_squared_error')\n",
        "      return metric.min()\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'n_estimators': (20, 500),'max_features': (1,1) }\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "  f=hyperparameter_function_rf,\n",
        "  pbounds=pbounds,\n",
        "  random_state=17,\n",
        "  allow_duplicate_points= True\n",
        ")\n",
        "\n",
        "optimizer.maximize(\n",
        "  init_points=10,\n",
        "  n_iter=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hQwPa9Li2zl"
      },
      "outputs": [],
      "source": [
        "rf_regressor = RandomForestRegressor(n_estimators=296,max_features=4)\n",
        "rf_regressor.fit(X_train_2_opt,y_train_2_opt.values.ravel())\n",
        "y_pred_2_opt = rf_regressor.predict(X_test_2_opt)\n",
        "print(np.sqrt(mean_squared_error(y_test_2_opt, y_pred_2_opt)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxMxbxgxSsCs"
      },
      "source": [
        "Results: 34.75\n",
        "\n",
        "Notes: This might result in weaker results.\n",
        "RandomForest still performs rather well and is at the same level as XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZG-XM-Ai3ER"
      },
      "outputs": [],
      "source": [
        "def hyperparameter_function_lasso(alpha, max_iter, ):\n",
        "      \"\"\" Function for hyperparameter optimization\n",
        "      \"\"\"\n",
        "      max_iter = max_iter.round().astype(int)\n",
        "\n",
        "      lasso_regressor = Lasso(alpha=alpha,max_iter=max_iter,random_state=17)\n",
        "      metric = cross_val_score(lasso_regressor, X=X_train_1_opt, y=y_train_1_opt.values.ravel(), cv=5, scoring='neg_root_mean_squared_error')\n",
        "      return metric.min()\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'alpha': (0.001, 1),'max_iter': (100,10000) }\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "  f=hyperparameter_function_lasso,\n",
        "  pbounds=pbounds,\n",
        "  random_state=17,\n",
        "  allow_duplicate_points= True\n",
        ")\n",
        "\n",
        "optimizer.maximize(\n",
        "  init_points=10,\n",
        "  n_iter=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zms-YrB-i3Vv"
      },
      "outputs": [],
      "source": [
        "# alpha= , max_iter= ->\n",
        "lasso_regressor = Lasso(alpha=0.9457,max_iter=694)\n",
        "lasso_regressor.fit(X_train_2_opt,y_train_2_opt )\n",
        "y_pred_2_opt = lasso_regressor.predict(X_test_2_opt)\n",
        "print(np.sqrt(mean_squared_error(y_test_2_opt, y_pred_2_opt)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C22qJ9LS0xe"
      },
      "source": [
        "Results: 86.99\n",
        "\n",
        "Notes: In this dataset Lasso doesnt perform good, but it seems to be far more stable then in the other cases with RMSE of over 1000."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4nsnsz3i4W5"
      },
      "outputs": [],
      "source": [
        "def hyperparameter_function_xgboost(eta, gamma ,max_depth,reg_lambda,reg_alpha ):\n",
        "      \"\"\" Function for hyperparameter optimization\n",
        "      \"\"\"\n",
        "      max_depth = max_depth.round().astype(int)\n",
        "\n",
        "      xgb_regressor = XGBRegressor(eta=eta,gamma=gamma,max_depth=max_depth,reg_lambda=reg_lambda, reg_alpha=reg_alpha)\n",
        "      metric = cross_val_score(xgb_regressor, X=X_train_1_opt, y=y_train_1_opt.values.ravel(), cv=5, scoring='neg_root_mean_squared_error')\n",
        "      return metric.min()\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'eta': (0, 1),'gamma': (0,2),'max_depth':(1,10),'reg_lambda':(1,1),'reg_alpha':(0,0) }\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "  f=hyperparameter_function_xgboost,\n",
        "  pbounds=pbounds,\n",
        "  random_state=17,\n",
        "  allow_duplicate_points= True\n",
        ")\n",
        "\n",
        "optimizer.maximize(\n",
        "  init_points=10,\n",
        "  n_iter=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rup6BBQSi4td"
      },
      "outputs": [],
      "source": [
        "xgb_regressor = XGBRegressor(eta=0.02803,gamma=0.8998,max_depth=6,reg_lambda=1, reg_alpha=0)\n",
        "xgb_regressor.fit(X_train_2_opt,y_train_2_opt )\n",
        "y_pred_2_opt = xgb_regressor.predict(X_test_2_opt)\n",
        "print(np.sqrt(mean_squared_error(y_test_2_opt, y_pred_2_opt)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RywEqx6RS5mv"
      },
      "source": [
        "Results: 34.11\n",
        "\n",
        "Notes: Is performing really well. If the was a need to further improve the result XGBoostRegressor would still possess options to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTxH_ELZWZrE"
      },
      "outputs": [],
      "source": [
        "#Due to a different pipeline, the ExtraTreesRegressor is not fully implemented in this notebook\n",
        "# for further refrence, please have a look in the notebook Niklas_Model_approaches\n",
        "\n",
        "#Bayesian Optimzier f√ºr Dataset 2\n",
        "# Define the function to optimize\n",
        "def evaluate_model(n_estimators, max_depth, min_samples_split, min_samples_leaf):\n",
        "    # Make sure parameters are integer\n",
        "    n_estimators = int(n_estimators)\n",
        "    max_depth = int(max_depth)\n",
        "    min_samples_split = int(min_samples_split)\n",
        "    min_samples_leaf = int(min_samples_leaf)\n",
        "\n",
        "    # Define the model with the parameters\n",
        "    model = ExtraTreesRegressor(n_estimators=n_estimators, max_depth=max_depth,\n",
        "                                min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n",
        "                                random_state=42, n_jobs=-1)\n",
        "\n",
        "    # Fit and predict\n",
        "    model.fit(X_train_2, y_train_2)\n",
        "    pred = model.predict(X_val_2)\n",
        "\n",
        "    # Calculate RMSE\n",
        "    rmse = mean_squared_error(y_val_2, pred, squared=False)\n",
        "\n",
        "    # We want to minimize RMSE, so we return the negative value\n",
        "    return -rmse\n",
        "\n",
        "\n",
        "# Define the bounds of the parameters\n",
        "param_bounds = {\n",
        "    'n_estimators': (100, 200),\n",
        "    'max_depth': (10, 30),\n",
        "    'min_samples_split': (2, 10),\n",
        "    'min_samples_leaf': (1, 4)\n",
        "}\n",
        "# Create the BayesianOptimization object\n",
        "optimizer = BayesianOptimization(\n",
        "    f=evaluate_model,\n",
        "    pbounds=param_bounds,\n",
        "    random_state=42,\n",
        ")\n",
        "# Perform the optimization\n",
        "optimizer.maximize(init_points=5, n_iter=12)\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Beste Hyperparameter-Kombination:\", optimizer.max['params'])\n",
        "#Best hyperparameter-combination: {'max_depth': 30.0, 'min_samples_leaf': 1.0, 'min_samples_split': 2.0, 'n_estimators': 151.63447537285498}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ks__2GHVXLP8"
      },
      "outputs": [],
      "source": [
        "#prediction on the testdata\n",
        "extra_trees = ExtraTreesRegressor(n_estimators=140, max_depth= 15, min_samples_leaf=5, random_state=42)\n",
        "\n",
        "# Training\n",
        "extra_trees.fit(X_train_scaled_2, y_train_2)\n",
        "test_predictions = extra_trees.predict(X_test_scaled_2)\n",
        "test_rmse = sklearn.metrics.root_mean_squared_error(y_test_2, test_predictions)\n",
        "print(f\"Test RMSE: {test_rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Result: Test RMSE: 79.9300"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PD53NKy9dZyC"
      },
      "source": [
        "# Dataset 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvKSE7X9Q0WG"
      },
      "outputs": [],
      "source": [
        "## Load third dataset\n",
        "train_data_3_opt, test_data_3_opt,test_rul_data_3_opt = load_data(config_path=PATH_TO_CONFIG, dataset_num=3)\n",
        "cleaned_train_3_opt, cleaned_test_3_opt = clean_data(train_data_3_opt, test_data_3_opt, method=None, ignore_columns=['UnitNumber', 'Cycle'], threshold_missing=0.1, threshold_corr=0.5)\n",
        "\n",
        "# Train Val Split\n",
        "cl_train_3_opt, cl_val_3_opt = train_val_split_by_group(df = cleaned_train_3_opt,group = \"UnitNumber\",test_size = 0.18,n_splits = 2,random_state = 7)\n",
        "\n",
        "## RollingWindowParameter\n",
        "min_ts_3_opt = 29\n",
        "max_ts_3_opt = 30\n",
        "feature_list_ds_3 = [\"c3\", \"quantile\", \"mean\", \"root_mean_square\", \"median\", \"time_reversal_asymmetry_statistic\", \"absolute_maximum\", \"maximum\", \"minimum\", \"agg_autocorrelation\", \"autocorrelation\" ]\n",
        "# Create RollingWindows\n",
        "rwCreator = RollingWindowDatasetCreator(max_timeshift=max_ts_3_opt,min_timeshift=min_ts_3_opt,feature_extraction_mode= 'custom',feature_list=feature_list_ds_3)\n",
        "X_train_3_opt, y_train_3_opt = rwCreator._process_data(cl_train_3_opt, 'train')\n",
        "X_val_3_opt, y_val_3_opt = rwCreator._process_data(cl_val_3_opt, 'train')\n",
        "X_test_3_opt, y_test_3_opt = rwCreator._process_data(cleaned_test_3_opt, 'test', test_rul_data_3_opt)\n",
        "\n",
        "# Data Preprocessing\n",
        "y_train_3_opt = y_train_3_opt.clip(upper=125)\n",
        "scaler = StandardScaler()\n",
        "X_train_3_opt[2:] = scaler.fit_transform(X_train_3_opt[2:])\n",
        "X_val_3_opt[2:] = scaler.fit_transform(X_val_3_opt[2:])\n",
        "X_test_3_opt[2:] = scaler.fit_transform(X_test_3_opt[2:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPACF76Lh9Wb"
      },
      "outputs": [],
      "source": [
        "def hyperparameter_function_knn(neighbours):\n",
        "      \"\"\" Function for hyperparameter optimization\n",
        "      \"\"\"\n",
        "      neighbours = neighbours.round().astype(int)\n",
        "      knn_regressor = KNeighborsRegressor(n_neighbors=neighbours)\n",
        "      metric = cross_val_score(knn_regressor, X=X_train_3_opt, y=y_train_3_opt, cv=5, scoring='neg_root_mean_squared_error')\n",
        "      return metric.min()\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'neighbours': (1, 750)}\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "  f=hyperparameter_function_knn,\n",
        "  pbounds=pbounds,\n",
        "  random_state=17,\n",
        "  allow_duplicate_points= True\n",
        ")\n",
        "\n",
        "optimizer.maximize(\n",
        "  init_points=10,\n",
        "  n_iter=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyQKfsheh9G8"
      },
      "outputs": [],
      "source": [
        "# Neigbors = 300\n",
        "knn_regressor = KNeighborsRegressor(n_neighbors=300)\n",
        "knn_regressor.fit(X_train_3_opt,y_train_3_opt )\n",
        "y_pred_3_opt = knn_regressor.predict(X_test_3_opt)\n",
        "print(np.sqrt(mean_squared_error(y_test_3_opt, y_pred_3_opt)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSEDXNWMaWB1"
      },
      "source": [
        "Result: 24.76\n",
        "\n",
        "Notes: Worse performance compare to dataset1 but still a solid result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjUEs1uPh966"
      },
      "outputs": [],
      "source": [
        "def hyperparameter_function_rf(n_estimators, max_features, ):\n",
        "      \"\"\" Function for hyperparameter optimization\n",
        "      \"\"\"\n",
        "      n_estimators = n_estimators.round().astype(int)\n",
        "      max_features = max_features.round().astype(int)\n",
        "\n",
        "      rf_regressor = RandomForestRegressor(n_estimators=n_estimators,max_features=max_features,random_state=17,n_jobs=-1)\n",
        "      metric = cross_val_score(rf_regressor, X=X_train_3_opt, y=y_train_3_opt.values.ravel(), cv=5, scoring='neg_root_mean_squared_error')\n",
        "      return metric.min()\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'n_estimators': (20, 500),'max_features': (1,1) }\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "  f=hyperparameter_function_rf,\n",
        "  pbounds=pbounds,\n",
        "  random_state=17,\n",
        "  allow_duplicate_points= True\n",
        ")\n",
        "\n",
        "optimizer.maximize(\n",
        "  init_points=10,\n",
        "  n_iter=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqRLbC7Uh9uF"
      },
      "outputs": [],
      "source": [
        "rf_regressor = RandomForestRegressor(n_estimators=333,max_features=1,random_state=17)\n",
        "rf_regressor.fit(X_train_3_opt,y_train_3_opt.values.ravel())\n",
        "y_pred_3_opt = rf_regressor.predict(X_test_3_opt)\n",
        "print(np.sqrt(mean_squared_error(y_test_3_opt, y_pred_3_opt)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYzeyeiman2w"
      },
      "source": [
        "Result: 23.36\n",
        "\n",
        "Notes: Differing from the from first the max_features parameter will no longer be optimized for.\n",
        "Strong performance. The optimization improved the results by quite a lot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTjhHvy6h-Zd"
      },
      "outputs": [],
      "source": [
        "def hyperparameter_function_lasso(alpha, max_iter, ):\n",
        "      \"\"\" Function for hyperparameter optimization\n",
        "      \"\"\"\n",
        "      max_iter = max_iter.round().astype(int)\n",
        "\n",
        "      lasso_regressor = Lasso(alpha=alpha,max_iter=max_iter,random_state=17)\n",
        "      metric = cross_val_score(lasso_regressor, X=X_train_3_opt, y=y_train_3_opt.values.ravel(), cv=5, scoring='neg_root_mean_squared_error')\n",
        "      return metric.min()\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'alpha': (0.001, 1),'max_iter': (100,10000) }\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "  f=hyperparameter_function_lasso,\n",
        "  pbounds=pbounds,\n",
        "  random_state=17,\n",
        "  allow_duplicate_points= True\n",
        ")\n",
        "\n",
        "optimizer.maximize(\n",
        "  init_points=10,\n",
        "  n_iter=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QuebvlXh-O9"
      },
      "outputs": [],
      "source": [
        "lasso_regressor = Lasso(alpha=0.1923,max_iter=772,random_state=17)\n",
        "lasso_regressor.fit(X_train_3_opt,y_train_3_opt )\n",
        "y_pred_3_opt = lasso_regressor.predict(X_test_3_opt)\n",
        "print(np.sqrt(mean_squared_error(y_test_3_opt, y_pred_3_opt)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UupWDyGayG0"
      },
      "source": [
        "Result: 8080.99\n",
        "\n",
        "Notes: Similar to the previous optimizations. Lasso disappoints again despite almost an hour in optimization time in colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMIMMc5Jh-3t"
      },
      "outputs": [],
      "source": [
        "def hyperparameter_function_xgboost(eta, gamma ,max_depth,reg_lambda,reg_alpha ):\n",
        "      \"\"\" Function for hyperparameter optimization\n",
        "      \"\"\"\n",
        "      max_depth = max_depth.round().astype(int)\n",
        "\n",
        "      xgb_regressor = XGBRegressor(eta=eta,gamma=gamma,max_depth=max_depth,reg_lambda=reg_lambda, reg_alpha=reg_alpha)\n",
        "      metric = cross_val_score(xgb_regressor, X=X_train_3_opt, y=y_train_3_opt.values.ravel(), cv=5, scoring='neg_root_mean_squared_error')\n",
        "      return metric.min()\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'eta': (0, 1),'gamma': (0,2),'max_depth':(1,10),'reg_lambda':(1,1),'reg_alpha':(0,0) }\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "  f=hyperparameter_function_xgboost,\n",
        "  pbounds=pbounds,\n",
        "  random_state=17,\n",
        "  allow_duplicate_points= True\n",
        ")\n",
        "\n",
        "optimizer.maximize(\n",
        "  init_points=10,\n",
        "  n_iter=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pciUjD15h-vP"
      },
      "outputs": [],
      "source": [
        "xgb_regressor = XGBRegressor(eta=0.2079,gamma=1.342,max_depth=2,reg_lambda=1, reg_alpha=0)\n",
        "xgb_regressor.fit(X_train_3_opt,y_train_3_opt )\n",
        "y_pred_3_opt = xgb_regressor.predict(X_test_3_opt)\n",
        "\n",
        "print(np.sqrt(mean_squared_error(y_test_3_opt, y_pred_3_opt)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Tcsiv9Dsnfa"
      },
      "outputs": [],
      "source": [
        "#Due to a different pipeline, the ExtraTreesRegressor is not fully implemented in this notebook\n",
        "# for further refrence, please have a look in the notebook Niklas_Model_approaches\n",
        "\n",
        "#Bayesian Optimzier f√ºr Dataset 3\n",
        "# Define the function to optimize\n",
        "def evaluate_model(n_estimators, max_depth, min_samples_split, min_samples_leaf):\n",
        "    # Make sure parameters are integer\n",
        "    n_estimators = int(n_estimators)\n",
        "    max_depth = int(max_depth)\n",
        "    min_samples_split = int(min_samples_split)\n",
        "    min_samples_leaf = int(min_samples_leaf)\n",
        "\n",
        "    # Define the model with the parameters\n",
        "    model = ExtraTreesRegressor(n_estimators=n_estimators, max_depth=max_depth,\n",
        "                                min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n",
        "                                random_state=42, n_jobs=-1)\n",
        "\n",
        "    # Fit and predict\n",
        "    model.fit(X_train_3, y_train_3)\n",
        "    pred = model.predict(X_val_3)\n",
        "\n",
        "    # Calculate RMSE\n",
        "    rmse = mean_squared_error(y_val_3, pred, squared=False)\n",
        "\n",
        "    # We want to minimize RMSE, so we return the negative value\n",
        "    return -rmse\n",
        "\n",
        "\n",
        "# Define the bounds of the parameters\n",
        "param_bounds = {\n",
        "    'n_estimators': (100, 200),\n",
        "    'max_depth': (10, 30),\n",
        "    'min_samples_split': (2, 10),\n",
        "    'min_samples_leaf': (1, 4)\n",
        "}\n",
        "# Create the BayesianOptimization object\n",
        "optimizer = BayesianOptimization(\n",
        "    f=evaluate_model,\n",
        "    pbounds=param_bounds,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "# Perform the optimization\n",
        "optimizer.maximize(init_points=5, n_iter=12)\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Beste Hyperparameter-Kombination:\", optimizer.max['params'])\n",
        "#Best hyperparameter-combination: {'max_depth': 30.0, 'min_samples_leaf': 1.0, 'min_samples_split': 2.0, 'n_estimators': 167.97080791958393}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z01DYKy7sq7x"
      },
      "outputs": [],
      "source": [
        "#prediction on the testdata auf den Testdaten\n",
        "extra_trees = ExtraTreesRegressor(n_estimators=168, max_depth=30, min_samples_split=2, min_samples_leaf=1, random_state=42)\n",
        "# Training\n",
        "extra_trees.fit(X_train_scaled_3, y_train_3)\n",
        "test_predictions = extra_trees.predict(X_test_scaled_3)\n",
        "test_rmse = sklearn.metrics.root_mean_squared_error(y_test_3, test_predictions)\n",
        "print(f\"Test RMSE: {test_rmse:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OzHJtzVa7rz"
      },
      "source": [
        "Result: Test RMSE: 77.1180\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Notes: This time the XGBoostRegressors performs the best over the custom feature_list. Which is inline with the promised results. (The custom feature_list is only used by KNeighbors, RandomForest, Lasso and XGBoost)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfXErIg_dbJ8"
      },
      "source": [
        "# Dataset 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "J03_3L1kQ1wf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 19:18:49 [\u001b[34msrc.data_loading:43\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loading data set 4...\u001b[0m\n",
            "2024-06-01 19:18:49 [\u001b[34msrc.data_loading:72\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loaded raw data for dataset 4.\u001b[0m\n",
            "2024-06-01 19:18:49 [\u001b[34msrc.data_loading:73\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Train Data: (61249, 26)\u001b[0m\n",
            "2024-06-01 19:18:49 [\u001b[34msrc.data_loading:74\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test Data: (41214, 26)\u001b[0m\n",
            "2024-06-01 19:18:49 [\u001b[34msrc.data_loading:75\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test RUL Data: (248, 1)\u001b[0m\n",
            "2024-06-01 19:18:49 [\u001b[34msrc.data_cleaning:134\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Cleaning train and test data...\u001b[0m\n",
            "2024-06-01 19:18:49 [\u001b[34msrc.data_cleaning:136\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Formatting column types...\u001b[0m\n",
            "2024-06-01 19:18:49 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
            "2024-06-01 19:18:49 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
            "2024-06-01 19:18:49 [\u001b[34msrc.data_cleaning:141\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Handling duplicates...\u001b[0m\n",
            "2024-06-01 19:18:49 [\u001b[34msrc.data_cleaning:146\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Removing outliers...\u001b[0m\n",
            "2024-06-01 19:18:49 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
            "2024-06-01 19:18:49 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
            "2024-06-01 19:18:49 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
            "2024-06-01 19:18:49 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
            "2024-06-01 19:18:49 [\u001b[34msrc.data_cleaning:150\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Filter features based train data...\u001b[0m\n",
            "2024-06-01 19:18:49 [\u001b[34msrc.data_cleaning:26\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 features with missing values above the threshold of 0.1.\u001b[0m\n",
            "2024-06-01 19:18:49 [\u001b[34msrc.data_cleaning:46\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 features with only a single unique value: []\u001b[0m\n",
            "2024-06-01 19:18:49 [\u001b[34msrc.data_cleaning:103\u001b[0m] [DEBUG\u001b[0m] >>>> Found 25 uncorrelated features with a correlation threshold of 0.5: ['UnitNumber', 'Operation Setting 1', 'Operation Setting 2', 'Operation Setting 3', 'Sensor Measure 1', 'Sensor Measure 2', 'Sensor Measure 3', 'Sensor Measure 4', 'Sensor Measure 5', 'Sensor Measure 6', 'Sensor Measure 7', 'Sensor Measure 8', 'Sensor Measure 9', 'Sensor Measure 10', 'Sensor Measure 11', 'Sensor Measure 12', 'Sensor Measure 13', 'Sensor Measure 14', 'Sensor Measure 15', 'Sensor Measure 16', 'Sensor Measure 17', 'Sensor Measure 18', 'Sensor Measure 19', 'Sensor Measure 20', 'Sensor Measure 21']\u001b[0m\n",
            "2024-06-01 19:18:49 [\u001b[34msrc.data_cleaning:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Dropping features based on missing values, single unique values, and no target correlation...\u001b[0m\n",
            "2024-06-01 19:18:49 [\u001b[34msrc.data_cleaning:172\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Data cleaning completed.\u001b[0m\n",
            "2024-06-01 19:18:49 [\u001b[34msrc.data_cleaning:173\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original train DataFrame shape: (61249, 12), Resulting train DataFrame shape: (61249, 12)\u001b[0m\n",
            "2024-06-01 19:18:49 [\u001b[34msrc.data_cleaning:174\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original test DataFrame shape: (41214, 12), Resulting test DataFrame shape: (41214, 12)\u001b[0m\n",
            "2024-06-01 19:18:49 [\u001b[34msrc.data_splitting:47\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Train set contains 204 different engines --> in total 50356\u001b[0m\n",
            "2024-06-01 19:18:49 [\u001b[34msrc.data_splitting:48\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>>  Test set contains 45 different engines --> in total 10893\u001b[0m\n",
            "2024-06-01 19:18:49 [\u001b[34msrc.rolling_window_creator:134\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Creating rolling windows for train data...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rolling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:06<00:00,  3.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 19:18:56 [\u001b[34msrc.rolling_window_creator:140\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Extracting features for train data...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Feature Extraction: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:48<00:00,  2.43s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 19:19:52 [\u001b[34msrc.rolling_window_creator:148\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Calculating target for train data...\u001b[0m\n",
            "2024-06-01 19:19:52 [\u001b[34msrc.rolling_window_creator:134\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Creating rolling windows for train data...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rolling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 10.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 19:19:54 [\u001b[34msrc.rolling_window_creator:140\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Extracting features for train data...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Feature Extraction: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:08<00:00,  2.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 19:20:03 [\u001b[34msrc.rolling_window_creator:148\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Calculating target for train data...\u001b[0m\n",
            "2024-06-01 19:20:03 [\u001b[34msrc.rolling_window_creator:134\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Creating rolling windows for test data...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rolling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 19:20:08 [\u001b[34msrc.rolling_window_creator:140\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Extracting features for test data...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Feature Extraction: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 13.14it/s]\n"
          ]
        }
      ],
      "source": [
        "## Load fourth dataset\n",
        "train_data_4_opt, test_data_4_opt,test_rul_data_4_opt = load_data(config_path=PATH_TO_CONFIG, dataset_num=4)\n",
        "cleaned_train_4_opt, cleaned_test_4_opt = clean_data(train_data_4_opt, test_data_4_opt, method=None, ignore_columns=['UnitNumber', 'Cycle','Operation Setting 2','Operation Setting 3','Sensor Measure 2','Sensor Measure 3','Sensor Measure 4','Sensor Measure 8','Sensor Measure 9','Sensor Measure 11','Sensor Measure 15','Sensor Measure 17'], threshold_missing=0.1, threshold_corr=0.5)\n",
        "\n",
        "# Train Val Split\n",
        "cl_train_4_opt, cl_val_4_opt = train_val_split_by_group(df = cleaned_train_4_opt,group = \"UnitNumber\",test_size = 0.18,n_splits = 2,random_state = 7)\n",
        "\n",
        "## RollingWindowParameter\n",
        "min_ts_4_opt = 17\n",
        "max_ts_4_opt = 18\n",
        "feature_list_ds_4 = [\"c3\", \"quantile\", \"mean\", \"median\", \"root_mean_square\", \"variance\", \"mean_abs_change\", \"standard_deviation\", \"skewness\", \"variation_coefficient\", \"last_location_of_maximum\", \"first_location_of_maximum\"]\n",
        "\n",
        "#rwCreator = RollingWindowDatasetCreator(max_timeshift=max_ts_4_opt,min_timeshift=min_ts_4_opt,feature_extraction_mode= 'custom',feature_list=feature_list_ds_4)\n",
        "rwCreator = RollingWindowDatasetCreator(max_timeshift=max_ts_4_opt,min_timeshift=min_ts_4_opt,feature_extraction_mode= 'minimal',feature_list=[\"median\"])\n",
        "\n",
        "\n",
        "X_train_4_opt, y_train_4_opt = rwCreator._process_data(cl_train_4_opt, 'train')\n",
        "X_val_4_opt, y_val_4_opt = rwCreator._process_data(cl_val_4_opt, 'train')\n",
        "X_test_4_opt, y_test_4_opt = rwCreator._process_data(cleaned_test_4_opt, 'test', test_rul_data_4_opt)\n",
        "\n",
        "y_train_4_opt = y_train_4_opt.clip(upper=125)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_4_opt[2:] = scaler.fit_transform(X_train_4_opt[2:])\n",
        "X_val_4_opt[2:] = scaler.fit_transform(X_val_4_opt[2:])\n",
        "X_test_4_opt[2:] = scaler.fit_transform(X_test_4_opt[2:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0H5NnGMbSEg"
      },
      "source": [
        "Notes: This is by far the worst performing dataset in regards to the achieved RMSE by our models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CceTN3NbmSV"
      },
      "outputs": [],
      "source": [
        "def hyperparameter_function_knn(neighbours):\n",
        "      \"\"\" Function for hyperparameter optimization\n",
        "      \"\"\"\n",
        "      neighbours = neighbours.round().astype(int)\n",
        "      knn_regressor = KNeighborsRegressor(n_neighbors=neighbours)\n",
        "      metric = cross_val_score(knn_regressor, X=X_train_4_opt, y=y_train_4_opt, cv=5, scoring='neg_root_mean_squared_error')\n",
        "      return metric.min()\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'neighbours': (1, 750)}\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "  f=hyperparameter_function_knn,\n",
        "  pbounds=pbounds,\n",
        "  random_state=17,\n",
        "  allow_duplicate_points= True\n",
        ")\n",
        "\n",
        "optimizer.maximize(\n",
        "  init_points=10,\n",
        "  n_iter=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4vQbXvYjwmL"
      },
      "outputs": [],
      "source": [
        "# Neigbors =\n",
        "knn_regressor = KNeighborsRegressor(n_neighbors=15)\n",
        "knn_regressor.fit(X_train_4_opt,y_train_4_opt )\n",
        "y_pred_4_opt = knn_regressor.predict(X_test_4_opt)\n",
        "print(np.sqrt(mean_squared_error(y_test_4_opt, y_pred_4_opt)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKXNwqWabMsS"
      },
      "source": [
        "Result for knn: 42.10\n",
        "\n",
        "Notes: For this dataset KNeighbors is squarely in the middlefield."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXq2xg14jx3h"
      },
      "outputs": [],
      "source": [
        "def hyperparameter_function_rf(n_estimators, max_features, ):\n",
        "      \"\"\" Function for hyperparameter optimization\n",
        "      \"\"\"\n",
        "      n_estimators = n_estimators.round().astype(int)\n",
        "      max_features = max_features.round().astype(int)\n",
        "\n",
        "      rf_regressor = RandomForestRegressor(n_estimators=n_estimators,max_features=max_features,random_state=17,n_jobs=-1)\n",
        "      metric = cross_val_score(rf_regressor, X=X_train_4_opt, y=y_train_4_opt.values.ravel(), cv=5, scoring='neg_root_mean_squared_error')\n",
        "      return metric.min()\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'n_estimators': (20, 500),'max_features': (1,1) }\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "  f=hyperparameter_function_rf,\n",
        "  pbounds=pbounds,\n",
        "  random_state=17,\n",
        "  allow_duplicate_points= True\n",
        ")\n",
        "\n",
        "optimizer.maximize(\n",
        "  init_points=10,\n",
        "  n_iter=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_x_TArL7jxwD"
      },
      "outputs": [],
      "source": [
        "rf_regressor = RandomForestRegressor(n_estimators=264,max_features=1)\n",
        "rf_regressor.fit(X_train_4_opt,y_train_4_opt.values.ravel())\n",
        "y_pred_4_opt = rf_regressor.predict(X_test_4_opt)\n",
        "print(np.sqrt(mean_squared_error(y_test_4_opt, y_pred_4_opt)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGK4kdthbcIK"
      },
      "source": [
        "Result for RandomForestRegressor: 39.53\n",
        "\n",
        "Notes: The best model for dataset 4 and the only one to breach below 40. Despite extensive optimization it wasn't possible to further reduce the RMSE with the RandomForest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKaR3T4Vjxn4"
      },
      "outputs": [],
      "source": [
        "def hyperparameter_function_lasso(alpha, max_iter, ):\n",
        "      \"\"\" Function for hyperparameter optimization\n",
        "      \"\"\"\n",
        "      max_iter = max_iter.round().astype(int)\n",
        "\n",
        "      lasso_regressor = Lasso(alpha=alpha,max_iter=max_iter,random_state=17)\n",
        "      metric = cross_val_score(lasso_regressor, X=X_train_4_opt, y=y_train_4_opt.values.ravel(), cv=5, scoring='neg_root_mean_squared_error')\n",
        "      return metric.min()\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'alpha': (0.001, 1),'max_iter': (100,10000) }\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "  f=hyperparameter_function_lasso,\n",
        "  pbounds=pbounds,\n",
        "  random_state=17,\n",
        "  allow_duplicate_points= True\n",
        ")\n",
        "\n",
        "optimizer.maximize(\n",
        "  init_points=10,\n",
        "  n_iter=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "0xWa442UjxfK",
        "outputId": "f209bad4-e849-4cdd-e799-0d232d1078a4"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Lasso' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a1404f2be5b7>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# alpha= , max_iter= ->\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlasso_regressor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLasso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9457\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m694\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlasso_regressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_4_opt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_4_opt\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_pred_4_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlasso_regressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_4_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_4_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_4_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Lasso' is not defined"
          ]
        }
      ],
      "source": [
        "lasso_regressor = Lasso(alpha=0.9457,max_iter=694)\n",
        "lasso_regressor.fit(X_train_4_opt,y_train_4_opt )\n",
        "y_pred_4_opt = lasso_regressor.predict(X_test_4_opt)\n",
        "print(np.sqrt(mean_squared_error(y_test_4_opt, y_pred_4_opt)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM8WK0Dpb4U0"
      },
      "source": [
        "Result for Lasso: 50.39\n",
        "\n",
        "Notes: The best overall result for the Lasso Regressor despite the most difficult dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsGT8w9KjxRL"
      },
      "outputs": [],
      "source": [
        "def hyperparameter_function_xgboost(eta, gamma ,max_depth,reg_lambda,reg_alpha ):\n",
        "      \"\"\" Function for hyperparameter optimization\n",
        "      \"\"\"\n",
        "      max_depth = max_depth.round().astype(int)\n",
        "\n",
        "      xgb_regressor = XGBRegressor(eta=eta,gamma=gamma,max_depth=max_depth,reg_lambda=reg_lambda, reg_alpha=reg_alpha)\n",
        "      metric = cross_val_score(xgb_regressor, X=X_train_4_opt, y=y_train_4_opt.values.ravel(), cv=5, scoring='neg_root_mean_squared_error')\n",
        "      return metric.min()\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'eta': (0, 1),'gamma': (0,2),'max_depth':(1,10),'reg_lambda':(1,1),'reg_alpha':(0,0) }\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "  f=hyperparameter_function_xgboost,\n",
        "  pbounds=pbounds,\n",
        "  random_state=17,\n",
        "  allow_duplicate_points= True\n",
        ")\n",
        "\n",
        "optimizer.maximize(\n",
        "  init_points=10,\n",
        "  n_iter=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1bc5CwnjxDF"
      },
      "outputs": [],
      "source": [
        "# eta= , gamma= , max_depth= ,lambda=1 , alpha=0 ->\n",
        "xgb_regressor = XGBRegressor(eta=0.1149,gamma=0.4352,max_depth=4,reg_lambda=1, reg_alpha=0)\n",
        "xgb_regressor.fit(X_train_4_opt,y_train_4_opt )\n",
        "y_pred_4_opt = xgb_regressor.predict(X_test_4_opt)\n",
        "print(np.sqrt(mean_squared_error(y_test_4_opt, y_pred_4_opt)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqyEDpgbcE5l"
      },
      "source": [
        "Result for XGBRegressor: 43.26\n",
        "\n",
        "Notes: Solid result but really didn't outperform any other regressor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMe36ysHPzfr"
      },
      "source": [
        "# Findings\n",
        "\n",
        "Certain regressors are heavily hyperparameter dependent, especially lasso.\n",
        "The feature selection was a mixed bag. While some regressors seemed to profit others less so. A more extensive evaluation could be performed in the future.\n",
        "The datasets 2 & 4 were as promised by the EDA much more dificult to get a lower rmse. While datasets 1 & 3 we were able to reduce the rmse to the low twenties. This was not directly the case for 2 and 4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEQWL6rTP1SL"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wab3ajvQcWE8"
      },
      "source": [
        "RandomForest and XGBoostRegressor were overall our best performers. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z1-qtd6dIIt"
      },
      "source": [
        "Final Notes: The original notebooks are in the referenced Repository in case any of our merged results do not reflect the communicated results, they should be used to compare.\n",
        "\n",
        "Part of this was done in colab. The part can be found under the following link:\n",
        "https://colab.research.google.com/drive/1F_hpmXcxYoJT3LsvXjF65c3_lZ7ltEr6?usp=sharing"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
