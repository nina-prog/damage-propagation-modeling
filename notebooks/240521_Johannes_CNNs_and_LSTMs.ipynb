{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T11:40:46.027177Z",
     "start_time": "2024-05-31T11:40:45.887651Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T11:40:46.167622Z",
     "start_time": "2024-05-31T11:40:46.028177Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Topic: EX2 - Turbofan RUL Prediction\n",
    "**Task**: Predict the remaining useful life (RUL) of turbofan engines based on given sensor data (time series data). It is a forcasting problem, where the goal is to predict the number of cycles an engine will last before it fails.\n",
    "**Data**: Turbofan engine degradation simulation data (NASA) - [Link](https://data.nasa.gov/dataset/Turbofan-Engine-Degradation-Simulation-Data-Set/vrks-gjie). See also in the topic [introduction notebook](https://github.com/nina-prog/damage-propagation-modeling/blob/2fb8c1a1102a48d7abbf04e4031807790a913a99/notebooks/Turbofan%20remaining%20useful%20life%20Prediction.ipynb).\n",
    "\n",
    "**Subtasks**:\n",
    "1. Perform a deep **exploratory data analysis (EDA)** on the given data.\n",
    "2. Implement a more efficient **sliding window method** for time series data analysis. -> ğŸ¯ **Focus on this task**\n",
    "3. Apply **traditional machine learning methods** (SOTA) to predict the remaining useful life. Includes data preparation, feature extraction, feature selection, model selection, and model parameter optimization.\n",
    "4. Create **neural network models** to predict the remaining useful life. Includes different architectures like Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), or Attention Models. Note: You can search for SOTA research papers and reproduce current state-of-the-art models.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports + Settings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# third-party libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "import time\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T11:40:57.284505Z",
     "start_time": "2024-05-31T11:40:54.170858Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "# source code\n",
    "from src.data_loading import load_data, load_config\n",
    "from src.data_splitting import train_val_split_by_group\n",
    "from src.nn_utils import create_sliding_window, create_sliding_window_test\n",
    "from src.rolling_window_creator import calculate_RUL\n",
    "from src.data_processing import apply_padding_on_train_data_and_test_data, drop_samples_with_clipped_values, extract_peaks_from_sensor_signal\n",
    "from src.nn_util.nn_models.ligthning.cnnModel2 import CNNModel2 as CNNModel\n",
    "from src.nn_util.datamodule.lightning.turbofanDatamodule import TurbofanDatamodule\n",
    "from src.data_cleaning import clean_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T11:40:58.314480Z",
     "start_time": "2024-05-31T11:40:57.288506Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "# settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"Set2\")\n",
    "sns.set(rc={\"figure.dpi\":100, 'savefig.dpi':200})\n",
    "sns.set_context('notebook')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T11:40:58.548123Z",
     "start_time": "2024-05-31T11:40:58.316490Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "np.random.seed(42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T11:40:58.780406Z",
     "start_time": "2024-05-31T11:40:58.550161Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Paths"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Make sure to execute this cell only once for one kernel session, before running any other cell below.\n",
    "os.chdir(\"../\") # set working directory to root of project\n",
    "os.getcwd() # check current working directory"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T11:40:59.031132Z",
     "start_time": "2024-05-31T11:40:58.782919Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Johannes\\\\PycharmProjects\\\\damage-propagation-modeling'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "PATH_TO_CONFIG = \"configs/config.yaml\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T11:40:59.264335Z",
     "start_time": "2024-05-31T11:40:59.032645Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Config + Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "config = load_config(PATH_TO_CONFIG) # config is dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T11:40:59.514637Z",
     "start_time": "2024-05-31T11:40:59.266289Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": "train_data, test_data, test_RUL_data = load_data(config_path=PATH_TO_CONFIG, dataset_num=2)",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T11:41:00.386572Z",
     "start_time": "2024-05-31T11:40:59.516635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-31 13:40:59 [\u001B[34msrc.data_loading:43\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Loading data set 2...\u001B[0m\n",
      "2024-05-31 13:41:00 [\u001B[34msrc.data_loading:72\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Loaded raw data for dataset 2.\u001B[0m\n",
      "2024-05-31 13:41:00 [\u001B[34msrc.data_loading:73\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Train Data: (53759, 26)\u001B[0m\n",
      "2024-05-31 13:41:00 [\u001B[34msrc.data_loading:74\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Test Data: (33991, 26)\u001B[0m\n",
      "2024-05-31 13:41:00 [\u001B[34msrc.data_loading:75\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Test RUL Data: (259, 1)\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ğŸ“ << Subtask X: TOPIC >>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[TEMPLATE]\n",
    "\n",
    "Findings:\n",
    "* Interpretation of plots\n",
    "* or other key take aways from previous code"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "TODO: Write down explanation of pipeline!"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T11:41:07.760716Z",
     "start_time": "2024-05-31T11:41:07.411894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# some hyperparameters\n",
    "time_column = 'Cycle'\n",
    "group_column = 'UnitNumber'\n",
    "\n",
    "window_size = 30\n",
    "clip_value = 125\n",
    "apply_data_cleaning = True\n",
    "# If activated, adds for every sensor a new column with the commutative sum of the peaks\n",
    "apply_peaks_generation = True\n",
    "\n",
    "# Apply scaler. The order in the list represents the order in which they are applied\n",
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "robust_scaler = RobustScaler()\n",
    "scaler = [std_scaler, minmax_scaler, robust_scaler]\n",
    "\n",
    "# The model_type can be 'CNN_2D' or 'CNN_1D' and defines what type of convolutional layers has been used.\n",
    "model_type = 'CNN_1D' \n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(21)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T11:22:02.357087Z",
     "start_time": "2024-05-31T11:21:46.153634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if apply_peaks_generation:\n",
    "    train_data = extract_peaks_from_sensor_signal(train_data)\n",
    "    test_data = extract_peaks_from_sensor_signal(test_data)\n",
    "    \n",
    "if apply_data_cleaning:\n",
    "    train_data, test_data = clean_data(train_data, test_data, method=None, ignore_columns=['UnitNumber', 'Cycle'], threshold_missing=0.1, threshold_corr=0.0, contamination=0.05)\n",
    "    \n",
    "# Add column RUL to train_data\n",
    "train_data = calculate_RUL(train_data, time_column, group_column, clip_value)\n",
    "\n",
    "train_data, test_data = apply_padding_on_train_data_and_test_data(train_data=train_data, test_data=test_data, window_size=window_size)\n",
    "\n",
    "X_test, _ = create_sliding_window_test(test_data, column_RUL=False, drop_columns=['UnitNumber'])\n",
    "y_test = test_RUL_data.values\n",
    "\n",
    "train, val = train_val_split_by_group(train_data, test_size=0.2, random_state=12)\n",
    "\n",
    "X_train, y_train = create_sliding_window(train, window_size=window_size)  #, drop_columns=['UnitNumber', 'Cycle, 'RUL'])\n",
    "X_val, y_val = create_sliding_window(val, window_size=window_size)  #, drop_columns=['UnitNumber', 'Cycle', 'RUL'])\n",
    "\n",
    "X_train_temp, y_train_temp = drop_samples_with_clipped_values(X_train, y_train, clip_value)\n",
    "X_val_temp, y_val_temp = drop_samples_with_clipped_values(X_val, y_val, clip_value)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-31 13:21:46 [\u001B[34msrc.data_cleaning:134\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Cleaning train and test data...\u001B[0m\n",
      "2024-05-31 13:21:46 [\u001B[34msrc.data_cleaning:136\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Formatting column types...\u001B[0m\n",
      "2024-05-31 13:21:46 [\u001B[34msrc.data_cleaning:69\u001B[0m] [DEBUG\u001B[0m] >>>> Found 0 categorical columns: []\u001B[0m\n",
      "2024-05-31 13:21:46 [\u001B[34msrc.data_cleaning:69\u001B[0m] [DEBUG\u001B[0m] >>>> Found 0 categorical columns: []\u001B[0m\n",
      "2024-05-31 13:21:46 [\u001B[34msrc.data_cleaning:141\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Handling duplicates...\u001B[0m\n",
      "2024-05-31 13:21:46 [\u001B[34msrc.data_cleaning:146\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Removing outliers...\u001B[0m\n",
      "2024-05-31 13:21:46 [\u001B[34msrc.outlier_detection:150\u001B[0m] [DEBUG\u001B[0m] >>>> Removing outliers using method: None ...\u001B[0m\n",
      "2024-05-31 13:21:46 [\u001B[34msrc.outlier_detection:162\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001B[0m\n",
      "2024-05-31 13:21:46 [\u001B[34msrc.outlier_detection:150\u001B[0m] [DEBUG\u001B[0m] >>>> Removing outliers using method: None ...\u001B[0m\n",
      "2024-05-31 13:21:46 [\u001B[34msrc.outlier_detection:162\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001B[0m\n",
      "2024-05-31 13:21:46 [\u001B[34msrc.data_cleaning:150\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Filter features based train data...\u001B[0m\n",
      "2024-05-31 13:21:46 [\u001B[34msrc.data_cleaning:26\u001B[0m] [DEBUG\u001B[0m] >>>> Found 0 features with missing values above the threshold of 0.1.\u001B[0m\n",
      "2024-05-31 13:21:46 [\u001B[34msrc.data_cleaning:46\u001B[0m] [DEBUG\u001B[0m] >>>> Found 0 features with only a single unique value: []\u001B[0m\n",
      "2024-05-31 13:21:46 [\u001B[34msrc.data_cleaning:103\u001B[0m] [DEBUG\u001B[0m] >>>> Found 0 uncorrelated features with a correlation threshold of 0.0: []\u001B[0m\n",
      "2024-05-31 13:21:46 [\u001B[34msrc.data_cleaning:162\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Dropping features based on missing values, single unique values, and no target correlation...\u001B[0m\n",
      "2024-05-31 13:21:46 [\u001B[34msrc.data_cleaning:172\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Data cleaning completed.\u001B[0m\n",
      "2024-05-31 13:21:46 [\u001B[34msrc.data_cleaning:173\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Original train DataFrame shape: (53759, 47), Resulting train DataFrame shape: (53759, 47)\u001B[0m\n",
      "2024-05-31 13:21:46 [\u001B[34msrc.data_cleaning:174\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Original test DataFrame shape: (33991, 47), Resulting test DataFrame shape: (33991, 47)\u001B[0m\n",
      "2024-05-31 13:21:46 [\u001B[34msrc.data_processing:37\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The minimum number of cycles of a motor before the padding is 21.\u001B[0m\n",
      "2024-05-31 13:21:46 [\u001B[34msrc.data_processing:43\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The padding value is 9.\u001B[0m\n",
      "2024-05-31 13:21:49 [\u001B[34msrc.data_processing:52\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The minimum number of cycles of a motor with the padding is 21.\u001B[0m\n",
      "2024-05-31 13:21:49 [\u001B[34msrc.data_splitting:47\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> Train set contains 208 different engines --> in total 44779\u001B[0m\n",
      "2024-05-31 13:21:49 [\u001B[34msrc.data_splitting:48\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>>  Test set contains 52 different engines --> in total 11320\u001B[0m\n",
      "2024-05-31 13:22:02 [\u001B[34msrc.data_processing:110\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The median of the frequency of each RUL value in the data is 208.0.\u001B[0m\n",
      "2024-05-31 13:22:02 [\u001B[34msrc.data_processing:111\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The the frequency of the clipped RUL value in the data is 13019.\u001B[0m\n",
      "2024-05-31 13:22:02 [\u001B[34msrc.data_processing:117\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The the frequency of the clipped RUL value after dropping is 416.\u001B[0m\n",
      "2024-05-31 13:22:02 [\u001B[34msrc.data_processing:125\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The number of samples in the data has dropped from 38747 to 26144.\u001B[0m\n",
      "2024-05-31 13:22:02 [\u001B[34msrc.data_processing:110\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The median of the frequency of each RUL value in the data is 52.0.\u001B[0m\n",
      "2024-05-31 13:22:02 [\u001B[34msrc.data_processing:111\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The the frequency of the clipped RUL value in the data is 3364.\u001B[0m\n",
      "2024-05-31 13:22:02 [\u001B[34msrc.data_processing:117\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The the frequency of the clipped RUL value after dropping is 104.\u001B[0m\n",
      "2024-05-31 13:22:02 [\u001B[34msrc.data_processing:125\u001B[0m] [\u001B[32mINFO\u001B[0m] >>>> The number of samples in the data has dropped from 9812 to 6552.\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Scale the data"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T11:22:14.431623Z",
     "start_time": "2024-05-31T11:22:02.359086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train_shape = X_train.shape\n",
    "X_val_shape = X_val.shape\n",
    "X_test_shape = X_test.shape\n",
    "\n",
    "# Note: Do not normalize the cycle value! That is why we start with one\n",
    "for single_scaler in scaler:\n",
    "    for i in range(1, X_train.shape[-1]):\n",
    "        X_train[:, :, i] = single_scaler.fit_transform(X_train[:, :, i])\n",
    "        X_val[:, :, i] = single_scaler.transform(X_val[:, :, i])\n",
    "        X_test[:, :, i] = single_scaler.transform(X_test[:, :, i])"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Change data types of arrays to float32 and swap axes if necessary:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T11:22:20.167727Z",
     "start_time": "2024-05-31T11:22:19.636137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_type = 'CNN_1D'\n",
    "\n",
    "print(X_train.shape)\n",
    "if model_type == 'CNN_1D':\n",
    "    X_train = np.swapaxes(X_train, 1, 2)\n",
    "elif model_type == 'CNN_2D':\n",
    "    X_train = np.swapaxes(X_train, 1, 2)\n",
    "    X_train = X_train[:, np.newaxis, :, :]\n",
    "X_train = np.array(X_train, dtype=np.float32)\n",
    "y_train = np.array(y_train, dtype=np.float32)\n",
    "print(X_train.shape)\n",
    "\n",
    "print(X_val.shape)\n",
    "if model_type == 'CNN_1D':\n",
    "    X_val = np.swapaxes(X_val, 1, 2)\n",
    "elif model_type == 'CNN_2D':\n",
    "    X_val = np.swapaxes(X_val, 1, 2)\n",
    "    X_val = X_val[:, np.newaxis, :, :]\n",
    "X_val = np.array(X_val, dtype=np.float32)\n",
    "y_val = np.array(y_val, dtype=np.float32)\n",
    "print(X_val.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "if model_type == 'CNN_1D':\n",
    "    X_test = np.swapaxes(X_test, 1, 2)\n",
    "elif model_type == 'CNN_2D':\n",
    "    X_test = np.swapaxes(X_test, 1, 2)\n",
    "    X_test = X_test[:, np.newaxis, :, :]\n",
    "X_test = np.array(X_test, dtype=np.float32)\n",
    "y_test = np.array(y_test, dtype=np.float32)\n",
    "print(X_test.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38747, 30, 46)\n",
      "(38747, 46, 30)\n",
      "(9812, 30, 46)\n",
      "(9812, 46, 30)\n",
      "(259, 30, 46)\n",
      "(259, 46, 30)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Only CNN"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Inspiration: Paper Dynamic predictive maintenance for multiple components using data-driven\n",
    "probabilistic RUL prognostics: The case of turbofan engines"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T11:22:52.424543Z",
     "start_time": "2024-05-31T11:22:51.582645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Select hyperparameters of trainer!\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\")\n",
    "trainer = Trainer(min_epochs=1, max_epochs=150, callbacks=[checkpoint_callback], deterministic=True)\n",
    "datamodule = TurbofanDatamodule(batch_size=128)\n",
    "datamodule.set_train_dataset(X_train, y_train)\n",
    "datamodule.set_val_dataset(X_val, y_val)\n",
    "datamodule.set_predict_dataset(X_test)\n",
    "datamodule.set_test_dataset(X_test, y_test[:, 0])\n",
    "model = CNNModel(lr=0.001, window_size=window_size, features=46, dropout_rate=0.2)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T11:31:09.770144Z",
     "start_time": "2024-05-31T11:22:53.295471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "# For visualization write 'tensorboard --logdir=lightning_logs/' in console\n",
    "\n",
    "trainer.fit(model, datamodule=datamodule)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name        | Type    | Params\n",
      "----------------------------------------\n",
      "0 | loss        | MSELoss | 0     \n",
      "1 | dropout     | Dropout | 0     \n",
      "2 | layer1_conv | Conv1d  | 9.2 K \n",
      "3 | layer2_conv | Conv1d  | 8.0 K \n",
      "4 | layer3_conv | Conv1d  | 8.0 K \n",
      "5 | layer4_conv | Conv1d  | 8.0 K \n",
      "6 | fc1         | Linear  | 153 K \n",
      "7 | fc2         | Linear  | 8.3 K \n",
      "8 | fc3         | Linear  | 65    \n",
      "----------------------------------------\n",
      "195 K     Trainable params\n",
      "0         Non-trainable params\n",
      "195 K     Total params\n",
      "0.782     Total estimated model params size (MB)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T11:31:13.898136Z",
     "start_time": "2024-05-31T11:31:12.478414Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.validate(model, datamodule=datamodule, ckpt_path='best')",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at C:\\Users\\Johannes\\PycharmProjects\\damage-propagation-modeling\\lightning_logs\\version_0\\checkpoints\\epoch=40-step=12423.ckpt\n",
      "Loaded model weights from the checkpoint at C:\\Users\\Johannes\\PycharmProjects\\damage-propagation-modeling\\lightning_logs\\version_0\\checkpoints\\epoch=40-step=12423.ckpt\n",
      "C:\\Users\\Johannes\\anaconda3\\envs\\PSDA\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e27f37e32cbd49ad978c99ad240bd3ee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001B[1m \u001B[0m\u001B[1m     Validate metric     \u001B[0m\u001B[1m \u001B[0mâ”ƒ\u001B[1m \u001B[0m\u001B[1m      DataLoader 0       \u001B[0m\u001B[1m \u001B[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001B[36m \u001B[0m\u001B[36m        val_loss         \u001B[0m\u001B[36m \u001B[0mâ”‚\u001B[35m \u001B[0m\u001B[35m    330.914306640625     \u001B[0m\u001B[35m \u001B[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\">      Validate metric      </span>â”ƒ<span style=\"font-weight: bold\">       DataLoader 0        </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">     330.914306640625      </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 330.914306640625}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T11:31:18.850026Z",
     "start_time": "2024-05-31T11:31:18.205324Z"
    }
   },
   "cell_type": "code",
   "source": "pred = trainer.test(model, datamodule=datamodule, ckpt_path='best')",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at C:\\Users\\Johannes\\PycharmProjects\\damage-propagation-modeling\\lightning_logs\\version_0\\checkpoints\\epoch=40-step=12423.ckpt\n",
      "Loaded model weights from the checkpoint at C:\\Users\\Johannes\\PycharmProjects\\damage-propagation-modeling\\lightning_logs\\version_0\\checkpoints\\epoch=40-step=12423.ckpt\n",
      "C:\\Users\\Johannes\\anaconda3\\envs\\PSDA\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e22b2faeef340c2b79a9fb633f029a0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001B[1m \u001B[0m\u001B[1m       Test metric       \u001B[0m\u001B[1m \u001B[0mâ”ƒ\u001B[1m \u001B[0m\u001B[1m      DataLoader 0       \u001B[0m\u001B[1m \u001B[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001B[36m \u001B[0m\u001B[36m        test_loss        \u001B[0m\u001B[36m \u001B[0mâ”‚\u001B[35m \u001B[0m\u001B[35m    878.3591918945312    \u001B[0m\u001B[35m \u001B[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\">        Test metric        </span>â”ƒ<span style=\"font-weight: bold\">       DataLoader 0        </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\">     878.3591918945312     </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T11:32:02.267Z",
     "start_time": "2024-05-31T11:32:00.801763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "from lightning import LightningModule\n",
    "import torch \n",
    "model_from_checkpoint = LightningModule.load_from_checkpoint(\"C:/Users/Johannes/PycharmProjects/damage-propagation-modeling/lightning_logs/version_134/checkpoints/epoch=114-step=26105.ckpt\")\n",
    "# disable randomness, dropout, etc...\n",
    "model_from_checkpoint.eval()\n",
    "# predict with the model\n",
    "pred = model_from_checkpoint(torch.tensor(X_test)).detach().numpy()\n",
    "root_mean_squared_error(pred, torch.tensor(y_test))"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/Johannes/PycharmProjects/damage-propagation-modeling/lightning_logs/version_134/checkpoints/epoch=114-step=26105.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[22], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlightning\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LightningModule\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m \n\u001B[1;32m----> 4\u001B[0m model_from_checkpoint \u001B[38;5;241m=\u001B[39m \u001B[43mLightningModule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_from_checkpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mC:/Users/Johannes/PycharmProjects/damage-propagation-modeling/lightning_logs/version_134/checkpoints/epoch=114-step=26105.ckpt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# disable randomness, dropout, etc...\u001B[39;00m\n\u001B[0;32m      6\u001B[0m model_from_checkpoint\u001B[38;5;241m.\u001B[39meval()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\PSDA\\lib\\site-packages\\lightning\\pytorch\\utilities\\model_helpers.py:125\u001B[0m, in \u001B[0;36m_restricted_classmethod_impl.__get__.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    120\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m instance \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_scripting:\n\u001B[0;32m    121\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[0;32m    122\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe classmethod `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmethod\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` cannot be called on an instance.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    123\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Please call it on the class type and make sure the return value is used.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    124\u001B[0m     )\n\u001B[1;32m--> 125\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmethod(\u001B[38;5;28mcls\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\PSDA\\lib\\site-packages\\lightning\\pytorch\\core\\module.py:1581\u001B[0m, in \u001B[0;36mLightningModule.load_from_checkpoint\u001B[1;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001B[0m\n\u001B[0;32m   1492\u001B[0m \u001B[38;5;129m@_restricted_classmethod\u001B[39m\n\u001B[0;32m   1493\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_from_checkpoint\u001B[39m(\n\u001B[0;32m   1494\u001B[0m     \u001B[38;5;28mcls\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1499\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m   1500\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Self:\n\u001B[0;32m   1501\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001B[39;00m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;124;03m    passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1579\u001B[0m \n\u001B[0;32m   1580\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1581\u001B[0m     loaded \u001B[38;5;241m=\u001B[39m _load_from_checkpoint(\n\u001B[0;32m   1582\u001B[0m         \u001B[38;5;28mcls\u001B[39m,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[0;32m   1583\u001B[0m         checkpoint_path,\n\u001B[0;32m   1584\u001B[0m         map_location,\n\u001B[0;32m   1585\u001B[0m         hparams_file,\n\u001B[0;32m   1586\u001B[0m         strict,\n\u001B[0;32m   1587\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   1588\u001B[0m     )\n\u001B[0;32m   1589\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(Self, loaded)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\PSDA\\lib\\site-packages\\lightning\\pytorch\\core\\saving.py:63\u001B[0m, in \u001B[0;36m_load_from_checkpoint\u001B[1;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001B[0m\n\u001B[0;32m     61\u001B[0m map_location \u001B[38;5;241m=\u001B[39m map_location \u001B[38;5;129;01mor\u001B[39;00m _default_map_location\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m pl_legacy_patch():\n\u001B[1;32m---> 63\u001B[0m     checkpoint \u001B[38;5;241m=\u001B[39m \u001B[43mpl_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcheckpoint_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmap_location\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;66;03m# convert legacy checkpoints to the new format\u001B[39;00m\n\u001B[0;32m     66\u001B[0m checkpoint \u001B[38;5;241m=\u001B[39m _pl_migrate_checkpoint(\n\u001B[0;32m     67\u001B[0m     checkpoint, checkpoint_path\u001B[38;5;241m=\u001B[39m(checkpoint_path \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(checkpoint_path, (\u001B[38;5;28mstr\u001B[39m, Path)) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m     68\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\PSDA\\lib\\site-packages\\lightning\\fabric\\utilities\\cloud_io.py:56\u001B[0m, in \u001B[0;36m_load\u001B[1;34m(path_or_url, map_location)\u001B[0m\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mhub\u001B[38;5;241m.\u001B[39mload_state_dict_from_url(\n\u001B[0;32m     52\u001B[0m         \u001B[38;5;28mstr\u001B[39m(path_or_url),\n\u001B[0;32m     53\u001B[0m         map_location\u001B[38;5;241m=\u001B[39mmap_location,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[0;32m     54\u001B[0m     )\n\u001B[0;32m     55\u001B[0m fs \u001B[38;5;241m=\u001B[39m get_filesystem(path_or_url)\n\u001B[1;32m---> 56\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mfs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath_or_url\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mload(f, map_location\u001B[38;5;241m=\u001B[39mmap_location)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\PSDA\\lib\\site-packages\\fsspec\\spec.py:1298\u001B[0m, in \u001B[0;36mAbstractFileSystem.open\u001B[1;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001B[0m\n\u001B[0;32m   1296\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1297\u001B[0m     ac \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mautocommit\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_intrans)\n\u001B[1;32m-> 1298\u001B[0m     f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_open(\n\u001B[0;32m   1299\u001B[0m         path,\n\u001B[0;32m   1300\u001B[0m         mode\u001B[38;5;241m=\u001B[39mmode,\n\u001B[0;32m   1301\u001B[0m         block_size\u001B[38;5;241m=\u001B[39mblock_size,\n\u001B[0;32m   1302\u001B[0m         autocommit\u001B[38;5;241m=\u001B[39mac,\n\u001B[0;32m   1303\u001B[0m         cache_options\u001B[38;5;241m=\u001B[39mcache_options,\n\u001B[0;32m   1304\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   1305\u001B[0m     )\n\u001B[0;32m   1306\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m compression \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1307\u001B[0m         \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mfsspec\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompression\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m compr\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\PSDA\\lib\\site-packages\\fsspec\\implementations\\local.py:191\u001B[0m, in \u001B[0;36mLocalFileSystem._open\u001B[1;34m(self, path, mode, block_size, **kwargs)\u001B[0m\n\u001B[0;32m    189\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_mkdir \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m    190\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmakedirs(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_parent(path), exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m--> 191\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m LocalFileOpener(path, mode, fs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\PSDA\\lib\\site-packages\\fsspec\\implementations\\local.py:355\u001B[0m, in \u001B[0;36mLocalFileOpener.__init__\u001B[1;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001B[0m\n\u001B[0;32m    353\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompression \u001B[38;5;241m=\u001B[39m get_compression(path, compression)\n\u001B[0;32m    354\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblocksize \u001B[38;5;241m=\u001B[39m io\u001B[38;5;241m.\u001B[39mDEFAULT_BUFFER_SIZE\n\u001B[1;32m--> 355\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\PSDA\\lib\\site-packages\\fsspec\\implementations\\local.py:360\u001B[0m, in \u001B[0;36mLocalFileOpener._open\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    358\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf\u001B[38;5;241m.\u001B[39mclosed:\n\u001B[0;32m    359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mautocommit \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode:\n\u001B[1;32m--> 360\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    361\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompression:\n\u001B[0;32m    362\u001B[0m             compress \u001B[38;5;241m=\u001B[39m compr[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompression]\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'C:/Users/Johannes/PycharmProjects/damage-propagation-modeling/lightning_logs/version_134/checkpoints/epoch=114-step=26105.ckpt'"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T11:32:03.514575Z",
     "start_time": "2024-05-31T11:32:02.900742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pred = model(torch.tensor(X_test)).detach().numpy()\n",
    "model.eval()\n",
    "root_mean_squared_error(pred, torch.tensor(y_test))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.64586"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T15:06:25.960289Z",
     "start_time": "2024-05-30T15:06:25.359986Z"
    }
   },
   "cell_type": "code",
   "source": "pred",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 51.862972,  67.89349 ,  26.498564,  95.374695,  79.14195 ,\n",
       "        99.26876 ,  70.10647 ,  60.206066,  78.893234,  75.16445 ,\n",
       "        68.97512 ,  75.68833 , 106.93179 ,  31.458738, 111.88981 ,\n",
       "        70.50948 , 121.09294 , 133.38309 ,  88.324196,  93.30068 ,\n",
       "        20.652811, 116.759964,  92.665634,  16.95975 , 100.89441 ,\n",
       "        16.584661, 142.51463 , 109.31307 , 110.87555 ,  78.90963 ,\n",
       "        85.24842 ,  45.71385 ,  86.61725 ,  63.96631 , 103.03752 ,\n",
       "       113.956795, 127.93344 ,  37.64642 ,   9.226481,  43.823063,\n",
       "       112.50242 ,  74.93342 , 115.161705,  78.06849 , 146.9397  ,\n",
       "         9.05889 , 125.63947 ,  41.578754, 106.79327 ,  13.959208,\n",
       "        63.320995, 141.03683 ,  81.06794 , 112.00667 ,  65.58863 ,\n",
       "        62.390923,  57.31184 ,  38.89475 ,  36.391994,  92.64628 ,\n",
       "       134.31812 ,  64.032   ,  80.11501 ,  34.75124 ,  94.39489 ,\n",
       "        83.86167 ,  88.42881 , 127.99012 , 112.436226, 118.27959 ,\n",
       "        51.29933 ,  80.9912  , 141.13812 , 146.47522 ,  49.28496 ,\n",
       "        11.749313, 104.11753 ,  23.058588,  33.81395 , 131.55167 ,\n",
       "        21.847551,  11.284406,  91.10576 ,  81.15719 ,  70.78307 ,\n",
       "        18.305836,  93.13181 , 162.69672 ,  47.337345,  64.771324,\n",
       "       113.64936 ,  19.879753,  90.09495 ,  13.075133, 113.97542 ,\n",
       "        94.58347 , 129.0021  ,  11.363343,  25.780993,  20.930113],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T15:06:26.955901Z",
     "start_time": "2024-05-30T15:06:26.341986Z"
    }
   },
   "cell_type": "code",
   "source": "y_test[:, 0]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 44.,  51.,  27., 120., 101.,  99.,  71.,  55.,  55.,  66.,  77.,\n",
       "       115., 115.,  31., 108.,  56., 136., 132.,  85.,  56.,  18., 119.,\n",
       "        78.,   9.,  58.,  11.,  88., 144., 124.,  89.,  79.,  55.,  71.,\n",
       "        65.,  87., 137., 145.,  22.,   8.,  41., 131., 115., 128.,  69.,\n",
       "       111.,   7., 137.,  55., 135.,  11.,  78., 120.,  87.,  87.,  55.,\n",
       "        93.,  88.,  40.,  49., 128., 129.,  58., 117.,  28., 115.,  87.,\n",
       "        92., 103., 100.,  63.,  35.,  45.,  99., 117.,  45.,  27.,  86.,\n",
       "        20.,  18., 133.,  15.,   6., 145., 104.,  56.,  25.,  68., 144.,\n",
       "        41.,  51.,  81.,  14.,  67.,  10., 127., 113., 123.,  17.,   8.,\n",
       "        28.], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
