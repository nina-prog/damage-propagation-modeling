{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:37.900316500Z",
     "start_time": "2024-05-09T15:31:37.830560800Z"
    },
    "collapsed": false
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:37.980322700Z",
     "start_time": "2024-05-09T15:31:37.900316500Z"
    },
    "collapsed": false
   },
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Topic: EX2 - Turbofan RUL Prediction\n",
    "**Task**: Predict the remaining useful life (RUL) of turbofan engines based on given sensor data (time series data). It is a forcasting problem, where the goal is to predict the number of cycles an engine will last before it fails.\n",
    "**Data**: Turbofan engine degradation simulation data (NASA) - [Link](https://data.nasa.gov/dataset/Turbofan-Engine-Degradation-Simulation-Data-Set/vrks-gjie). See also in the topic [introduction notebook](https://github.com/nina-prog/damage-propagation-modeling/blob/2fb8c1a1102a48d7abbf04e4031807790a913a99/notebooks/Turbofan%20remaining%20useful%20life%20Prediction.ipynb).\n",
    "\n",
    "**Subtasks**:\n",
    "1. Perform a deep **exploratory data analysis (EDA)** on the given data.\n",
    "2. Implement a more efficient **sliding window method** for time series data analysis. -> üéØ **Focus on this task**\n",
    "3. Apply **traditional machine learning methods** (SOTA) to predict the remaining useful life. Includes data preparation, feature extraction, feature selection, model selection, and model parameter optimization.\n",
    "4. Create **neural network models** to predict the remaining useful life. Includes different architectures like Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), or Attention Models. Note: You can search for SOTA research papers and reproduce current state-of-the-art models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports + Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:38.193478100Z",
     "start_time": "2024-05-09T15:31:37.980322700Z"
    },
    "collapsed": false
   },
   "source": [
    "# third-party libraries\n",
    "#os.getcwd() # check current working directory\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:38.400456500Z",
     "start_time": "2024-05-09T15:31:38.304026600Z"
    },
    "collapsed": false
   },
   "source": [
    "# settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"Set2\")\n",
    "sns.set(rc={\"figure.dpi\":100, 'savefig.dpi':200})\n",
    "sns.set_context('notebook')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:38.492590100Z",
     "start_time": "2024-05-09T15:31:38.400456500Z"
    },
    "collapsed": false
   },
   "source": [
    "np.random.seed(42)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "os.chdir(\"../\") # set working directory to root of project\n",
    "#os.getcwd() # check current working directory"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:38.652221700Z",
     "start_time": "2024-05-09T15:31:38.560578800Z"
    },
    "collapsed": false
   },
   "source": [
    "# source code\n",
    "from src.utils import load_data, load_config, train_val_split_by_group\n",
    "import src.nn_utils as nu\n",
    "from src.utils import load_data, load_config\n",
    "from src.rolling_window_creator import RollingWindowDatasetCreator, calculate_RUL\n",
    "from src.data_cleaning import identify_missing_values, identify_single_unique_features, format_dtype, clean_data\n",
    "import src.nn_utils as nu\n",
    "PATH_TO_CONFIG = \"configs/config.yaml\""
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load Config + Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:38.732034900Z",
     "start_time": "2024-05-09T15:31:38.652221700Z"
    },
    "collapsed": false
   },
   "source": [
    "config = load_config(PATH_TO_CONFIG) # config is dict"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:39.154853100Z",
     "start_time": "2024-05-09T15:31:38.740553100Z"
    },
    "collapsed": false
   },
   "source": [
    "train_data, test_data, test_data_RUL = load_data(config_path=PATH_TO_CONFIG, dataset_num=1)\n",
    "train_data = calculate_RUL(train_data, time_column= \"Cycle\", group_column= \"UnitNumber\")\n",
    "\n",
    "#train_data, val_data = train_val_split_by_group(train_data)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# üìç << Subtask NN: Try out NN Architecture >>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "[TEMPLATE]\n",
    "\n",
    "Findings:\n",
    "* Interpretation of plots\n",
    "* or other key take aways from previous code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "train_data.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "## Scale Data Min Max\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def scale_data(df):\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    float_columns = df.select_dtypes(include=float).columns.tolist()\n",
    "    scaled_data = scaler.fit_transform(df[float_columns])\n",
    "    df[float_columns] = scaled_data\n",
    "\n",
    "    return df\n",
    "\n",
    "## create sliding window\n",
    "def create_sliding_window(df, window_size = 30, drop_columns = [\"UnitNumber\", \"Cycle\", \"RUL\"]):\n",
    "    \n",
    "    number_engines = df[\"UnitNumber\"].unique()\n",
    "    X, y = [], []\n",
    "\n",
    "    for engine in number_engines:\n",
    "\n",
    "        ## get all data with engine same engine type\n",
    "        temp = df[df[\"UnitNumber\"] == engine]\n",
    "        assert temp[\"UnitNumber\"].unique() == engine\n",
    "\n",
    "        ## loop over group\n",
    "        for i in range(len(temp) - window_size + 1):\n",
    "\n",
    "            X_temp = temp.iloc[i : (i + window_size)].drop(columns = drop_columns)\n",
    "            Y_temp = temp.iloc[(i + window_size - 1)][\"RUL\"]\n",
    "            assert len(X_temp) == 30\n",
    "            X.append(X_temp.to_numpy())\n",
    "            y.append(Y_temp)\n",
    "            if i == (len(temp) - window_size):\n",
    "                assert Y_temp == 1\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return X, y"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "def scale_data(df):\n",
    "    \"\"\"\n",
    "    Scales the numerical columns in the DataFrame using MinMaxScaler.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Scaled DataFrame.\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # Select float columns\n",
    "    float_columns = df.select_dtypes(include=float).columns.tolist()\n",
    "    \n",
    "    # Scale the data\n",
    "    scaled_data = scaler.fit_transform(df[float_columns])\n",
    "    \n",
    "    # Update the DataFrame with scaled data\n",
    "    df[float_columns] = scaled_data\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_sliding_window(df, window_size=30, drop_columns=[\"UnitNumber\", \"Cycle\", \"RUL\"]):\n",
    "    \"\"\"\n",
    "    Creates a sliding window of data for time series prediction.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input DataFrame containing time series data.\n",
    "        window_size (int): Size of the sliding window.\n",
    "        drop_columns (list): List of columns to drop from the input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing X (input) and y (output) arrays.\n",
    "    \"\"\"\n",
    "    number_engines = df[\"UnitNumber\"].unique()\n",
    "    X, y = [], []\n",
    "\n",
    "    for engine in number_engines:\n",
    "        # Get data for the current engine\n",
    "        temp = df[df[\"UnitNumber\"] == engine]\n",
    "        assert temp[\"UnitNumber\"].unique() == engine\n",
    "\n",
    "        for i in range(len(temp) - window_size + 1):\n",
    "            # Extract windowed data and RUL for each window\n",
    "            X_temp = temp.iloc[i : (i + window_size)].drop(columns=drop_columns)\n",
    "            Y_temp = temp.iloc[(i + window_size - 1)][\"RUL\"]\n",
    "            assert len(X_temp) == window_size\n",
    "            X.append(X_temp.to_numpy())\n",
    "            y.append(Y_temp)\n",
    "            if i == (len(temp) - window_size):\n",
    "                assert Y_temp == 1\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return X, y\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "train_data = scale_data(train_data)\n",
    "X_train, y_train = create_sliding_window(train_data)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (seq_len, batch_size, feature_size)\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, feature_size: int, num_heads: int, num_layers: int, dropout: float = 0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.feature_size = feature_size\n",
    "        \n",
    "        # Positional Encoding\n",
    "        self.positional_encoding = PositionalEncoding(feature_size, dropout)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc_out = nn.Linear(feature_size, 1) \n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (seq_len, batch_size, feature_size)\n",
    "            mask: Optional mask of shape (seq_len, seq_len)\n",
    "            \n",
    "        Returns:\n",
    "            out: Tensor of shape (batch_size, 1) for regression\n",
    "        \"\"\"\n",
    "        # Add positional encoding\n",
    "        x = self.positional_encoding(x)# (seq_len, batch_size, feature_size)\n",
    "        x = x.to(torch.float32)\n",
    "\n",
    "        # Pass through transformer encoder\n",
    "        x = self.transformer_encoder(x, mask)  # (seq_len, batch_size, feature_size)\n",
    "    \n",
    "        # Take the mean across the sequence length dimension\n",
    "        x = torch.mean(x, dim=0)  # (batch_size, feature_size)\n",
    "        \n",
    "        # Output layer\n",
    "        out = self.fc_out(x)  # (batch_size, 1)\n",
    "    \n",
    "        return out"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "# Example dataset class\n",
    "class TurbofanDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = torch.from_numpy(data).to(torch.float32)\n",
    "        self.targets = torch.from_numpy(targets).to(torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "# Training function\n",
    "def train_model(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    count = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        if count % 44 == 0:\n",
    "            print(f\"--> {count}/{len(dataloader)}\")\n",
    "        count += 1\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "# Main function to execute training\n",
    "if __name__ == \"__main__\":\n",
    "    # Example data (replace with actual data loading)\n",
    "    seq_len, batch_size, feature_size = X_train.shape[1], 32, X_train.shape[2]\n",
    "    num_heads, num_layers = 4, 2\n",
    "    num_epochs = 20\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    # Create dataset and dataloaders\n",
    "    dataset = TurbofanDataset(X_train, y_train)\n",
    "    train_size = int(0.1 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize model, criterion, optimizer\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = TransformerModel(feature_size, num_heads, num_layers).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_model(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss = evaluate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "source": [
    "len(train_loader)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "source": [
    "transformer_model = nn.Transformer(d_model = 128, nhead=2, num_encoder_layers=2)\n",
    "src = torch.rand((10, 32, 128))\n",
    "tgt = torch.rand((20, 32, 128))\n",
    "print(src.dtype)\n",
    "print(tgt.dtype)\n",
    "out = transformer_model(src, tgt)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=num_heads, dropout=dropout)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "src = torch.rand((30, 32, 24))\n",
    "print(src.dtype)\n",
    "transformer_encoder(src)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:39.220343800Z",
     "start_time": "2024-05-09T15:31:39.160435700Z"
    },
    "collapsed": false
   },
   "source": [
    "# [TEMPLATE] - save processed data (as pickle)\n",
    "df = pd.DataFrame()\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "df.to_pickle(f\"{config['paths']['processed_data_dir']}ex2_topic_{timestamp}.pkl\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:39.318667400Z",
     "start_time": "2024-05-09T15:31:39.220343800Z"
    },
    "collapsed": false
   },
   "source": [
    "# [TEMPLATE] - save data predictions (as csv)\n",
    "df = pd.DataFrame()\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "df.to_csv(f\"{config['paths']['prediction_dir']}ex2_topic_{timestamp}.csv\", sep=',', decimal='.')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:39.485116500Z",
     "start_time": "2024-05-09T15:31:39.320350800Z"
    },
    "collapsed": false
   },
   "source": [
    "# [TEMPLATE] - save plot results (as png)\n",
    "fig = plt.figure(figsize=(9, 6))\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "fig.savefig(f\"{config['paths']['plot_dir']}ex2_topic_{timestamp}.png\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:39.522027500Z",
     "start_time": "2024-05-09T15:31:39.474058600Z"
    },
    "collapsed": false
   },
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
