{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:37.900316500Z",
     "start_time": "2024-05-09T15:31:37.830560800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:37.980322700Z",
     "start_time": "2024-05-09T15:31:37.900316500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Topic: EX2 - Turbofan RUL Prediction\n",
    "**Task**: Predict the remaining useful life (RUL) of turbofan engines based on given sensor data (time series data). It is a forcasting problem, where the goal is to predict the number of cycles an engine will last before it fails.\n",
    "**Data**: Turbofan engine degradation simulation data (NASA) - [Link](https://data.nasa.gov/dataset/Turbofan-Engine-Degradation-Simulation-Data-Set/vrks-gjie). See also in the topic [introduction notebook](https://github.com/nina-prog/damage-propagation-modeling/blob/2fb8c1a1102a48d7abbf04e4031807790a913a99/notebooks/Turbofan%20remaining%20useful%20life%20Prediction.ipynb).\n",
    "\n",
    "**Subtasks**:\n",
    "1. Perform a deep **exploratory data analysis (EDA)** on the given data.\n",
    "2. Implement a more efficient **sliding window method** for time series data analysis. -> üéØ **Focus on this task**\n",
    "3. Apply **traditional machine learning methods** (SOTA) to predict the remaining useful life. Includes data preparation, feature extraction, feature selection, model selection, and model parameter optimization.\n",
    "4. Create **neural network models** to predict the remaining useful life. Includes different architectures like Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), or Attention Models. Note: You can search for SOTA research papers and reproduce current state-of-the-art models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports + Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:38.193478100Z",
     "start_time": "2024-05-09T15:31:37.980322700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# third-party libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:38.400456500Z",
     "start_time": "2024-05-09T15:31:38.304026600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"Set2\")\n",
    "sns.set(rc={\"figure.dpi\":100, 'savefig.dpi':200})\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:38.492590100Z",
     "start_time": "2024-05-09T15:31:38.400456500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/frbroy/Library/Mobile Documents/com~apple~CloudDocs/KIT/SoSe2024/PSDA/damage-propagation-modeling'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure to execute this cell only once for one kernel session, before running any other cell below.\n",
    "os.chdir(\"../\") # set working directory to root of project\n",
    "os.getcwd() # check current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:38.652221700Z",
     "start_time": "2024-05-09T15:31:38.560578800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# source code\n",
    "from src.utils import load_data, load_config, train_val_split_by_group\n",
    "from src.data_preprocessing import calculate_RUL\n",
    "PATH_TO_CONFIG = \"configs/config.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load Config + Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:38.732034900Z",
     "start_time": "2024-05-09T15:31:38.652221700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = load_config(PATH_TO_CONFIG) # config is dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:39.154853100Z",
     "start_time": "2024-05-09T15:31:38.740553100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-18 15:39:09 [\u001b[34msrc.utils:62\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loading data set 1...\u001b[0m\n",
      "2024-05-18 15:39:09 [\u001b[34msrc.utils:91\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loaded raw data for dataset 1.\u001b[0m\n",
      "2024-05-18 15:39:09 [\u001b[34msrc.utils:92\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Train Data: (20631, 26)\u001b[0m\n",
      "2024-05-18 15:39:09 [\u001b[34msrc.utils:93\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test Data: (13096, 26)\u001b[0m\n",
      "2024-05-18 15:39:09 [\u001b[34msrc.utils:94\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test RUL Data: (100, 1)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, test_data_RUL = load_data(config_path=PATH_TO_CONFIG, dataset_num=1)\n",
    "train_data = calculate_RUL(train_data, time_column= \"Cycle\", group_column= \"UnitNumber\")\n",
    "\n",
    "#train_data, val_data = train_val_split_by_group(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# üìç << Subtask NN: Try out NN Architecture >>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "[TEMPLATE]\n",
    "\n",
    "Findings:\n",
    "* Interpretation of plots\n",
    "* or other key take aways from previous code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UnitNumber</th>\n",
       "      <th>Cycle</th>\n",
       "      <th>Operation Setting 1</th>\n",
       "      <th>Operation Setting 2</th>\n",
       "      <th>Operation Setting 3</th>\n",
       "      <th>Sensor Measure 1</th>\n",
       "      <th>Sensor Measure 2</th>\n",
       "      <th>Sensor Measure 3</th>\n",
       "      <th>Sensor Measure 4</th>\n",
       "      <th>Sensor Measure 5</th>\n",
       "      <th>...</th>\n",
       "      <th>Sensor Measure 13</th>\n",
       "      <th>Sensor Measure 14</th>\n",
       "      <th>Sensor Measure 15</th>\n",
       "      <th>Sensor Measure 16</th>\n",
       "      <th>Sensor Measure 17</th>\n",
       "      <th>Sensor Measure 18</th>\n",
       "      <th>Sensor Measure 19</th>\n",
       "      <th>Sensor Measure 20</th>\n",
       "      <th>Sensor Measure 21</th>\n",
       "      <th>RUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   UnitNumber  Cycle  Operation Setting 1  Operation Setting 2  \\\n",
       "0           1      1              -0.0007              -0.0004   \n",
       "1           1      2               0.0019              -0.0003   \n",
       "2           1      3              -0.0043               0.0003   \n",
       "3           1      4               0.0007               0.0000   \n",
       "4           1      5              -0.0019              -0.0002   \n",
       "\n",
       "   Operation Setting 3  Sensor Measure 1  Sensor Measure 2  Sensor Measure 3  \\\n",
       "0                100.0            518.67            641.82           1589.70   \n",
       "1                100.0            518.67            642.15           1591.82   \n",
       "2                100.0            518.67            642.35           1587.99   \n",
       "3                100.0            518.67            642.35           1582.79   \n",
       "4                100.0            518.67            642.37           1582.85   \n",
       "\n",
       "   Sensor Measure 4  Sensor Measure 5  ...  Sensor Measure 13  \\\n",
       "0           1400.60             14.62  ...            2388.02   \n",
       "1           1403.14             14.62  ...            2388.07   \n",
       "2           1404.20             14.62  ...            2388.03   \n",
       "3           1401.87             14.62  ...            2388.08   \n",
       "4           1406.22             14.62  ...            2388.04   \n",
       "\n",
       "   Sensor Measure 14  Sensor Measure 15  Sensor Measure 16  Sensor Measure 17  \\\n",
       "0            8138.62             8.4195               0.03                392   \n",
       "1            8131.49             8.4318               0.03                392   \n",
       "2            8133.23             8.4178               0.03                390   \n",
       "3            8133.83             8.3682               0.03                392   \n",
       "4            8133.80             8.4294               0.03                393   \n",
       "\n",
       "   Sensor Measure 18  Sensor Measure 19  Sensor Measure 20  Sensor Measure 21  \\\n",
       "0               2388              100.0              39.06            23.4190   \n",
       "1               2388              100.0              39.00            23.4236   \n",
       "2               2388              100.0              38.95            23.3442   \n",
       "3               2388              100.0              38.88            23.3739   \n",
       "4               2388              100.0              38.90            23.4044   \n",
       "\n",
       "   RUL  \n",
       "0  192  \n",
       "1  191  \n",
       "2  190  \n",
       "3  189  \n",
       "4  188  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scale Data Min Max\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def scale_data(df):\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    float_columns = df.select_dtypes(include=float).columns.tolist()\n",
    "    scaled_data = scaler.fit_transform(df[float_columns])\n",
    "    df[float_columns] = scaled_data\n",
    "\n",
    "    return df\n",
    "\n",
    "## create sliding window\n",
    "def create_sliding_window(df, window_size = 30, drop_columns = [\"UnitNumber\", \"Cycle\", \"RUL\"]):\n",
    "    \n",
    "    number_engines = df[\"UnitNumber\"].unique()\n",
    "    X, y = [], []\n",
    "\n",
    "    for engine in number_engines:\n",
    "\n",
    "        ## get all data with engine same engine type\n",
    "        temp = df[df[\"UnitNumber\"] == engine]\n",
    "        assert temp[\"UnitNumber\"].unique() == engine\n",
    "\n",
    "        ## loop over group\n",
    "        for i in range(len(temp) - window_size + 1):\n",
    "\n",
    "            X_temp = temp.iloc[i : (i + window_size)].drop(columns = drop_columns)\n",
    "            Y_temp = temp.iloc[(i + window_size - 1)][\"RUL\"]\n",
    "            assert len(X_temp) == 30\n",
    "            X.append(X_temp.to_numpy())\n",
    "            y.append(Y_temp)\n",
    "            if i == (len(temp) - window_size):\n",
    "                assert Y_temp == 1\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "def scale_data(df):\n",
    "    \"\"\"\n",
    "    Scales the numerical columns in the DataFrame using MinMaxScaler.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Scaled DataFrame.\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # Select float columns\n",
    "    float_columns = df.select_dtypes(include=float).columns.tolist()\n",
    "    \n",
    "    # Scale the data\n",
    "    scaled_data = scaler.fit_transform(df[float_columns])\n",
    "    \n",
    "    # Update the DataFrame with scaled data\n",
    "    df[float_columns] = scaled_data\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_sliding_window(df, window_size=30, drop_columns=[\"UnitNumber\", \"Cycle\", \"RUL\"]):\n",
    "    \"\"\"\n",
    "    Creates a sliding window of data for time series prediction.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input DataFrame containing time series data.\n",
    "        window_size (int): Size of the sliding window.\n",
    "        drop_columns (list): List of columns to drop from the input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing X (input) and y (output) arrays.\n",
    "    \"\"\"\n",
    "    number_engines = df[\"UnitNumber\"].unique()\n",
    "    X, y = [], []\n",
    "\n",
    "    for engine in number_engines:\n",
    "        # Get data for the current engine\n",
    "        temp = df[df[\"UnitNumber\"] == engine]\n",
    "        assert temp[\"UnitNumber\"].unique() == engine\n",
    "\n",
    "        for i in range(len(temp) - window_size + 1):\n",
    "            # Extract windowed data and RUL for each window\n",
    "            X_temp = temp.iloc[i : (i + window_size)].drop(columns=drop_columns)\n",
    "            Y_temp = temp.iloc[(i + window_size - 1)][\"RUL\"]\n",
    "            assert len(X_temp) == window_size\n",
    "            X.append(X_temp.to_numpy())\n",
    "            y.append(Y_temp)\n",
    "            if i == (len(temp) - window_size):\n",
    "                assert Y_temp == 1\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = scale_data(train_data)\n",
    "X_train, y_train = create_sliding_window(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (seq_len, batch_size, feature_size)\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, feature_size: int, num_heads: int, num_layers: int, dropout: float = 0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.feature_size = feature_size\n",
    "        \n",
    "        # Positional Encoding\n",
    "        self.positional_encoding = PositionalEncoding(feature_size, dropout)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc_out = nn.Linear(feature_size, 1) \n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (seq_len, batch_size, feature_size)\n",
    "            mask: Optional mask of shape (seq_len, seq_len)\n",
    "            \n",
    "        Returns:\n",
    "            out: Tensor of shape (batch_size, 1) for regression\n",
    "        \"\"\"\n",
    "        # Add positional encoding\n",
    "        x = self.positional_encoding(x)# (seq_len, batch_size, feature_size)\n",
    "        x = x.to(torch.float32)\n",
    "\n",
    "        # Pass through transformer encoder\n",
    "        x = self.transformer_encoder(x, mask)  # (seq_len, batch_size, feature_size)\n",
    "    \n",
    "        # Take the mean across the sequence length dimension\n",
    "        x = torch.mean(x, dim=0)  # (batch_size, feature_size)\n",
    "        \n",
    "        # Output layer\n",
    "        out = self.fc_out(x)  # (batch_size, 1)\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frbroy/miniconda3/envs/py10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> 0/444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frbroy/miniconda3/envs/py10/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> 44/444\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "# Example dataset class\n",
    "class TurbofanDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = torch.from_numpy(data).to(torch.float32)\n",
    "        self.targets = torch.from_numpy(targets).to(torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "# Training function\n",
    "def train_model(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    count = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        if count % 44 == 0:\n",
    "            print(f\"--> {count}/{len(dataloader)}\")\n",
    "        count += 1\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "# Main function to execute training\n",
    "if __name__ == \"__main__\":\n",
    "    # Example data (replace with actual data loading)\n",
    "    seq_len, batch_size, feature_size = X_train.shape[1], 32, X_train.shape[2]\n",
    "    num_heads, num_layers = 4, 2\n",
    "    num_epochs = 20\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    # Create dataset and dataloaders\n",
    "    dataset = TurbofanDataset(X_train, y_train)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize model, criterion, optimizer\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = TransformerModel(feature_size, num_heads, num_layers).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_model(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss = evaluate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frbroy/miniconda3/envs/py10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "transformer_model = nn.Transformer(d_model = 128, nhead=2, num_encoder_layers=2)\n",
    "src = torch.rand((10, 32, 128))\n",
    "tgt = torch.rand((20, 32, 128))\n",
    "print(src.dtype)\n",
    "print(tgt.dtype)\n",
    "out = transformer_model(src, tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frbroy/miniconda3/envs/py10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.7688, -0.9155,  1.0146,  ...,  0.1923,  0.6935, -0.8276],\n",
       "         [ 0.2994, -0.7637,  0.1553,  ..., -1.0956, -0.3762,  0.9257],\n",
       "         [ 1.0649,  1.6741, -0.6682,  ...,  0.8168, -0.4372, -1.1820],\n",
       "         ...,\n",
       "         [-0.6294,  0.1968, -0.6976,  ...,  0.3974, -1.0983,  0.7739],\n",
       "         [ 0.8888, -0.2703,  1.7898,  ..., -2.0396,  0.6095,  0.5825],\n",
       "         [ 0.4155,  0.4953,  0.8681,  ...,  0.0821,  0.7833,  0.1519]],\n",
       "\n",
       "        [[-0.3351,  1.0243,  0.8632,  ..., -2.3093, -0.2934,  1.0051],\n",
       "         [ 1.3462,  1.1734, -1.0217,  ...,  0.1424, -1.8163, -0.4943],\n",
       "         [ 0.1971, -0.5985,  0.7071,  ..., -0.4826,  0.4047, -0.8395],\n",
       "         ...,\n",
       "         [-0.5127,  0.5455, -0.3165,  ..., -2.2351, -0.2323,  0.8548],\n",
       "         [ 0.9956, -0.7790,  0.5878,  ..., -2.2242,  0.8267,  0.1239],\n",
       "         [-0.8507, -0.0728,  0.8290,  ...,  0.4038,  0.8052,  0.5599]],\n",
       "\n",
       "        [[ 0.7925,  0.0844,  0.1540,  ..., -1.5850, -0.9625,  0.7286],\n",
       "         [ 1.5805, -0.2128,  0.7889,  ...,  0.5723, -1.1587, -0.1513],\n",
       "         [-0.2035,  0.0415,  0.9443,  ..., -0.7965, -0.8225, -0.3480],\n",
       "         ...,\n",
       "         [ 0.0657,  0.6826, -0.8302,  ..., -1.3193, -0.1135, -0.1383],\n",
       "         [ 1.5329,  0.8047,  1.7300,  ..., -0.7944,  0.0809,  0.1351],\n",
       "         [ 0.9340, -0.9556,  1.4603,  ..., -1.3703,  0.5676,  0.8697]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.0953, -0.9205,  1.2414,  ..., -0.7144, -1.0278,  1.1754],\n",
       "         [ 0.1232,  1.0119, -1.4813,  ..., -0.2703,  0.6881,  1.2752],\n",
       "         [-0.4555,  0.1325, -0.1574,  ..., -0.9540,  2.0769, -0.4390],\n",
       "         ...,\n",
       "         [-0.4626, -0.6152, -0.2690,  ...,  0.2545, -0.6190,  0.0152],\n",
       "         [-0.5349, -1.0542,  1.0276,  ..., -0.5399, -0.2981,  0.5625],\n",
       "         [ 0.6797, -1.3484,  0.6044,  ...,  0.1720, -2.0294, -1.0295]],\n",
       "\n",
       "        [[ 0.6516,  0.0671,  1.8727,  ..., -0.6973, -0.1733,  0.3699],\n",
       "         [ 0.6843, -0.6861,  0.4128,  ..., -1.5438, -1.6082,  1.1953],\n",
       "         [-0.2152,  0.6544,  1.6443,  ..., -0.4507,  1.0468,  0.3043],\n",
       "         ...,\n",
       "         [-1.0231, -1.3432,  1.3599,  ..., -1.8037, -0.0536, -0.3324],\n",
       "         [ 1.1736,  0.6817,  1.0925,  ..., -1.7057, -0.1076,  0.2404],\n",
       "         [ 0.1521, -0.3339,  0.2272,  ...,  1.5927,  0.2021, -0.1956]],\n",
       "\n",
       "        [[ 1.9365,  0.4727,  1.2931,  ..., -1.8081, -0.8631, -0.7627],\n",
       "         [ 0.6672, -1.0572,  1.0827,  ..., -1.5292, -0.9102,  0.4241],\n",
       "         [ 1.6446,  0.3044, -0.0521,  ...,  1.2232, -0.1083, -0.8134],\n",
       "         ...,\n",
       "         [-0.3294, -0.1746,  1.3355,  ..., -0.0283, -0.9607, -0.0684],\n",
       "         [ 1.1637,  1.4266,  1.7508,  ..., -1.8261, -0.1401, -0.0332],\n",
       "         [ 0.2468,  1.1619, -0.3955,  ..., -0.7287,  0.6564,  1.1712]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=num_heads, dropout=dropout)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "src = torch.rand((30, 32, 24))\n",
    "print(src.dtype)\n",
    "transformer_encoder(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:39.220343800Z",
     "start_time": "2024-05-09T15:31:39.160435700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# [TEMPLATE] - save processed data (as pickle)\n",
    "df = pd.DataFrame()\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "df.to_pickle(f\"{config['paths']['processed_data_dir']}ex2_topic_{timestamp}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:39.318667400Z",
     "start_time": "2024-05-09T15:31:39.220343800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# [TEMPLATE] - save data predictions (as csv)\n",
    "df = pd.DataFrame()\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "df.to_csv(f\"{config['paths']['prediction_dir']}ex2_topic_{timestamp}.csv\", sep=',', decimal='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:39.485116500Z",
     "start_time": "2024-05-09T15:31:39.320350800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 900x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [TEMPLATE] - save plot results (as png)\n",
    "fig = plt.figure(figsize=(9, 6))\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "fig.savefig(f\"{config['paths']['plot_dir']}ex2_topic_{timestamp}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:39.522027500Z",
     "start_time": "2024-05-09T15:31:39.474058600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
