{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-09T15:31:37.900316500Z",
          "start_time": "2024-05-09T15:31:37.830560800Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5F5XXw_dG30W",
        "outputId": "fa5c361f-7ce1-4c60-9ec0-12b984af133f"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-09T15:31:37.980322700Z",
          "start_time": "2024-05-09T15:31:37.900316500Z"
        },
        "id": "c3wYfrYSG30Y"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "DthkG3zAG30Y"
      },
      "source": [
        "# Topic: EX2 - Turbofan RUL Prediction\n",
        "**Task**: Predict the remaining useful life (RUL) of turbofan engines based on given sensor data (time series data). It is a forcasting problem, where the goal is to predict the number of cycles an engine will last before it fails.\n",
        "**Data**: Turbofan engine degradation simulation data (NASA) - [Link](https://data.nasa.gov/dataset/Turbofan-Engine-Degradation-Simulation-Data-Set/vrks-gjie). See also in the topic [introduction notebook](https://github.com/nina-prog/damage-propagation-modeling/blob/2fb8c1a1102a48d7abbf04e4031807790a913a99/notebooks/Turbofan%20remaining%20useful%20life%20Prediction.ipynb).\n",
        "\n",
        "**Subtasks**:\n",
        "1. Perform a deep **exploratory data analysis (EDA)** on the given data.\n",
        "2. Implement a more efficient **sliding window method** for time series data analysis. -> 🎯 **Focus on this task**\n",
        "3. Apply **traditional machine learning methods** (SOTA) to predict the remaining useful life. Includes data preparation, feature extraction, feature selection, model selection, and model parameter optimization.\n",
        "4. Create **neural network models** to predict the remaining useful life. Includes different architectures like Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), or Attention Models. Note: You can search for SOTA research papers and reproduce current state-of-the-art models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "QtIxFnhBG30Z"
      },
      "source": [
        "# Imports + Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-09T15:31:38.193478100Z",
          "start_time": "2024-05-09T15:31:37.980322700Z"
        },
        "id": "Dq9s8mxyG30Z"
      },
      "outputs": [],
      "source": [
        "#imports aller Classifier\n",
        "\n",
        "# third-party libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "#sklearn models\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, AdaBoostRegressor, RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.linear_model import ridge_regression, LogisticRegression, Lasso, LinearRegression\n",
        "# sklearn tools\n",
        "from sklearn.metrics import mean_squared_error, root_mean_squared_error\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "#tsfresh\n",
        "from tsfresh.feature_extraction import feature_calculators, MinimalFCParameters, EfficientFCParameters\n",
        "#xgboost\n",
        "from xgboost import XGBRegressor\n",
        "# Bayesion Optimizer\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-09T15:31:38.304026600Z",
          "start_time": "2024-05-09T15:31:38.200491900Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "DLvHCR4AG30a",
        "outputId": "ede887e4-4573-49a1-d9b8-28b378347776"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Make sure to execute this cell only once for one kernel session, before running any other cell below.\n",
        "os.chdir(\"/Users/niklasquendt/Documents/Uni/PSDA/Übung 2/damage-propagation-modeling\") # set working directory to root of project\n",
        "os.getcwd() # check current working directory\n",
        "# source code\n",
        "from src.data_loading import load_data, load_config\n",
        "from src.data_splitting import train_val_split_by_group\n",
        "from src.data_cleaning import clean_data, format_dtype\n",
        "from src.rolling_window_creator import calculate_RUL, RollingWindowDatasetCreator\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-09T15:31:38.400456500Z",
          "start_time": "2024-05-09T15:31:38.304026600Z"
        },
        "id": "DwZsljLeG30a"
      },
      "outputs": [],
      "source": [
        "# settings\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_palette(\"Set2\")\n",
        "sns.set(rc={\"figure.dpi\":100, 'savefig.dpi':200})\n",
        "sns.set_context('notebook')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-09T15:31:38.492590100Z",
          "start_time": "2024-05-09T15:31:38.400456500Z"
        },
        "id": "rMUHqB3XG30b"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "_yJmC8GlG30b"
      },
      "source": [
        "# Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "Mz1y0iQ1G30b",
        "outputId": "c2b8c271-67d1-460a-8209-09dce73c8103"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/niklasquendt/Documents/Uni/PSDA/Übung 2/damage-propagation-modeling'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Make sure to execute this cell only once for one kernel session, before running any other cell below.\n",
        "os.chdir(\"/Users/niklasquendt/Documents/Uni/PSDA/Übung 2/damage-propagation-modeling\") # set working directory to root of project\n",
        "os.getcwd() # check current working directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-09T15:31:38.652221700Z",
          "start_time": "2024-05-09T15:31:38.560578800Z"
        },
        "id": "mgdYuLBxG30c"
      },
      "outputs": [],
      "source": [
        "PATH_TO_CONFIG = \"configs/config.yaml\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "FM9WDvx3G30c"
      },
      "source": [
        "# Load Config + Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-09T15:31:38.732034900Z",
          "start_time": "2024-05-09T15:31:38.652221700Z"
        },
        "id": "n_wsaNOuG30c"
      },
      "outputs": [],
      "source": [
        "config = load_config(PATH_TO_CONFIG) # config is dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-09T15:31:39.154853100Z",
          "start_time": "2024-05-09T15:31:38.740553100Z"
        },
        "id": "dYsb-EXNG30d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 17:52:30 [\u001b[34msrc.data_loading:43\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loading data set 1...\u001b[0m\n",
            "2024-06-01 17:52:30 [\u001b[34msrc.data_loading:72\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loaded raw data for dataset 1.\u001b[0m\n",
            "2024-06-01 17:52:30 [\u001b[34msrc.data_loading:73\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Train Data: (20631, 26)\u001b[0m\n",
            "2024-06-01 17:52:30 [\u001b[34msrc.data_loading:74\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test Data: (13096, 26)\u001b[0m\n",
            "2024-06-01 17:52:30 [\u001b[34msrc.data_loading:75\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test RUL Data: (100, 1)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#Load Dataset 1\n",
        "train_data_1, test_data_1, test_RUL_data_1 = load_data(config_path=PATH_TO_CONFIG, dataset_num=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fJGsJAdCIYIw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 17:52:31 [\u001b[34msrc.data_loading:43\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loading data set 2...\u001b[0m\n",
            "2024-06-01 17:52:32 [\u001b[34msrc.data_loading:72\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loaded raw data for dataset 2.\u001b[0m\n",
            "2024-06-01 17:52:32 [\u001b[34msrc.data_loading:73\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Train Data: (53759, 26)\u001b[0m\n",
            "2024-06-01 17:52:32 [\u001b[34msrc.data_loading:74\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test Data: (33991, 26)\u001b[0m\n",
            "2024-06-01 17:52:32 [\u001b[34msrc.data_loading:75\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test RUL Data: (259, 1)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#Load Dataset 2\n",
        "train_data_2, test_data_2, test_RUL_data_2 = load_data(config_path=PATH_TO_CONFIG, dataset_num=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ThMLwpDXIYn2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 17:52:34 [\u001b[34msrc.data_loading:43\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loading data set 3...\u001b[0m\n",
            "2024-06-01 17:52:34 [\u001b[34msrc.data_loading:72\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loaded raw data for dataset 3.\u001b[0m\n",
            "2024-06-01 17:52:34 [\u001b[34msrc.data_loading:73\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Train Data: (24720, 26)\u001b[0m\n",
            "2024-06-01 17:52:34 [\u001b[34msrc.data_loading:74\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test Data: (16596, 26)\u001b[0m\n",
            "2024-06-01 17:52:34 [\u001b[34msrc.data_loading:75\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test RUL Data: (100, 1)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#Load Dataset 3\n",
        "train_data_3, test_data_3, test_RUL_data_3 = load_data(config_path=PATH_TO_CONFIG, dataset_num=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xItee_5VIY6m"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 17:52:35 [\u001b[34msrc.data_loading:43\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loading data set 4...\u001b[0m\n",
            "2024-06-01 17:52:35 [\u001b[34msrc.data_loading:72\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loaded raw data for dataset 4.\u001b[0m\n",
            "2024-06-01 17:52:35 [\u001b[34msrc.data_loading:73\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Train Data: (61249, 26)\u001b[0m\n",
            "2024-06-01 17:52:35 [\u001b[34msrc.data_loading:74\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test Data: (41214, 26)\u001b[0m\n",
            "2024-06-01 17:52:35 [\u001b[34msrc.data_loading:75\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test RUL Data: (248, 1)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#Load Dataset 4\n",
        "train_data_4, test_data_4, test_RUL_data_4 = load_data(config_path=PATH_TO_CONFIG, dataset_num=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "4VTJOiDVG30d"
      },
      "source": [
        "# 📍 << Subtask 3: Traditional ML >>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0IylsBbJ5TL"
      },
      "source": [
        "# Best Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFxeTQYvJ74I"
      },
      "outputs": [],
      "source": [
        "# Dataset 1\n",
        "# On Dataset 1 our best Training model was the ExtraTrees Regressor\n",
        "# the parameter are the following:\n",
        "# n_estimators = 140\n",
        "# max_depth = 15\n",
        "# min_sample_leaf = 5\n",
        "# random_state = 42\n",
        "\n",
        "# please find the complete pipeline and how we achieved this score below\n",
        "# RMSE on the Train-data: 15.1911\n",
        "# models performs exceptional good on the test-data (RMSE:7.9324)\n",
        "# we assume that there might be a small inconsistency in the pipeline, that causes the RMSE, that is way better than expected\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMWZuNAEKelF"
      },
      "outputs": [],
      "source": [
        "# Dataset 2\n",
        "# On Dataset 2 our best Training model was the ExtraTrees Regressor\n",
        "# the parameter are the following:\n",
        "# n_estimators = 140\n",
        "# max_depth = 15\n",
        "# min_sample_leaf = 5\n",
        "# random_state = 42\n",
        "\n",
        "# please find the complete pipeline and how we achieved this score below\n",
        "# RMSE on the Train-data: 23.2641\n",
        "# models performs exceptional good on the test-data (RMSE:8.7472)\n",
        "# we assume that there might be a small inconsistency in the pipeline, that causes the RMSE, that is way better than expected\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nb3XPLlbKfC0"
      },
      "outputs": [],
      "source": [
        "# Dataset 3\n",
        "# On the dataset 3 was XGBoostRegressor our best performing model.\n",
        "# eta= 0.2079\n",
        "# gamma= 1.342\n",
        "# max_depth= 2\n",
        "# It achieved an RMSE of 21.48 in this configuration\n",
        "\n",
        "# The complete pipeline is listed down below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DUqNa41KfYO"
      },
      "outputs": [],
      "source": [
        "# Dataset 4\n",
        "# For dataset 4 the RandomForestRegressor performed the best.\n",
        "# n_estimators = 264\n",
        "# max_features = 1\n",
        "# random_state = 17\n",
        "# In this case it achieved a RMSE of 39.53. When comparing the 4 datasets is by far the worst.\n",
        "\n",
        "# Again the whole pipeline and process is detailed below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTIrxsquK1H7"
      },
      "source": [
        "# Procedure\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoN6TVu9J3IX"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-05-09T15:31:39.522027500Z",
          "start_time": "2024-05-09T15:31:39.474058600Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "A4sMLpUfG30e",
        "outputId": "bdead425-e33b-46a0-ff55-0f2b4d2d4f9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 17:52:43 [\u001b[34msrc.data_cleaning:134\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Cleaning train and test data...\u001b[0m\n",
            "2024-06-01 17:52:43 [\u001b[34msrc.data_cleaning:136\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Formatting column types...\u001b[0m\n",
            "2024-06-01 17:52:43 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
            "2024-06-01 17:52:43 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
            "2024-06-01 17:52:43 [\u001b[34msrc.data_cleaning:141\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Handling duplicates...\u001b[0m\n",
            "2024-06-01 17:52:43 [\u001b[34msrc.data_cleaning:146\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Removing outliers...\u001b[0m\n",
            "2024-06-01 17:52:43 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
            "2024-06-01 17:52:43 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
            "2024-06-01 17:52:43 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
            "2024-06-01 17:52:43 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
            "2024-06-01 17:52:43 [\u001b[34msrc.data_cleaning:150\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Filter features based train data...\u001b[0m\n",
            "2024-06-01 17:52:43 [\u001b[34msrc.data_cleaning:26\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 features with missing values above the threshold of 0.1.\u001b[0m\n",
            "2024-06-01 17:52:43 [\u001b[34msrc.data_cleaning:46\u001b[0m] [DEBUG\u001b[0m] >>>> Found 7 features with only a single unique value: ['Operation Setting 3', 'Sensor Measure 1', 'Sensor Measure 5', 'Sensor Measure 10', 'Sensor Measure 16', 'Sensor Measure 18', 'Sensor Measure 19']\u001b[0m\n",
            "2024-06-01 17:52:43 [\u001b[34msrc.data_cleaning:103\u001b[0m] [DEBUG\u001b[0m] >>>> Found 4 uncorrelated features with a correlation threshold of 0.3: ['UnitNumber', 'Operation Setting 1', 'Operation Setting 2', 'Sensor Measure 6']\u001b[0m\n",
            "2024-06-01 17:52:43 [\u001b[34msrc.data_cleaning:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Dropping features based on missing values, single unique values, and no target correlation...\u001b[0m\n",
            "2024-06-01 17:52:43 [\u001b[34msrc.data_cleaning:172\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Data cleaning completed.\u001b[0m\n",
            "2024-06-01 17:52:43 [\u001b[34msrc.data_cleaning:173\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original train DataFrame shape: (20631, 16), Resulting train DataFrame shape: (20631, 16)\u001b[0m\n",
            "2024-06-01 17:52:43 [\u001b[34msrc.data_cleaning:174\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original test DataFrame shape: (13096, 16), Resulting test DataFrame shape: (13096, 16)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Default Data cleaning\n",
        "cleaned_train_1, cleaned_test_1 = clean_data(train_data_1, test_data_1, method=None, ignore_columns=['UnitNumber', 'Cycle'], threshold_missing=0.1, threshold_corr=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "KX9QRSvzLZoR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 17:52:45 [\u001b[34msrc.data_cleaning:134\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Cleaning train and test data...\u001b[0m\n",
            "2024-06-01 17:52:45 [\u001b[34msrc.data_cleaning:136\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Formatting column types...\u001b[0m\n",
            "2024-06-01 17:52:45 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
            "2024-06-01 17:52:45 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
            "2024-06-01 17:52:45 [\u001b[34msrc.data_cleaning:141\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Handling duplicates...\u001b[0m\n",
            "2024-06-01 17:52:45 [\u001b[34msrc.data_cleaning:146\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Removing outliers...\u001b[0m\n",
            "2024-06-01 17:52:45 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
            "2024-06-01 17:52:45 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
            "2024-06-01 17:52:45 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
            "2024-06-01 17:52:45 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
            "2024-06-01 17:52:45 [\u001b[34msrc.data_cleaning:150\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Filter features based train data...\u001b[0m\n",
            "2024-06-01 17:52:45 [\u001b[34msrc.data_cleaning:26\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 features with missing values above the threshold of 0.1.\u001b[0m\n",
            "2024-06-01 17:52:45 [\u001b[34msrc.data_cleaning:46\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 features with only a single unique value: []\u001b[0m\n",
            "2024-06-01 17:52:45 [\u001b[34msrc.data_cleaning:103\u001b[0m] [DEBUG\u001b[0m] >>>> Found 25 uncorrelated features with a correlation threshold of 0.3: ['UnitNumber', 'Operation Setting 1', 'Operation Setting 2', 'Operation Setting 3', 'Sensor Measure 1', 'Sensor Measure 2', 'Sensor Measure 3', 'Sensor Measure 4', 'Sensor Measure 5', 'Sensor Measure 6', 'Sensor Measure 7', 'Sensor Measure 8', 'Sensor Measure 9', 'Sensor Measure 10', 'Sensor Measure 11', 'Sensor Measure 12', 'Sensor Measure 13', 'Sensor Measure 14', 'Sensor Measure 15', 'Sensor Measure 16', 'Sensor Measure 17', 'Sensor Measure 18', 'Sensor Measure 19', 'Sensor Measure 20', 'Sensor Measure 21']\u001b[0m\n",
            "2024-06-01 17:52:45 [\u001b[34msrc.data_cleaning:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Dropping features based on missing values, single unique values, and no target correlation...\u001b[0m\n",
            "2024-06-01 17:52:45 [\u001b[34msrc.data_cleaning:172\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Data cleaning completed.\u001b[0m\n",
            "2024-06-01 17:52:45 [\u001b[34msrc.data_cleaning:173\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original train DataFrame shape: (53759, 2), Resulting train DataFrame shape: (53759, 2)\u001b[0m\n",
            "2024-06-01 17:52:45 [\u001b[34msrc.data_cleaning:174\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original test DataFrame shape: (33991, 2), Resulting test DataFrame shape: (33991, 2)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "cleaned_train_2, cleaned_test_2 = clean_data(train_data_2, test_data_2, method=None, ignore_columns=['UnitNumber', 'Cycle'], threshold_missing=0.1, threshold_corr=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "dPX0td5_LdOs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 17:52:50 [\u001b[34msrc.data_cleaning:134\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Cleaning train and test data...\u001b[0m\n",
            "2024-06-01 17:52:50 [\u001b[34msrc.data_cleaning:136\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Formatting column types...\u001b[0m\n",
            "2024-06-01 17:52:50 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
            "2024-06-01 17:52:50 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
            "2024-06-01 17:52:50 [\u001b[34msrc.data_cleaning:141\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Handling duplicates...\u001b[0m\n",
            "2024-06-01 17:52:50 [\u001b[34msrc.data_cleaning:146\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Removing outliers...\u001b[0m\n",
            "2024-06-01 17:52:50 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
            "2024-06-01 17:52:50 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
            "2024-06-01 17:52:50 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
            "2024-06-01 17:52:50 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
            "2024-06-01 17:52:50 [\u001b[34msrc.data_cleaning:150\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Filter features based train data...\u001b[0m\n",
            "2024-06-01 17:52:50 [\u001b[34msrc.data_cleaning:26\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 features with missing values above the threshold of 0.1.\u001b[0m\n",
            "2024-06-01 17:52:50 [\u001b[34msrc.data_cleaning:46\u001b[0m] [DEBUG\u001b[0m] >>>> Found 6 features with only a single unique value: ['Operation Setting 3', 'Sensor Measure 1', 'Sensor Measure 5', 'Sensor Measure 16', 'Sensor Measure 18', 'Sensor Measure 19']\u001b[0m\n",
            "2024-06-01 17:52:50 [\u001b[34msrc.data_cleaning:103\u001b[0m] [DEBUG\u001b[0m] >>>> Found 7 uncorrelated features with a correlation threshold of 0.3: ['UnitNumber', 'Operation Setting 1', 'Operation Setting 2', 'Sensor Measure 6', 'Sensor Measure 15', 'Sensor Measure 20', 'Sensor Measure 21']\u001b[0m\n",
            "2024-06-01 17:52:50 [\u001b[34msrc.data_cleaning:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Dropping features based on missing values, single unique values, and no target correlation...\u001b[0m\n",
            "2024-06-01 17:52:50 [\u001b[34msrc.data_cleaning:172\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Data cleaning completed.\u001b[0m\n",
            "2024-06-01 17:52:50 [\u001b[34msrc.data_cleaning:173\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original train DataFrame shape: (24720, 14), Resulting train DataFrame shape: (24720, 14)\u001b[0m\n",
            "2024-06-01 17:52:50 [\u001b[34msrc.data_cleaning:174\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original test DataFrame shape: (16596, 14), Resulting test DataFrame shape: (16596, 14)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "cleaned_train_3, cleaned_test_3 = clean_data(train_data_3, test_data_3, method=None, ignore_columns=['UnitNumber', 'Cycle'], threshold_missing=0.1, threshold_corr=0.3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_QcyyJE4LicB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 17:52:53 [\u001b[34msrc.data_cleaning:134\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Cleaning train and test data...\u001b[0m\n",
            "2024-06-01 17:52:53 [\u001b[34msrc.data_cleaning:136\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Formatting column types...\u001b[0m\n",
            "2024-06-01 17:52:53 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
            "2024-06-01 17:52:53 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
            "2024-06-01 17:52:53 [\u001b[34msrc.data_cleaning:141\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Handling duplicates...\u001b[0m\n",
            "2024-06-01 17:52:53 [\u001b[34msrc.data_cleaning:146\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Removing outliers...\u001b[0m\n",
            "2024-06-01 17:52:53 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
            "2024-06-01 17:52:53 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
            "2024-06-01 17:52:53 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
            "2024-06-01 17:52:53 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
            "2024-06-01 17:52:53 [\u001b[34msrc.data_cleaning:150\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Filter features based train data...\u001b[0m\n",
            "2024-06-01 17:52:53 [\u001b[34msrc.data_cleaning:26\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 features with missing values above the threshold of 0.1.\u001b[0m\n",
            "2024-06-01 17:52:53 [\u001b[34msrc.data_cleaning:46\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 features with only a single unique value: []\u001b[0m\n",
            "2024-06-01 17:52:53 [\u001b[34msrc.data_cleaning:103\u001b[0m] [DEBUG\u001b[0m] >>>> Found 25 uncorrelated features with a correlation threshold of 0.3: ['UnitNumber', 'Operation Setting 1', 'Operation Setting 2', 'Operation Setting 3', 'Sensor Measure 1', 'Sensor Measure 2', 'Sensor Measure 3', 'Sensor Measure 4', 'Sensor Measure 5', 'Sensor Measure 6', 'Sensor Measure 7', 'Sensor Measure 8', 'Sensor Measure 9', 'Sensor Measure 10', 'Sensor Measure 11', 'Sensor Measure 12', 'Sensor Measure 13', 'Sensor Measure 14', 'Sensor Measure 15', 'Sensor Measure 16', 'Sensor Measure 17', 'Sensor Measure 18', 'Sensor Measure 19', 'Sensor Measure 20', 'Sensor Measure 21']\u001b[0m\n",
            "2024-06-01 17:52:53 [\u001b[34msrc.data_cleaning:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Dropping features based on missing values, single unique values, and no target correlation...\u001b[0m\n",
            "2024-06-01 17:52:53 [\u001b[34msrc.data_cleaning:172\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Data cleaning completed.\u001b[0m\n",
            "2024-06-01 17:52:53 [\u001b[34msrc.data_cleaning:173\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original train DataFrame shape: (61249, 2), Resulting train DataFrame shape: (61249, 2)\u001b[0m\n",
            "2024-06-01 17:52:53 [\u001b[34msrc.data_cleaning:174\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original test DataFrame shape: (41214, 2), Resulting test DataFrame shape: (41214, 2)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "cleaned_train_4, cleaned_test_4 = clean_data(train_data_4, test_data_4, method=None, ignore_columns=['UnitNumber', 'Cycle'], threshold_missing=0.1, threshold_corr=0.3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrjaiXcaMvW8"
      },
      "source": [
        "Reasons for default data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "OEMGuCD_MMJD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 17:52:55 [\u001b[34msrc.data_cleaning:134\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Cleaning train and test data...\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.data_cleaning:136\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Formatting column types...\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.data_cleaning:141\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Handling duplicates...\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.data_cleaning:146\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Removing outliers...\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.data_cleaning:150\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Filter features based train data...\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.data_cleaning:26\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 features with missing values above the threshold of 0.1.\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.data_cleaning:46\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 features with only a single unique value: []\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.data_cleaning:103\u001b[0m] [DEBUG\u001b[0m] >>>> Found 3 uncorrelated features with a correlation threshold of 0.5: ['UnitNumber', 'Sensor Measure 9', 'Sensor Measure 14']\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.data_cleaning:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Dropping features based on missing values, single unique values, and no target correlation...\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.data_cleaning:172\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Data cleaning completed.\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.data_cleaning:173\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original train DataFrame shape: (20631, 14), Resulting train DataFrame shape: (20631, 14)\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.data_cleaning:174\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original test DataFrame shape: (13096, 14), Resulting test DataFrame shape: (13096, 14)\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.data_cleaning:134\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Cleaning train and test data...\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.data_cleaning:136\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Formatting column types...\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.data_cleaning:141\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Handling duplicates...\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.data_cleaning:146\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Removing outliers...\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: None ...\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.outlier_detection:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> No outlier detection method specified. Skipping outlier detection.\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.data_cleaning:150\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Filter features based train data...\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.data_cleaning:26\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 features with missing values above the threshold of 0.1.\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.data_cleaning:46\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 features with only a single unique value: []\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.data_cleaning:103\u001b[0m] [DEBUG\u001b[0m] >>>> Found 6 uncorrelated features with a correlation threshold of 0.5: ['UnitNumber', 'Cycle', 'Sensor Measure 7', 'Sensor Measure 10', 'Sensor Measure 12', 'Sensor Measure 14']\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.data_cleaning:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Dropping features based on missing values, single unique values, and no target correlation...\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.data_cleaning:172\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Data cleaning completed.\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.data_cleaning:173\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original train DataFrame shape: (24720, 10), Resulting train DataFrame shape: (24720, 10)\u001b[0m\n",
            "2024-06-01 17:52:55 [\u001b[34msrc.data_cleaning:174\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original test DataFrame shape: (16596, 10), Resulting test DataFrame shape: (16596, 10)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Variations C\n",
        "cleaned_train_1_varC, cleaned_test_1_varC = clean_data(train_data_1, test_data_1, method=None, ignore_columns=['UnitNumber', 'Cycle'], threshold_missing=0.1, threshold_corr=0.5)\n",
        "\n",
        "cleaned_train_3_varC, cleaned_test_1_varC = clean_data(train_data_3, test_data_3, method=None, ignore_columns=['UnitNumber', 'Cycle'], threshold_missing=0.1, threshold_corr=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnoV1PaxSeih"
      },
      "source": [
        "Notes: Data cleaning Variation C for dataset 1 is used instead of the default configuration to reduce clutter by not helpful sensor data. This also improves computation time in all following steps which is very important."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMN9DyQPLT4o"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXTSv4GRLWAy"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "y7pUk0YDMfUv"
      },
      "outputs": [],
      "source": [
        "# Feature Selection -- 1 -> 4\n",
        "feature_list_ds_1 = [\"c3\", \"quantile\", \"mean\", \"root_mean_square\", \"median\", \"time_reversal_asymmetry_statistic\", \"absolute_maximum\", \"maximum\", \"minimum\", \"agg_autocorrelation\", \"autocorrelation\" ]\n",
        "feature_list_ds_2 = [\"c3\", \"quantile\", \"mean\", \"median\", \"root_mean_square\", \"variance\", \"mean_abs_change\", \"standard_deviation\", \"skewness\", \"variation_coefficient\", \"last_location_of_maximum\", \"first_location_of_maximum\"]\n",
        "feature_list_ds_3 = [\"c3\", \"quantile\", \"mean\", \"root_mean_square\", \"median\", \"time_reversal_asymmetry_statistic\", \"absolute_maximum\", \"maximum\", \"minimum\", \"agg_autocorrelation\", \"autocorrelation\" ]\n",
        "feature_list_ds_4 = [\"c3\", \"quantile\", \"mean\", \"median\", \"root_mean_square\", \"variance\", \"mean_abs_change\", \"standard_deviation\", \"skewness\", \"variation_coefficient\", \"last_location_of_maximum\", \"first_location_of_maximum\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gB1RN15cTf91"
      },
      "source": [
        "The feature list we generated by evaluing most of the tsfresh FCparameters by themselves and choosing the top performing ones.\n",
        "Since the datasets 1 & 3 and 2 & 4 are similar they share the same feature list. Further explainations for this are below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_6wM_PXUJMf"
      },
      "outputs": [],
      "source": [
        "#min_timeshift, max_timeshift = 17,18\n",
        "#for ds_train, ds_test, ds_rul in [cleaned_train_1,cleaned_train_2,cleaned_train_3,cleaned_train_4],[cleaned_test_1,cleaned_test_2,cleaned_test_3,cleaned_test_4],[test_RUL_data_1,test_RUL_data_2,test_RUL_data_3,test_RUL_data_4]:\n",
        "#  for feat in EfficientFCParameters():\n",
        "#    # RollingWindow\n",
        "#    rwCreator = RollingWindowDatasetCreator(max_timeshift=max_timeshift,min_timeshift=min_timeshift,feature_extraction_mode= 'custom',feature_list=[feat])\n",
        "#    X_train, y_train, X_test, y_test = rwCreator.create_rolling_windows_datasets(train_data=ds_train, test_data=ds_test,test_RUL_data=ds_rul,)\n",
        "#    # KNeighborsRegressor\n",
        "#    knr = KNeighborsRegressor(3)\n",
        "#    knr.fit(X_train, y_train.values.ravel())\n",
        "#    rgr1 = np.sqrt(mean_squared_error(y_test, knr.predict(X_test)))\n",
        "#    # RandomForestRegressor\n",
        "#    rfr  = RandomForestRegressor(max_depth=5, n_estimators=10, max_features=1, random_state=42)\n",
        "#    rfr.fit(X_train, y_train.values.ravel())\n",
        "#    rgr2 = np.sqrt(mean_squared_error(y_test, rfr.predict(X_test)))\n",
        "#    # Lasso\n",
        "#    lr = Lasso()\n",
        "#    lr.fit(X_train, y_train.values.ravel())\n",
        "#    rgr3 = np.sqrt(mean_squared_error(y_test, lr.predict(X_test)))\n",
        "#    # XGBRegressor\n",
        "#    xgbr = XGBRegressor(n_estimators=3, max_depth=1, learning_rate=0.211) # objective='binary:logistic'\n",
        "#    xgbr.fit(X_train, y_train.values.ravel())\n",
        "#    rgr4 = np.sqrt(mean_squared_error(y_test, xgbr.predict(X_test)))\n",
        "#    # Prev Data import\n",
        "#    df_in = pd.read_pickle(\"drive/MyDrive/PSDA_cml/data/processed/ds4_tsf-feat_eff_results.pkl\")\n",
        "#    df = pd.DataFrame(data={'Feature': feat, 'Regressor Results': [f\"KNR: {rgr1}\",f\"RFR: {rgr2}\", f\"Lasso: {rgr3}\", f\"XBGr: {rgr4}\"]})\n",
        "#    df_out = pd.concat([df_in, df])\n",
        "#    df_out.to_pickle(f\"drive/MyDrive/PSDA_cml/data/processed/ds_tsf-feat_eff_results.pkl\")\n",
        "#    print({'Feature': feat, 'Regressor Results': [f\"KNR: {rgr1}\",f\"RFR: {rgr2}\", f\"Lasso: {rgr3}\", f\"XBGr: {rgr4}\"]})\n",
        "#  df = pd.read_pickle(\"drive/MyDrive/PSDA_cml/data/processed/ds1_tsf-feat_eff_results.pkl\")\n",
        "#  dict_ds = dict()\n",
        "#  for i in range(0,df.shape[0],4):\n",
        "#    mean = (float(df.values[i, 1].partition(\":\")[2]) + float(df.values[i+1, 1].partition(\":\")[2]) + float(df.values[i+2, 1].partition(\":\")[2]) + float(df.values[i+3, 1].partition(\":\")[2])) /4\n",
        "#    dict_ds[df.values[i,0]] = mean\n",
        "#  df_ds = pd.DataFrame.from_dict(data=dict_ds,orient='index',columns=['mean'])\n",
        "#  print(f\"Dataset {ds_train}: \")\n",
        "#  print(df_ds.sort_values(by='mean'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH9JRI8tW7aQ"
      },
      "source": [
        "This code loops over all datasets and all features of tsfreshs EfficientFCParameters. \\\\\n",
        "This took multiple hours and never fully finished. The longest run computed 56 features while the other computed far less. Thus we based the feature_list on the longest succesfully running tries for dataset 1 & 3 and the dataset 2 & 4.\n",
        "The two datasets were combined due to their similarity. This approach can't be advised to repeat since it each try, even unsuccessfull took ones multiple hours. The shortest was somewhat over 2 hours before the runtime reached its limit. Sadly because tsfresh was uncompatible with jupyterhub there was no real alternative. [Computation for this was done in Colab: https://colab.research.google.com/drive/1F_hpmXcxYoJT3LsvXjF65c3_lZ7ltEr6?usp=sharing ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu5JfkMsNEu4"
      },
      "source": [
        "# Windowing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "QHW-7VlKSUhU"
      },
      "outputs": [],
      "source": [
        "# Var\n",
        "min_ts = 5\n",
        "max_ts = 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "fl1gJ9fESWC8"
      },
      "outputs": [],
      "source": [
        "# Var\n",
        "min_ts_ds1_varc = 29\n",
        "max_ts_ds1_varc = 30\n",
        "\n",
        "min_ts_ds2_varc = 17\n",
        "max_ts_ds2_varc = 18\n",
        "\n",
        "min_ts_ds3_varc = 29\n",
        "max_ts_ds3_varc = 30\n",
        "\n",
        "min_ts_ds4_varc = 17\n",
        "max_ts_ds4_varc = 18"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udNp6gPwZXoQ"
      },
      "source": [
        "The best results were achieved with a window size of around 30. Because of that variation C uses a max timeshift of 30. Similar to some papers.\n",
        "Sadly datasets 2 & 4 have entries which have fewer than 20 steps. Thus for them the timeshift is reduced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnyyyMdi37qH"
      },
      "outputs": [],
      "source": [
        "#Erstellen der Datensätze mittels unserer Rolling Window Methode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "hES8TXiOYQa2"
      },
      "outputs": [],
      "source": [
        "rwCreator_varc = RollingWindowDatasetCreator(max_timeshift=max_ts_ds1_varc,min_timeshift=min_ts_ds1_varc,feature_extraction_mode= 'custom',feature_list=feature_list_ds_1)\n",
        "\n",
        "rwCreator = RollingWindowDatasetCreator(max_timeshift=max_ts,min_timeshift=min_ts,feature_extraction_mode= 'minimal')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM5jyxjbPbCL"
      },
      "source": [
        "RollingWindowCreator Variation C with the custom extraction mode is helpful for datasets 1 & 3. The results for the custom feature list for 2 & 4 were heavily dependent on the used regressor model. For them minimal seemed usually sufficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "4scobRfOYxnL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 17:59:05 [\u001b[34msrc.rolling_window_creator:134\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Creating rolling windows for train data...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rolling: 100%|██████████| 20/20 [00:03<00:00,  6.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 17:59:09 [\u001b[34msrc.rolling_window_creator:140\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Extracting features for train data...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Feature Extraction: 100%|██████████| 20/20 [00:53<00:00,  2.70s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 18:00:09 [\u001b[34msrc.rolling_window_creator:148\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Calculating target for train data...\u001b[0m\n",
            "2024-06-01 18:00:09 [\u001b[34msrc.rolling_window_creator:134\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Creating rolling windows for test data...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rolling: 100%|██████████| 20/20 [00:02<00:00,  7.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 18:00:12 [\u001b[34msrc.rolling_window_creator:140\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Extracting features for test data...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Feature Extraction: 100%|██████████| 20/20 [00:01<00:00, 13.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 18:00:14 [\u001b[34msrc.rolling_window_creator:176\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Datasets created successfully.\u001b[0m\n",
            "2024-06-01 18:00:14 [\u001b[34msrc.rolling_window_creator:177\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Shape of X_train: (17731, 396)\u001b[0m\n",
            "2024-06-01 18:00:14 [\u001b[34msrc.rolling_window_creator:178\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Shape of y_train: (17731, 1)\u001b[0m\n",
            "2024-06-01 18:00:14 [\u001b[34msrc.rolling_window_creator:179\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Shape of X_test: (100, 264)\u001b[0m\n",
            "2024-06-01 18:00:14 [\u001b[34msrc.rolling_window_creator:180\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Shape of y_test: (100, 1)\u001b[0m\n",
            "2024-06-01 18:00:14 [\u001b[34msrc.rolling_window_creator:134\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Creating rolling windows for train data...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Rolling: 100%|██████████| 20/20 [00:02<00:00,  7.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 18:00:16 [\u001b[34msrc.rolling_window_creator:140\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Extracting features for train data...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Feature Extraction: 100%|██████████| 20/20 [00:17<00:00,  1.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 18:00:35 [\u001b[34msrc.rolling_window_creator:148\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Calculating target for train data...\u001b[0m\n",
            "2024-06-01 18:00:35 [\u001b[34msrc.rolling_window_creator:134\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Creating rolling windows for test data...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rolling: 100%|██████████| 19/19 [00:02<00:00,  7.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 18:00:38 [\u001b[34msrc.rolling_window_creator:140\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Extracting features for test data...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Feature Extraction: 100%|██████████| 20/20 [00:01<00:00, 13.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 18:00:40 [\u001b[34msrc.rolling_window_creator:176\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Datasets created successfully.\u001b[0m\n",
            "2024-06-01 18:00:40 [\u001b[34msrc.rolling_window_creator:177\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Shape of X_train: (20131, 120)\u001b[0m\n",
            "2024-06-01 18:00:40 [\u001b[34msrc.rolling_window_creator:178\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Shape of y_train: (20131, 1)\u001b[0m\n",
            "2024-06-01 18:00:40 [\u001b[34msrc.rolling_window_creator:179\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Shape of X_test: (100, 120)\u001b[0m\n",
            "2024-06-01 18:00:40 [\u001b[34msrc.rolling_window_creator:180\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Shape of y_test: (100, 1)\u001b[0m\n",
            "2024-06-01 18:00:40 [\u001b[34msrc.rolling_window_creator:134\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Creating rolling windows for train data...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Rolling: 100%|██████████| 20/20 [00:05<00:00,  3.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-01 18:00:46 [\u001b[34msrc.rolling_window_creator:140\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Extracting features for train data...\u001b[0m\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Could not guess the value column! Please hand it to the function as an argument.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[29], line 18\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Teilt  die Trainingsdaten in Trainings- und Validierungsdaten auf\u001b[39;00m\n\u001b[1;32m     11\u001b[0m X_train_1, X_val_1, y_train_1, y_val_1 \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m     12\u001b[0m     X_train_1,\n\u001b[1;32m     13\u001b[0m     y_train_1,\n\u001b[1;32m     14\u001b[0m     test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,  \u001b[38;5;66;03m# 20% der Daten werden für die Validierung verwendet\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m X_train_2, y_train_2, X_test_2, y_test_2 \u001b[38;5;241m=\u001b[39m \u001b[43mrwCreator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_rolling_windows_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcleaned_train_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcleaned_test_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_RUL_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_RUL_data_2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#Train-test split\u001b[39;00m\n\u001b[1;32m     21\u001b[0m X_train_2, X_val_2, y_train_2, y_val_2 \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m     22\u001b[0m     X_train_2,\n\u001b[1;32m     23\u001b[0m     y_train_2,\n\u001b[1;32m     24\u001b[0m     test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,  \u001b[38;5;66;03m# 20% der Daten werden für die Validierung verwendet\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m  )\n",
            "File \u001b[0;32m~/Documents/Uni/PSDA/Übung 2/damage-propagation-modeling/src/rolling_window_creator.py:173\u001b[0m, in \u001b[0;36mRollingWindowDatasetCreator.create_rolling_windows_datasets\u001b[0;34m(self, train_data, test_data, test_RUL_data)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create the rolling windows datasets.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m:param train_data: The training dataset.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m:rtype: tuple\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_window_sizes(train_data, test_data)\n\u001b[0;32m--> 173\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(test_data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, test_RUL_data)\n\u001b[1;32m    176\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasets created successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/Documents/Uni/PSDA/Übung 2/damage-propagation-modeling/src/rolling_window_creator.py:141\u001b[0m, in \u001b[0;36mRollingWindowDatasetCreator._process_data\u001b[0;34m(self, data, data_type, test_RUL_data)\u001b[0m\n\u001b[1;32m    138\u001b[0m     rolled_data \u001b[38;5;241m=\u001b[39m rolled_data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumn_id)\u001b[38;5;241m.\u001b[39mtail(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_timeshift)\n\u001b[1;32m    140\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting features for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 141\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrolled_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mcolumn_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_sort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_sort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mdefault_fc_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_fc_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mimpute_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimpute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_warnings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m X\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mrename([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumn_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumn_sort])\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tsfresh/feature_extraction/extraction.py:164\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(timeseries_container, default_fc_parameters, kind_to_fc_parameters, column_id, column_sort, column_kind, column_value, chunksize, n_jobs, show_warnings, disable_progressbar, impute_function, profile, profiling_filename, profiling_sorting, distributor, pivot)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 164\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43m_do_extraction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeseries_container\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_kind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_kind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_sort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_sort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_warnings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_warnings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_fc_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_fc_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkind_to_fc_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind_to_fc_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistributor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistributor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpivot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpivot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# Impute the result if requested\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m impute_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tsfresh/feature_extraction/extraction.py:260\u001b[0m, in \u001b[0;36m_do_extraction\u001b[0;34m(df, column_id, column_value, column_kind, column_sort, default_fc_parameters, kind_to_fc_parameters, n_jobs, chunk_size, disable_progressbar, show_warnings, distributor, pivot)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_extraction\u001b[39m(\n\u001b[1;32m    194\u001b[0m     df,\n\u001b[1;32m    195\u001b[0m     column_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m     pivot,\n\u001b[1;32m    207\u001b[0m ):\n\u001b[1;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    Wrapper around the _do_extraction_on_chunk, which calls it on all chunks in the data frame.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m    A chunk is a subset of the data, with a given kind and id - so a single time series.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m    :rtype: pd.DataFrame\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mto_tsdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_kind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_sort\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m distributor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Iterable):\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tsfresh/feature_extraction/data.py:478\u001b[0m, in \u001b[0;36mto_tsdata\u001b[0;34m(df, column_id, column_kind, column_value, column_sort)\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m WideTsFrameAdapter(df, column_id, column_sort, [column_value])\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 478\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mWideTsFrameAdapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_sort\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(df, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TsDictAdapter(df, column_id, column_value, column_sort)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tsfresh/feature_extraction/data.py:184\u001b[0m, in \u001b[0;36mWideTsFrameAdapter.__init__\u001b[0;34m(self, df, column_id, column_sort, value_columns)\u001b[0m\n\u001b[1;32m    181\u001b[0m _check_nan(df, column_id)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value_columns:\n\u001b[0;32m--> 184\u001b[0m     value_columns \u001b[38;5;241m=\u001b[39m \u001b[43m_get_value_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_sort\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m _check_nan(df, \u001b[38;5;241m*\u001b[39mvalue_columns)\n\u001b[1;32m    187\u001b[0m _check_colname(\u001b[38;5;241m*\u001b[39mvalue_columns)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tsfresh/feature_extraction/data.py:152\u001b[0m, in \u001b[0;36m_get_value_columns\u001b[0;34m(df, *other_columns)\u001b[0m\n\u001b[1;32m    149\u001b[0m value_columns \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m other_columns]\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value_columns) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not guess the value column! Please hand it to the function as an argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m     )\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value_columns\n",
            "\u001b[0;31mValueError\u001b[0m: Could not guess the value column! Please hand it to the function as an argument."
          ]
        }
      ],
      "source": [
        "X_train_1_varc, y_train_1_varc, X_test_1_varc, y_test_1_varc = rwCreator_varc.create_rolling_windows_datasets(cleaned_train_1_varC,cleaned_test_1_varC,test_RUL_data_1)\n",
        "\n",
        "# Erstellen  die Rollfenster-Datensätze\n",
        "X_train_1, y_train_1, X_test_1, y_test_1 = rwCreator.create_rolling_windows_datasets(\n",
        "    train_data=cleaned_train_1,\n",
        "    test_data=cleaned_test_1,\n",
        "    test_RUL_data=test_RUL_data_1,\n",
        ")\n",
        "\n",
        "# Teilt  die Trainingsdaten in Trainings- und Validierungsdaten auf\n",
        "X_train_1, X_val_1, y_train_1, y_val_1 = train_test_split(\n",
        "    X_train_1,\n",
        "    y_train_1,\n",
        "    test_size=0.2,  # 20% der Daten werden für die Validierung verwendet\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_train_2, y_train_2, X_test_2, y_test_2 = rwCreator.create_rolling_windows_datasets(train_data=cleaned_train_2, test_data=cleaned_test_2,test_RUL_data=test_RUL_data_2)\n",
        "\n",
        "#Train-test split\n",
        "X_train_2, X_val_2, y_train_2, y_val_2 = train_test_split(\n",
        "    X_train_2,\n",
        "    y_train_2,\n",
        "    test_size=0.2,  # 20% der Daten werden für die Validierung verwendet\n",
        "    random_state=42  )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bfr_dedSZUjK"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SePwXGwXNj-h"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7PzljVdW8oS"
      },
      "outputs": [],
      "source": [
        "y_train_1_varc = y_train_1_varc.clip(upper=125)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BL7tfVI0Nmbw"
      },
      "outputs": [],
      "source": [
        "#Scalieren der Datensätze\n",
        "scaler_std = StandardScaler()\n",
        "\n",
        "# Data-Scaling\n",
        "scaler_1 = StandardScaler()\n",
        "X_train_scaled_1 = scaler_1.fit_transform(X_train_1)\n",
        "X_val_scaled_1 = scaler_1.transform(X_val_1)\n",
        "X_test_scaled_1 = scaler_1.transform(X_test_1)\n",
        "\n",
        "scaler_2 = StandardScaler()\n",
        "X_train_scaled_2 = scaler_2.fit_transform(X_train_2)\n",
        "X_val_scaled_2 = scaler_2.transform(X_val_2)\n",
        "X_test_scaled_2 = scaler_2.transform(X_test_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxGQ1UwGNotE"
      },
      "outputs": [],
      "source": [
        "X_train_1_varc[2:] = scaler_std.fit_transform(X_train_1_varc[2:])\n",
        "X_test_1_varc[2:] = scaler_std.fit_transform(X_test_1_varc[2:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaZclkiyORbn"
      },
      "source": [
        "# Traditional ML Models\n",
        "\n",
        "The models we tested are the following:\n",
        "\n",
        "KNeighborsRegressor  \\\\\n",
        "SupportVectorMachineRegressor \\\\\n",
        "RandomForestRegressor  \\\\\n",
        "MultiLayerPerceptronRegressor \\\\\n",
        "AdaBoostRegressor \\\\\n",
        "GaussianNaiveBayes \\\\\n",
        "KernelRidge \\\\\n",
        "Lasso  \\\\\n",
        "LinearRegressor \\\\\n",
        "LogisiticRegressor \\\\\n",
        "GradBoostRegressor \\\\\n",
        "XGBoostRegressor \\\\\n",
        "ExtraTrees\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8Ci80IHWFho"
      },
      "outputs": [],
      "source": [
        "#for reasons of clarity we only show the models on the first Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "HpiZQvAUQhC4",
        "outputId": "aa6b5d8d-fb79-4ab9-922c-df6692bdd1ea"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'KNeighborsRegressor' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-cfb92fdd615c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# KNeighorsRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrgr\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mrgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_1_varc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_1_varc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_1_varc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_1_varc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'KNeighborsRegressor' is not defined"
          ]
        }
      ],
      "source": [
        "# KNeighorsRegressor\n",
        "rgr  = KNeighborsRegressor(3)\n",
        "rgr.fit(X_train_1_varc, y_train_1_varc.values.ravel())\n",
        "print(np.sqrt(mean_squared_error(y_test_1_varc, rgr.predict(X_test_1_varc))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "EFsvLH8nX7zN",
        "outputId": "e1cc579e-6b33-487e-c1c9-8a6b2def109e"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'SVR' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-2b22df7e6b73>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# SupportVectorMachineRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"linear\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mrgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_1_varc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_1_varc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_1_varc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_1_varc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SVR' is not defined"
          ]
        }
      ],
      "source": [
        "# SupportVectorMachineRegressor\n",
        "rgr = SVR(kernel=\"linear\", C=0.025)\n",
        "rgr.fit(X_train_1_varc, y_train_1_varc.values.ravel())\n",
        "print(np.sqrt(mean_squared_error(y_test_1_varc, rgr.predict(X_test_1_varc))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuIVCSKiZsVy"
      },
      "outputs": [],
      "source": [
        "# RandomForestRegressor\n",
        "rgr  = RandomForestRegressor(max_depth=5, n_estimators=10, max_features=1, random_state=42)\n",
        "rgr.fit(X_train_1_varc, y_train_1_varc.values.ravel())\n",
        "print(np.sqrt(mean_squared_error(y_test_1_varc, rgr.predict(X_test_1_varc))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1yBOWnuZ6E8"
      },
      "outputs": [],
      "source": [
        "# MultiLayerPerceptronRegressor\n",
        "rgr  = MLPRegressor(alpha=1, max_iter=1000, random_state=42)\n",
        "rgr.fit(X_train_1_varc, y_train_1_varc.values.ravel())\n",
        "print(np.sqrt(mean_squared_error(y_test_1_varc, rgr.predict(X_test_1_varc))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W94wA5Wjacrr"
      },
      "outputs": [],
      "source": [
        "# AdaBoostRegressor\n",
        "rgr  = AdaBoostRegressor(random_state=42)\n",
        "rgr.fit(X_train_1_varc, y_train_1_varc.values.ravel())\n",
        "print(np.sqrt(mean_squared_error(y_test_1_varc, rgr.predict(X_test_1_varc))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKz9RpniahgG"
      },
      "outputs": [],
      "source": [
        "# GaussianNaiveBayes\n",
        "rgr  = GaussianNB()\n",
        "rgr.fit(X_train_1_varc, y_train_1_varc.values.ravel())\n",
        "print(np.sqrt(mean_squared_error(y_test_1_varc, rgr.predict(X_test_1_varc))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEUzFKiWaly2"
      },
      "outputs": [],
      "source": [
        "# KernelRidgeRegressor\n",
        "rgr  = KernelRidge()\n",
        "rgr.fit(X_train_1_varc, y_train_1_varc.values.ravel())\n",
        "print(np.sqrt(mean_squared_error(y_test_1_varc, rgr.predict(X_test_1_varc))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJVHwY4Kapk4"
      },
      "outputs": [],
      "source": [
        "# Lasso\n",
        "rgr  = Lasso()\n",
        "rgr.fit(X_train_1_varc, y_train_1_varc.values.ravel())\n",
        "print(np.sqrt(mean_squared_error(y_test_1_varc, rgr.predict(X_test_1_varc))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jc8GGzi5arSv"
      },
      "outputs": [],
      "source": [
        "# LinearRegressor\n",
        "rgr  = LinearRegression()\n",
        "rgr.fit(X_train_1_varc, y_train_1_varc.values.ravel())\n",
        "print(np.sqrt(mean_squared_error(y_test_1_varc, rgr.predict(X_test_1_varc))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzS70lbGauO4"
      },
      "outputs": [],
      "source": [
        "# LogisticRegressor\n",
        "rgr  = LogisticRegression()\n",
        "rgr.fit(X_train_1_varc, y_train_1_varc.values.ravel())\n",
        "print(np.sqrt(mean_squared_error(y_test_1_varc, rgr.predict(X_test_1_varc))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hp8tYg-Caxsb"
      },
      "outputs": [],
      "source": [
        "# GradientBoostRegressor\n",
        "rgr  = GradientBoostingRegressor()\n",
        "rgr.fit(X_train_1_varc, y_train_1_varc.values.ravel())\n",
        "print(np.sqrt(mean_squared_error(y_test_1_varc, rgr.predict(X_test_1_varc))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dp1cmvlfa06e"
      },
      "outputs": [],
      "source": [
        "# XGBoostRegressor\n",
        "rgr  = XGBRegressor()\n",
        "rgr.fit(X_train_1_varc, y_train_1_varc.values.ravel())\n",
        "print(np.sqrt(mean_squared_error(y_test_1_varc, rgr.predict(X_test_1_varc))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64fuhOF2fX41"
      },
      "outputs": [],
      "source": [
        "#ExtraTreeRegressor\n",
        "extra_trees = ExtraTreesRegressor(n_estimators=140, max_depth= 15, min_samples_leaf=5, random_state=42)\n",
        "\n",
        "# Training\n",
        "extra_trees.fit(X_train_scaled_1, y_train_1)\n",
        "\n",
        "# Vorhersagen und Bewerten des ExtraTreesRegressor\n",
        "et_predictions_1 = extra_trees.predict(X_val_scaled_1)\n",
        "print(sklearn.metrics.root_mean_squared_error(y_val_1, et_predictions_1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xw-5hLxPiSF"
      },
      "source": [
        "# Findings: Models\n",
        "\n",
        "Most notable of the first spectated models are KNeighbor, RandomForest, Lasso and GradientBoost. Except for GradientBoost all other regressors will be optimized for. Instead of GradientBoost we will instead optimize for XGBoost. \\\\\n",
        "The reasons for this decisions are that the computation time for XGB is far shorter than GradBoost and also according to some papers XGBoost can perform very well on this dataset if optimized correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ3HcLxaPqZv"
      },
      "source": [
        "### Optimization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vu_uw8J9MDp2"
      },
      "outputs": [],
      "source": [
        "# for the Optimization we also used TPOT to find the best model\n",
        "# the result of the TPOT was the ExtraTreeRegressor\n",
        "# due to very long runtime, the TPOT is in comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkOFyWGEVpll"
      },
      "outputs": [],
      "source": [
        "#tpot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tpot import TPOTRegressor\n",
        "\n",
        "# TPOT\n",
        "#tpot = TPOTRegressor(generations=3, population_size=20, cv=3, verbosity=2, random_state=42)\n",
        "\n",
        "#tpot.fit(X_train_scaled_1, y_train_1)\n",
        "\n",
        "#print(tpot.score(X_test_scaled_1, y_test_1))\n",
        "\n",
        "#tpot.export('best_model_pipeline.py')\n",
        "\n",
        "#print(tpot.fitted_pipeline_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KGGq6krfQcv"
      },
      "source": [
        "# Dataset 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "G-yJmoYSQxaJ",
        "outputId": "92917082-92f2-4dc3-f021-8528b7e525df"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'load_data' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c5c171a44670>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Load first Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_data_1_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_1_opt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_rul_data_1_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPATH_TO_CONFIG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcleaned_train_1_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcleaned_test_1_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_1_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_1_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'UnitNumber'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Cycle'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold_corr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Train Val Split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_data' is not defined"
          ]
        }
      ],
      "source": [
        "## Load first Dataset\n",
        "train_data_1_opt, test_data_1_opt,test_rul_data_1_opt = load_data(config_path=PATH_TO_CONFIG, dataset_num=1)\n",
        "cleaned_train_1_opt, cleaned_test_1_opt = clean_data(train_data_1_opt, test_data_1_opt, method=None, ignore_columns=['UnitNumber', 'Cycle'], threshold_missing=0.1, threshold_corr=0.5)\n",
        "\n",
        "# Train Val Split\n",
        "cl_train_1_opt, cl_val_1_opt = train_val_split_by_group(df = cleaned_train_1_opt,group = \"UnitNumber\",test_size = 0.18,n_splits = 2,random_state = 7)\n",
        "\n",
        "# RollingWindowParameter\n",
        "min_ts_1_opt = 29\n",
        "max_ts_1_opt = 30\n",
        "feature_list_ds_1 = [\"c3\", \"quantile\", \"mean\", \"root_mean_square\", \"median\", \"time_reversal_asymmetry_statistic\", \"absolute_maximum\", \"maximum\", \"minimum\", \"agg_autocorrelation\", \"autocorrelation\" ]\n",
        "# Create RollingWindows\n",
        "rwCreator = RollingWindowDatasetCreator(max_timeshift=max_ts_1_opt,min_timeshift=min_ts_1_opt,feature_extraction_mode= 'custom',feature_list=feature_list_ds_1)\n",
        "X_train_1_opt, y_train_1_opt = rwCreator._process_data(cl_train_1_opt, 'train')\n",
        "X_val_1_opt, y_val_1_opt = rwCreator._process_data(cl_val_1_opt, 'train')\n",
        "X_test_1_opt, y_test_1_opt = rwCreator._process_data(cleaned_test_1_opt, 'test', test_rul_data_1_opt)\n",
        "\n",
        "#Data Preprocessing\n",
        "y_train_1_opt = y_train_1_opt.clip(upper=125)\n",
        "scaler = StandardScaler()\n",
        "X_train_1_opt[2:] = scaler.fit_transform(X_train_1_opt[2:])\n",
        "X_val_1_opt[2:] = scaler.fit_transform(X_val_1_opt[2:])\n",
        "X_test_1_opt[2:] = scaler.fit_transform(X_test_1_opt[2:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGZlqcWjenQi"
      },
      "outputs": [],
      "source": [
        "#knn optimization\n",
        "def hyperparameter_function_knn(neighbours):\n",
        "      \"\"\" Function for hyperparameter optimization\n",
        "      \"\"\"\n",
        "      neighbours = neighbours.round().astype(int)\n",
        "      knn_regressor = KNeighborsRegressor(n_neighbors=neighbours)\n",
        "      metric = cross_val_score(knn_regressor, X=X_train_1_opt, y=y_train_1_opt, cv=5, scoring='neg_root_mean_squared_error')\n",
        "      return metric.min()\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'neighbours': (1, 750)}\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "  f=hyperparameter_function_knn,\n",
        "  pbounds=pbounds,\n",
        "  random_state=17,\n",
        "  allow_duplicate_points= True\n",
        ")\n",
        "\n",
        "optimizer.maximize(\n",
        "  init_points=10,\n",
        "  n_iter=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "fyxarzUsewUM",
        "outputId": "057c023b-a7c0-4980-fa54-99ec145d11e3"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'KNeighborsRegressor' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-43a6e1777add>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Neigbors = 383\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Testing of Knn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mknn_regressor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m383\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mknn_regressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_1_opt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_1_opt\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_pred_1_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn_regressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_1_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'KNeighborsRegressor' is not defined"
          ]
        }
      ],
      "source": [
        "knn_regressor = KNeighborsRegressor(n_neighbors=383)\n",
        "knn_regressor.fit(X_train_1_opt,y_train_1_opt )\n",
        "y_pred_1_opt = knn_regressor.predict(X_test_1_opt)\n",
        "print(np.sqrt(mean_squared_error(y_test_1_opt, y_pred_1_opt)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaAt3MyXQ6nF"
      },
      "source": [
        "Results: 23.36\n",
        "\n",
        "Notes: KNR works very well and achieved similar values in both validation and testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4o3_Faebey9_"
      },
      "outputs": [],
      "source": [
        "#Random Forest Optimization\n",
        "def hyperparameter_function_rf(n_estimators, max_features, ):\n",
        "      \"\"\" Function for hyperparameter optimization\n",
        "      \"\"\"\n",
        "      n_estimators = n_estimators.round().astype(int)\n",
        "      max_features = max_features.round().astype(int)\n",
        "\n",
        "      rf_regressor = RandomForestRegressor(n_estimators=n_estimators,max_features=max_features,random_state=17,n_jobs=-1)\n",
        "      metric = cross_val_score(rf_regressor, X=X_train_1_opt, y=y_train_1_opt.values.ravel(), cv=5, scoring='neg_root_mean_squared_error')\n",
        "      return metric.min()\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'n_estimators': (20, 500),'max_features': (1,1) }\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "  f=hyperparameter_function_rf,\n",
        "  pbounds=pbounds,\n",
        "  random_state=17,\n",
        "  allow_duplicate_points= True\n",
        ")\n",
        "\n",
        "optimizer.maximize(\n",
        "  init_points=10,\n",
        "  n_iter=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "IkOf6uOOezWX",
        "outputId": "73da7f9d-a09e-4e0e-8ea5-ab3aa317a421"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'RandomForestRegressor' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-df87e1625f83>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# n_estimators=296 , max_features=4 -> 22.93\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrf_regressor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m296\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mrf_regressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_1_opt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_1_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_pred_1_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_regressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_1_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_1_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_1_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'RandomForestRegressor' is not defined"
          ]
        }
      ],
      "source": [
        "# n_estimators=296 , max_features=4 -> 22.93\n",
        "\n",
        "#Random Forest Regressor Testing\n",
        "rf_regressor = RandomForestRegressor(n_estimators=296,max_features=4,random_state=17)\n",
        "rf_regressor.fit(X_train_1_opt,y_train_1_opt.values.ravel())\n",
        "y_pred_1_opt = rf_regressor.predict(X_test_1_opt)\n",
        "print(np.sqrt(mean_squared_error(y_test_1_opt, y_pred_1_opt)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVq__ks8Q9Hq"
      },
      "source": [
        "Results: 18.47\n",
        "\n",
        "Notes: RFR was one of the best performer for the custom feature set on tsfresh\n",
        "with reaching a peak in optimization of 18.56. This is far above expectation since similar papers stopped at the low twenties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpb-0Ihpe36V"
      },
      "outputs": [],
      "source": [
        "#Lasso Optimization\n",
        "def hyperparameter_function_lasso(alpha, max_iter, ):\n",
        "      \"\"\" Function for hyperparameter optimization\n",
        "      \"\"\"\n",
        "      max_iter = max_iter.round().astype(int)\n",
        "\n",
        "      lasso_regressor = Lasso(alpha=alpha,max_iter=max_iter,random_state=17)\n",
        "      metric = cross_val_score(lasso_regressor, X=X_train_1_opt, y=y_train_1_opt.values.ravel(), cv=5, scoring='neg_root_mean_squared_error')\n",
        "      return metric.min()\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'alpha': (0.001, 1),'max_iter': (100,10000) }\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "  f=hyperparameter_function_lasso,\n",
        "  pbounds=pbounds,\n",
        "  random_state=17,\n",
        "  allow_duplicate_points= True\n",
        ")\n",
        "\n",
        "optimizer.maximize(\n",
        "  init_points=10,\n",
        "  n_iter=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ipRp6RSe4Lx"
      },
      "outputs": [],
      "source": [
        "lasso_regressor = Lasso(alpha=0.0319,max_iter=656,random_state=17)\n",
        "lasso_regressor.fit(X_train_1_opt,y_train_1_opt )\n",
        "y_pred_1_opt = lasso_regressor.predict(X_test_1_opt)\n",
        "print(np.sqrt(mean_squared_error(y_test_1_opt, y_pred_1_opt)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7YUpWmYRAC_"
      },
      "source": [
        "Results: 1620.91\n",
        "\n",
        "Notes: Despite the first test Lasso seems to be underperforming. Which might be because of the hyperparameter optimization or other factors such as the random_stare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42An6m-je4a2"
      },
      "outputs": [],
      "source": [
        "#XGBoost Optimization\n",
        "def hyperparameter_function_xgboost(eta, gamma ,max_depth,reg_lambda,reg_alpha ):\n",
        "      \"\"\" Function for hyperparameter optimization\n",
        "      \"\"\"\n",
        "      max_depth = max_depth.round().astype(int)\n",
        "\n",
        "      xgb_regressor = XGBRegressor(eta=eta,gamma=gamma,max_depth=max_depth,reg_lambda=reg_lambda, reg_alpha=reg_alpha)\n",
        "      metric = cross_val_score(xgb_regressor, X=X_train_1_opt, y=y_train_1_opt.values.ravel(), cv=5, scoring='neg_root_mean_squared_error')\n",
        "      return metric.min()\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'eta': (0, 1),'gamma': (0,2),'max_depth':(1,10),'reg_lambda':(1,1),'reg_alpha':(0,0) }\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "  f=hyperparameter_function_xgboost,\n",
        "  pbounds=pbounds,\n",
        "  random_state=17,\n",
        "  allow_duplicate_points= True\n",
        ")\n",
        "\n",
        "optimizer.maximize(\n",
        "  init_points=10,\n",
        "  n_iter=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "do9vfC3le4oH"
      },
      "outputs": [],
      "source": [
        "xgb_regressor = XGBRegressor(eta=0.09569,gamma=0.05334,max_depth=4,reg_lambda=1, reg_alpha=0)\n",
        "xgb_regressor.fit(X_train_1_opt,y_train_1_opt )\n",
        "y_pred_1_opt = xgb_regressor.predict(X_test_1_opt)\n",
        "\n",
        "print(np.sqrt(mean_squared_error(y_test_1_opt, y_pred_1_opt)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCnCOABRRCHs"
      },
      "source": [
        "Results: 19.02\n",
        "\n",
        "Notes: The XGBoost results is also very solid and achieved comparable results to the paper which featured it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyQA_R8-WGm3"
      },
      "outputs": [],
      "source": [
        "# Define the function to optimize\n",
        "def evaluate_model(n_estimators, max_depth, min_samples_split, min_samples_leaf):\n",
        "    # Make sure parameters are integer\n",
        "    n_estimators = int(n_estimators)\n",
        "    max_depth = int(max_depth)\n",
        "    min_samples_split = int(min_samples_split)\n",
        "    min_samples_leaf = int(min_samples_leaf)\n",
        "\n",
        "    # Define the model with the parameters\n",
        "    model = ExtraTreesRegressor(n_estimators=n_estimators, max_depth=max_depth,\n",
        "                                min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n",
        "                                random_state=42, n_jobs=-1)\n",
        "\n",
        "    # Fit and predict\n",
        "    model.fit(X_train_1, y_train_1)\n",
        "    pred = model.predict(X_val_1)\n",
        "\n",
        "    # Calculate RMSE\n",
        "    rmse = mean_squared_error(y_val_1, pred, squared=False)\n",
        "\n",
        "    # We want to minimize RMSE, so we return the negative value\n",
        "    return -rmse\n",
        "\n",
        "\n",
        "# Define the bounds of the parameters\n",
        "param_bounds = {\n",
        "    'n_estimators': (100, 200),\n",
        "    'max_depth': (10, 30),\n",
        "    'min_samples_split': (2, 10),\n",
        "    'min_samples_leaf': (1, 4)\n",
        "}\n",
        "# Create the BayesianOptimization object\n",
        "optimizer = BayesianOptimization(\n",
        "    f=evaluate_model,\n",
        "    pbounds=param_bounds,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "# Perform the optimization\n",
        "optimizer.maximize(init_points=5, n_iter=12)\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Beste Hyperparameter-Kombination:\", optimizer.max['params'])\n",
        "#Beste Hyperparameter-Kombination: {'max_depth': 26.648852816008436, 'min_samples_leaf': 1.6370173320348285, 'min_samples_split': 3.454599737656805, 'n_estimators': 118.34045098534338}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhcCZk9MWGY_"
      },
      "outputs": [],
      "source": [
        "#Prediction on testdata\n",
        "extra_trees = ExtraTreesRegressor(n_estimators=118, max_depth=27, min_samples_split=3, min_samples_leaf=2, random_state=42)\n",
        "# Training\n",
        "extra_trees.fit(X_train_scaled_1, y_train_1)\n",
        "test_predictions = extra_trees.predict(X_test_scaled_1)\n",
        "test_rmse = np.sqrt(sklearn.metrics.root_mean_squared_error(y_test_1, test_predictions))\n",
        "print(f\"Test RMSE: {test_rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxOz11HOYLL0"
      },
      "source": [
        "Result:\n",
        "Test RMSE: 7.9324\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QvYXX2MfWE4"
      },
      "source": [
        "The optimization for the first dataset went well. We reach comparable results we found in papers for this dataset with traditional ML approaches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b78kYcudRq6"
      },
      "source": [
        "# Dataset 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfsB1YhuQzGC"
      },
      "outputs": [],
      "source": [
        "## Load second dataset\n",
        "train_data_2_opt, test_data_2_opt,test_rul_data_2_opt = load_data(config_path=PATH_TO_CONFIG, dataset_num=2)\n",
        "cleaned_train_2_opt, cleaned_test_2_opt = clean_data(train_data_2_opt, test_data_2_opt, method=None, ignore_columns=['UnitNumber', 'Cycle','Operation Setting 2','Operation Setting 3','Sensor Measure 2','Sensor Measure 3','Sensor Measure 4','Sensor Measure 8','Sensor Measure 9','Sensor Measure 11','Sensor Measure 15','Sensor Measure 17'], threshold_missing=0.1, threshold_corr=0.5)\n",
        "\n",
        "# Train Val Split\n",
        "cl_train_2_opt, cl_val_2_opt = train_val_split_by_group(df = cleaned_train_2_opt,group = \"UnitNumber\",test_size = 0.18,n_splits = 2,random_state = 7)\n",
        "\n",
        "## RollingWindowParameter\n",
        "min_ts_2_opt = 17\n",
        "max_ts_2_opt = 18\n",
        "feature_list_ds_2 = [\"c3\", \"quantile\", \"mean\", \"median\", \"root_mean_square\", \"variance\", \"mean_abs_change\", \"standard_deviation\", \"skewness\", \"variation_coefficient\", \"last_location_of_maximum\", \"first_location_of_maximum\"]\n",
        "\n",
        "# Create RollingWindows\n",
        "#rwCreator = RollingWindowDatasetCreator(max_timeshift=max_ts_2_opt,min_timeshift=min_ts_2_opt,feature_extraction_mode= 'custom',feature_list=feature_list_ds_3)\n",
        "rwCreator = RollingWindowDatasetCreator(max_timeshift=max_ts_2_opt,min_timeshift=min_ts_2_opt,feature_extraction_mode= 'minimal',feature_list=[\"median\"])\n",
        "X_train_2_opt, y_train_2_opt = rwCreator._process_data(cl_train_2_opt, 'train')\n",
        "X_val_2_opt, y_val_2_opt = rwCreator._process_data(cl_val_2_opt, 'train')\n",
        "X_test_2_opt, y_test_2_opt = rwCreator._process_data(cleaned_test_2_opt, 'test', test_rul_data_2_opt)\n",
        "\n",
        "# Data Preprocessing\n",
        "y_train_2_opt = y_train_2_opt.clip(upper=125)\n",
        "scaler = StandardScaler()\n",
        "X_train_2_opt[2:] = scaler.fit_transform(X_train_2_opt[2:])\n",
        "X_val_2_opt[2:] = scaler.fit_transform(X_val_2_opt[2:])\n",
        "X_test_2_opt[2:] = scaler.fit_transform(X_test_2_opt[2:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tQyle6ii2B8"
      },
      "outputs": [],
      "source": [
        "def hyperparameter_function_knn(neighbours):\n",
        "      \"\"\" Function for hyperparameter optimization\n",
        "      \"\"\"\n",
        "      neighbours = neighbours.round().astype(int)\n",
        "      knn_regressor = KNeighborsRegressor(n_neighbors=neighbours)\n",
        "      metric = cross_val_score(knn_regressor, X=X_train_2_opt, y=y_train_2_opt, cv=5, scoring='neg_root_mean_squared_error')\n",
        "      return metric.min()\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'neighbours': (1, 750)}\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "  f=hyperparameter_function_knn,\n",
        "  pbounds=pbounds,\n",
        "  random_state=17,\n",
        "  allow_duplicate_points= True\n",
        ")\n",
        "\n",
        "optimizer.maximize(\n",
        "  init_points=10,\n",
        "  n_iter=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owCMUTHoi2Sf"
      },
      "outputs": [],
      "source": [
        "knn_regressor = KNeighborsRegressor(n_neighbors=20)\n",
        "knn_regressor.fit(X_train_2_opt,y_train_2_opt )\n",
        "y_pred_2_opt = knn_regressor.predict(X_test_2_opt)\n",
        "print(np.sqrt(mean_squared_error(y_test_2_opt, y_pred_2_opt)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PL8vaR6Sn3g"
      },
      "source": [
        "Results: 38.46\n",
        "\n",
        "Notes: In general all results besides ExtraTrees perform significantly weaker than in datasets 1.\n",
        "Which might be another indicator for data leakage in the implementation of ExtraTrees.\n",
        "Still KNeighbors performs as a one of the worst (when Lasso is disregarded and out of competition)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0xhQMOUi2j8"
      },
      "outputs": [],
      "source": [
        "def hyperparameter_function_rf(n_estimators, max_features, ):\n",
        "      \"\"\" Function for hyperparameter optimization\n",
        "      \"\"\"\n",
        "      n_estimators = n_estimators.round().astype(int)\n",
        "      max_features = max_features.round().astype(int)\n",
        "\n",
        "      rf_regressor = RandomForestRegressor(n_estimators=n_estimators,max_features=max_features,random_state=17,n_jobs=-1)\n",
        "      metric = cross_val_score(rf_regressor, X=X_train_2_opt, y=y_train_2_opt.values.ravel(), cv=5, scoring='neg_root_mean_squared_error')\n",
        "      return metric.min()\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'n_estimators': (20, 500),'max_features': (1,1) }\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "  f=hyperparameter_function_rf,\n",
        "  pbounds=pbounds,\n",
        "  random_state=17,\n",
        "  allow_duplicate_points= True\n",
        ")\n",
        "\n",
        "optimizer.maximize(\n",
        "  init_points=10,\n",
        "  n_iter=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hQwPa9Li2zl"
      },
      "outputs": [],
      "source": [
        "rf_regressor = RandomForestRegressor(n_estimators=296,max_features=4)\n",
        "rf_regressor.fit(X_train_2_opt,y_train_2_opt.values.ravel())\n",
        "y_pred_2_opt = rf_regressor.predict(X_test_2_opt)\n",
        "print(np.sqrt(mean_squared_error(y_test_2_opt, y_pred_2_opt)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxMxbxgxSsCs"
      },
      "source": [
        "Results: 34.75\n",
        "\n",
        "Notes: This might result in weaker results.\n",
        "RandomForest still performs rather well and is at the same level as XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZG-XM-Ai3ER"
      },
      "outputs": [],
      "source": [
        "def hyperparameter_function_lasso(alpha, max_iter, ):\n",
        "      \"\"\" Function for hyperparameter optimization\n",
        "      \"\"\"\n",
        "      max_iter = max_iter.round().astype(int)\n",
        "\n",
        "      lasso_regressor = Lasso(alpha=alpha,max_iter=max_iter,random_state=17)\n",
        "      metric = cross_val_score(lasso_regressor, X=X_train_1_opt, y=y_train_1_opt.values.ravel(), cv=5, scoring='neg_root_mean_squared_error')\n",
        "      return metric.min()\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'alpha': (0.001, 1),'max_iter': (100,10000) }\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "  f=hyperparameter_function_lasso,\n",
        "  pbounds=pbounds,\n",
        "  random_state=17,\n",
        "  allow_duplicate_points= True\n",
        ")\n",
        "\n",
        "optimizer.maximize(\n",
        "  init_points=10,\n",
        "  n_iter=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zms-YrB-i3Vv"
      },
      "outputs": [],
      "source": [
        "# alpha= , max_iter= ->\n",
        "lasso_regressor = Lasso(alpha=0.9457,max_iter=694)\n",
        "lasso_regressor.fit(X_train_2_opt,y_train_2_opt )\n",
        "y_pred_2_opt = lasso_regressor.predict(X_test_2_opt)\n",
        "print(np.sqrt(mean_squared_error(y_test_2_opt, y_pred_2_opt)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C22qJ9LS0xe"
      },
      "source": [
        "Results: 86.99\n",
        "\n",
        "Notes: In this dataset Lasso doesnt perform good, but it seems to be far more stable then in the other cases with RMSE of over 1000."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4nsnsz3i4W5"
      },
      "outputs": [],
      "source": [
        "def hyperparameter_function_xgboost(eta, gamma ,max_depth,reg_lambda,reg_alpha ):\n",
        "      \"\"\" Function for hyperparameter optimization\n",
        "      \"\"\"\n",
        "      max_depth = max_depth.round().astype(int)\n",
        "\n",
        "      xgb_regressor = XGBRegressor(eta=eta,gamma=gamma,max_depth=max_depth,reg_lambda=reg_lambda, reg_alpha=reg_alpha)\n",
        "      metric = cross_val_score(xgb_regressor, X=X_train_1_opt, y=y_train_1_opt.values.ravel(), cv=5, scoring='neg_root_mean_squared_error')\n",
        "      return metric.min()\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'eta': (0, 1),'gamma': (0,2),'max_depth':(1,10),'reg_lambda':(1,1),'reg_alpha':(0,0) }\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "  f=hyperparameter_function_xgboost,\n",
        "  pbounds=pbounds,\n",
        "  random_state=17,\n",
        "  allow_duplicate_points= True\n",
        ")\n",
        "\n",
        "optimizer.maximize(\n",
        "  init_points=10,\n",
        "  n_iter=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rup6BBQSi4td"
      },
      "outputs": [],
      "source": [
        "xgb_regressor = XGBRegressor(eta=0.02803,gamma=0.8998,max_depth=6,reg_lambda=1, reg_alpha=0)\n",
        "xgb_regressor.fit(X_train_2_opt,y_train_2_opt )\n",
        "y_pred_2_opt = xgb_regressor.predict(X_test_2_opt)\n",
        "print(np.sqrt(mean_squared_error(y_test_2_opt, y_pred_2_opt)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RywEqx6RS5mv"
      },
      "source": [
        "Results: 34.11\n",
        "\n",
        "Notes: Is performing really well. If the was a need to further improve the result XGBoostRegressor would still possess options to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTxH_ELZWZrE"
      },
      "outputs": [],
      "source": [
        "#Bayesian Optimzier für Dataset 2\n",
        "# Define the function to optimize\n",
        "def evaluate_model(n_estimators, max_depth, min_samples_split, min_samples_leaf):\n",
        "    # Make sure parameters are integer\n",
        "    n_estimators = int(n_estimators)\n",
        "    max_depth = int(max_depth)\n",
        "    min_samples_split = int(min_samples_split)\n",
        "    min_samples_leaf = int(min_samples_leaf)\n",
        "\n",
        "    # Define the model with the parameters\n",
        "    model = ExtraTreesRegressor(n_estimators=n_estimators, max_depth=max_depth,\n",
        "                                min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n",
        "                                random_state=42, n_jobs=-1)\n",
        "\n",
        "    # Fit and predict\n",
        "    model.fit(X_train_2, y_train_2)\n",
        "    pred = model.predict(X_val_2)\n",
        "\n",
        "    # Calculate RMSE\n",
        "    rmse = mean_squared_error(y_val_2, pred, squared=False)\n",
        "\n",
        "    # We want to minimize RMSE, so we return the negative value\n",
        "    return -rmse\n",
        "\n",
        "\n",
        "# Define the bounds of the parameters\n",
        "param_bounds = {\n",
        "    'n_estimators': (100, 200),\n",
        "    'max_depth': (10, 30),\n",
        "    'min_samples_split': (2, 10),\n",
        "    'min_samples_leaf': (1, 4)\n",
        "}\n",
        "# Create the BayesianOptimization object\n",
        "optimizer = BayesianOptimization(\n",
        "    f=evaluate_model,\n",
        "    pbounds=param_bounds,\n",
        "    random_state=42,\n",
        ")\n",
        "# Perform the optimization\n",
        "optimizer.maximize(init_points=5, n_iter=12)\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Beste Hyperparameter-Kombination:\", optimizer.max['params'])\n",
        "#Best hyperparameter-combination: {'max_depth': 30.0, 'min_samples_leaf': 1.0, 'min_samples_split': 2.0, 'n_estimators': 151.63447537285498}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ks__2GHVXLP8"
      },
      "outputs": [],
      "source": [
        "#prediction on the testdata\n",
        "extra_trees = ExtraTreesRegressor(n_estimators=140, max_depth= 15, min_samples_leaf=5, random_state=42)\n",
        "\n",
        "# Training\n",
        "extra_trees.fit(X_train_scaled_2, y_train_2)\n",
        "test_predictions = extra_trees.predict(X_test_scaled_2)\n",
        "test_rmse = np.sqrt(sklearn.metrics.root_mean_squared_error(y_test_2, test_predictions))\n",
        "print(f\"Test RMSE: {test_rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_L8HtnjXwLs"
      },
      "source": [
        "Result:\n",
        "Test RMSE: 8.7472\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PD53NKy9dZyC"
      },
      "source": [
        "# Dataset 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvKSE7X9Q0WG"
      },
      "outputs": [],
      "source": [
        "## Load third dataset\n",
        "train_data_3_opt, test_data_3_opt,test_rul_data_3_opt = load_data(config_path=PATH_TO_CONFIG, dataset_num=3)\n",
        "cleaned_train_3_opt, cleaned_test_3_opt = clean_data(train_data_3_opt, test_data_3_opt, method=None, ignore_columns=['UnitNumber', 'Cycle'], threshold_missing=0.1, threshold_corr=0.5)\n",
        "\n",
        "# Train Val Split\n",
        "cl_train_3_opt, cl_val_3_opt = train_val_split_by_group(df = cleaned_train_3_opt,group = \"UnitNumber\",test_size = 0.18,n_splits = 2,random_state = 7)\n",
        "\n",
        "## RollingWindowParameter\n",
        "min_ts_3_opt = 29\n",
        "max_ts_3_opt = 30\n",
        "feature_list_ds_3 = [\"c3\", \"quantile\", \"mean\", \"root_mean_square\", \"median\", \"time_reversal_asymmetry_statistic\", \"absolute_maximum\", \"maximum\", \"minimum\", \"agg_autocorrelation\", \"autocorrelation\" ]\n",
        "# Create RollingWindows\n",
        "rwCreator = RollingWindowDatasetCreator(max_timeshift=max_ts_3_opt,min_timeshift=min_ts_3_opt,feature_extraction_mode= 'custom',feature_list=feature_list_ds_3)\n",
        "X_train_3_opt, y_train_3_opt = rwCreator._process_data(cl_train_3_opt, 'train')\n",
        "X_val_3_opt, y_val_3_opt = rwCreator._process_data(cl_val_3_opt, 'train')\n",
        "X_test_3_opt, y_test_3_opt = rwCreator._process_data(cleaned_test_3_opt, 'test', test_rul_data_3_opt)\n",
        "\n",
        "# Data Preprocessing\n",
        "y_train_3_opt = y_train_3_opt.clip(upper=125)\n",
        "scaler = StandardScaler()\n",
        "X_train_3_opt[2:] = scaler.fit_transform(X_train_3_opt[2:])\n",
        "X_val_3_opt[2:] = scaler.fit_transform(X_val_3_opt[2:])\n",
        "X_test_3_opt[2:] = scaler.fit_transform(X_test_3_opt[2:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPACF76Lh9Wb"
      },
      "outputs": [],
      "source": [
        "def hyperparameter_function_knn(neighbours):\n",
        "      \"\"\" Function for hyperparameter optimization\n",
        "      \"\"\"\n",
        "      neighbours = neighbours.round().astype(int)\n",
        "      knn_regressor = KNeighborsRegressor(n_neighbors=neighbours)\n",
        "      metric = cross_val_score(knn_regressor, X=X_train_3_opt, y=y_train_3_opt, cv=5, scoring='neg_root_mean_squared_error')\n",
        "      return metric.min()\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'neighbours': (1, 750)}\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "  f=hyperparameter_function_knn,\n",
        "  pbounds=pbounds,\n",
        "  random_state=17,\n",
        "  allow_duplicate_points= True\n",
        ")\n",
        "\n",
        "optimizer.maximize(\n",
        "  init_points=10,\n",
        "  n_iter=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyQKfsheh9G8"
      },
      "outputs": [],
      "source": [
        "# Neigbors = 300\n",
        "knn_regressor = KNeighborsRegressor(n_neighbors=300)\n",
        "knn_regressor.fit(X_train_3_opt,y_train_3_opt )\n",
        "y_pred_3_opt = knn_regressor.predict(X_test_3_opt)\n",
        "print(np.sqrt(mean_squared_error(y_test_3_opt, y_pred_3_opt)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSEDXNWMaWB1"
      },
      "source": [
        "Result: 24.76\n",
        "\n",
        "Notes: Worse performance compare to dataset1 but still a solid result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjUEs1uPh966"
      },
      "outputs": [],
      "source": [
        "def hyperparameter_function_rf(n_estimators, max_features, ):\n",
        "      \"\"\" Function for hyperparameter optimization\n",
        "      \"\"\"\n",
        "      n_estimators = n_estimators.round().astype(int)\n",
        "      max_features = max_features.round().astype(int)\n",
        "\n",
        "      rf_regressor = RandomForestRegressor(n_estimators=n_estimators,max_features=max_features,random_state=17,n_jobs=-1)\n",
        "      metric = cross_val_score(rf_regressor, X=X_train_3_opt, y=y_train_3_opt.values.ravel(), cv=5, scoring='neg_root_mean_squared_error')\n",
        "      return metric.min()\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'n_estimators': (20, 500),'max_features': (1,1) }\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "  f=hyperparameter_function_rf,\n",
        "  pbounds=pbounds,\n",
        "  random_state=17,\n",
        "  allow_duplicate_points= True\n",
        ")\n",
        "\n",
        "optimizer.maximize(\n",
        "  init_points=10,\n",
        "  n_iter=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqRLbC7Uh9uF"
      },
      "outputs": [],
      "source": [
        "rf_regressor = RandomForestRegressor(n_estimators=333,max_features=1,random_state=17)\n",
        "rf_regressor.fit(X_train_3_opt,y_train_3_opt.values.ravel())\n",
        "y_pred_3_opt = rf_regressor.predict(X_test_3_opt)\n",
        "print(np.sqrt(mean_squared_error(y_test_3_opt, y_pred_3_opt)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYzeyeiman2w"
      },
      "source": [
        "Result: 23.36\n",
        "\n",
        "Notes: Differing from the from first the max_features parameter will no longer be optimized for.\n",
        "Strong performance. The optimization improved the results by quite a lot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTjhHvy6h-Zd"
      },
      "outputs": [],
      "source": [
        "def hyperparameter_function_lasso(alpha, max_iter, ):\n",
        "      \"\"\" Function for hyperparameter optimization\n",
        "      \"\"\"\n",
        "      max_iter = max_iter.round().astype(int)\n",
        "\n",
        "      lasso_regressor = Lasso(alpha=alpha,max_iter=max_iter,random_state=17)\n",
        "      metric = cross_val_score(lasso_regressor, X=X_train_3_opt, y=y_train_3_opt.values.ravel(), cv=5, scoring='neg_root_mean_squared_error')\n",
        "      return metric.min()\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'alpha': (0.001, 1),'max_iter': (100,10000) }\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "  f=hyperparameter_function_lasso,\n",
        "  pbounds=pbounds,\n",
        "  random_state=17,\n",
        "  allow_duplicate_points= True\n",
        ")\n",
        "\n",
        "optimizer.maximize(\n",
        "  init_points=10,\n",
        "  n_iter=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QuebvlXh-O9"
      },
      "outputs": [],
      "source": [
        "lasso_regressor = Lasso(alpha=0.1923,max_iter=772,random_state=17)\n",
        "lasso_regressor.fit(X_train_3_opt,y_train_3_opt )\n",
        "y_pred_3_opt = lasso_regressor.predict(X_test_3_opt)\n",
        "print(np.sqrt(mean_squared_error(y_test_3_opt, y_pred_3_opt)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UupWDyGayG0"
      },
      "source": [
        "Result: 8080.99\n",
        "\n",
        "Notes: Similar to the previous optimizations. Lasso disappoints again despite almost an hour in optimization time in colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMIMMc5Jh-3t"
      },
      "outputs": [],
      "source": [
        "def hyperparameter_function_xgboost(eta, gamma ,max_depth,reg_lambda,reg_alpha ):\n",
        "      \"\"\" Function for hyperparameter optimization\n",
        "      \"\"\"\n",
        "      max_depth = max_depth.round().astype(int)\n",
        "\n",
        "      xgb_regressor = XGBRegressor(eta=eta,gamma=gamma,max_depth=max_depth,reg_lambda=reg_lambda, reg_alpha=reg_alpha)\n",
        "      metric = cross_val_score(xgb_regressor, X=X_train_3_opt, y=y_train_3_opt.values.ravel(), cv=5, scoring='neg_root_mean_squared_error')\n",
        "      return metric.min()\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'eta': (0, 1),'gamma': (0,2),'max_depth':(1,10),'reg_lambda':(1,1),'reg_alpha':(0,0) }\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "  f=hyperparameter_function_xgboost,\n",
        "  pbounds=pbounds,\n",
        "  random_state=17,\n",
        "  allow_duplicate_points= True\n",
        ")\n",
        "\n",
        "optimizer.maximize(\n",
        "  init_points=10,\n",
        "  n_iter=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pciUjD15h-vP"
      },
      "outputs": [],
      "source": [
        "xgb_regressor = XGBRegressor(eta=0.2079,gamma=1.342,max_depth=2,reg_lambda=1, reg_alpha=0)\n",
        "xgb_regressor.fit(X_train_3_opt,y_train_3_opt )\n",
        "y_pred_3_opt = xgb_regressor.predict(X_test_3_opt)\n",
        "\n",
        "print(np.sqrt(mean_squared_error(y_test_3_opt, y_pred_3_opt)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OzHJtzVa7rz"
      },
      "source": [
        "Result: 21.48\n",
        "\n",
        "Notes: This time the XGBoostRegressors performs the best over the custom feature_list. Which is inline with the promised results. (The custom feature_list is only used by KNeighbors, RandomForest, Lasso and XGBoost)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpSWIsTt7s-H"
      },
      "outputs": [],
      "source": [
        "#Bayesian Optimzier für Dataset 3\n",
        "\n",
        "def evaluate_model(n_estimators, max_depth, min_samples_split, min_samples_leaf):\n",
        "    n_estimators = int(n_estimators)\n",
        "    max_depth = int(max_depth)\n",
        "    min_samples_split = int(min_samples_split)\n",
        "    min_samples_leaf = int(min_samples_leaf)\n",
        "\n",
        "    # Define the model with the parameters\n",
        "    model = ExtraTreesRegressor(n_estimators=n_estimators, max_depth=max_depth,\n",
        "                                min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n",
        "                                random_state=42, n_jobs=-1)\n",
        "\n",
        "    # Fit and predict\n",
        "    model.fit(X_train_3_opt, y_train_3_opt)\n",
        "    pred = model.predict(X_test_3_opt)\n",
        "\n",
        "    # Calculate RMSE\n",
        "    rmse = mean_squared_error(y_test_3_opt, pred, squared=False)\n",
        "\n",
        "    # We want to minimize RMSE, so we return the negative value\n",
        "    return -rmse\n",
        "\n",
        "\n",
        "# Define the bounds of the parameters\n",
        "param_bounds = {\n",
        "    'n_estimators': (100, 200),\n",
        "    'max_depth': (10, 30),\n",
        "    'min_samples_split': (2, 10),\n",
        "    'min_samples_leaf': (1, 4)\n",
        "}\n",
        "# Create the BayesianOptimization object\n",
        "optimizer = BayesianOptimization(\n",
        "    f=evaluate_model,\n",
        "    pbounds=param_bounds,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "# Perform the optimization\n",
        "optimizer.maximize(init_points=5, n_iter=12)\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Beste Hyperparameter-Kombination:\", optimizer.max['params'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0WPolzcv6uw"
      },
      "outputs": [],
      "source": [
        "extra_trees = ExtraTreesRegressor(n_estimators=168, max_depth=30, min_samples_split=2, min_samples_leaf=1, random_state=42)\n",
        "extra_trees.fit(X_train_3_opt, y_train_3_opt)\n",
        "test_predictions = extra_trees.predict(X_test_3_opt)\n",
        "test_rmse = np.sqrt(sklearn.metrics.root_mean_squared_error(y_test_3_opt, test_predictions))\n",
        "print(f\"Test RMSE: {test_rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfXErIg_dbJ8"
      },
      "source": [
        "# Dataset 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J03_3L1kQ1wf"
      },
      "outputs": [],
      "source": [
        "## Load fourth dataset\n",
        "train_data_4_opt, test_data_4_opt,test_rul_data_4_opt = load_data(config_path=PATH_TO_CONFIG, dataset_num=4)\n",
        "cleaned_train_4_opt, cleaned_test_4_opt = clean_data(train_data_4_opt, test_data_4_opt, method=None, ignore_columns=['UnitNumber', 'Cycle','Operation Setting 2','Operation Setting 3','Sensor Measure 2','Sensor Measure 3','Sensor Measure 4','Sensor Measure 8','Sensor Measure 9','Sensor Measure 11','Sensor Measure 15','Sensor Measure 17'], threshold_missing=0.1, threshold_corr=0.5)\n",
        "\n",
        "# Train Val Split\n",
        "cl_train_4_opt, cl_val_4_opt = train_val_split_by_group(df = cleaned_train_4_opt,group = \"UnitNumber\",test_size = 0.18,n_splits = 2,random_state = 7)\n",
        "\n",
        "## RollingWindowParameter\n",
        "min_ts_4_opt = 17\n",
        "max_ts_4_opt = 18\n",
        "feature_list_ds_4 = [\"c3\", \"quantile\", \"mean\", \"median\", \"root_mean_square\", \"variance\", \"mean_abs_change\", \"standard_deviation\", \"skewness\", \"variation_coefficient\", \"last_location_of_maximum\", \"first_location_of_maximum\"]\n",
        "\n",
        "#rwCreator = RollingWindowDatasetCreator(max_timeshift=max_ts_4_opt,min_timeshift=min_ts_4_opt,feature_extraction_mode= 'custom',feature_list=feature_list_ds_4)\n",
        "rwCreator = RollingWindowDatasetCreator(max_timeshift=max_ts_4_opt,min_timeshift=min_ts_4_opt,feature_extraction_mode= 'minimal',feature_list=[\"median\"])\n",
        "\n",
        "\n",
        "X_train_4_opt, y_train_4_opt = rwCreator._process_data(cl_train_4_opt, 'train')\n",
        "X_val_4_opt, y_val_4_opt = rwCreator._process_data(cl_val_4_opt, 'train')\n",
        "X_test_4_opt, y_test_4_opt = rwCreator._process_data(cleaned_test_4_opt, 'test', test_rul_data_4_opt)\n",
        "\n",
        "y_train_4_opt = y_train_4_opt.clip(upper=125)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_4_opt[2:] = scaler.fit_transform(X_train_4_opt[2:])\n",
        "X_val_4_opt[2:] = scaler.fit_transform(X_val_4_opt[2:])\n",
        "X_test_4_opt[2:] = scaler.fit_transform(X_test_4_opt[2:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0H5NnGMbSEg"
      },
      "source": [
        "Notes: This is by far the worst performing dataset in regards to the achieved RMSE by our models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CceTN3NbmSV"
      },
      "outputs": [],
      "source": [
        "def hyperparameter_function_knn(neighbours):\n",
        "      \"\"\" Function for hyperparameter optimization\n",
        "      \"\"\"\n",
        "      neighbours = neighbours.round().astype(int)\n",
        "      knn_regressor = KNeighborsRegressor(n_neighbors=neighbours)\n",
        "      metric = cross_val_score(knn_regressor, X=X_train_4_opt, y=y_train_4_opt, cv=5, scoring='neg_root_mean_squared_error')\n",
        "      return metric.min()\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'neighbours': (1, 750)}\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "  f=hyperparameter_function_knn,\n",
        "  pbounds=pbounds,\n",
        "  random_state=17,\n",
        "  allow_duplicate_points= True\n",
        ")\n",
        "\n",
        "optimizer.maximize(\n",
        "  init_points=10,\n",
        "  n_iter=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4vQbXvYjwmL"
      },
      "outputs": [],
      "source": [
        "# Neigbors =\n",
        "knn_regressor = KNeighborsRegressor(n_neighbors=15)\n",
        "knn_regressor.fit(X_train_4_opt,y_train_4_opt )\n",
        "y_pred_4_opt = knn_regressor.predict(X_test_4_opt)\n",
        "print(np.sqrt(mean_squared_error(y_test_4_opt, y_pred_4_opt)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKXNwqWabMsS"
      },
      "source": [
        "Result: 42.10\n",
        "\n",
        "Notes: For this dataset KNeighbors is squarely in the middlefield."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXq2xg14jx3h"
      },
      "outputs": [],
      "source": [
        "def hyperparameter_function_rf(n_estimators, max_features, ):\n",
        "      \"\"\" Function for hyperparameter optimization\n",
        "      \"\"\"\n",
        "      n_estimators = n_estimators.round().astype(int)\n",
        "      max_features = max_features.round().astype(int)\n",
        "\n",
        "      rf_regressor = RandomForestRegressor(n_estimators=n_estimators,max_features=max_features,random_state=17,n_jobs=-1)\n",
        "      metric = cross_val_score(rf_regressor, X=X_train_4_opt, y=y_train_4_opt.values.ravel(), cv=5, scoring='neg_root_mean_squared_error')\n",
        "      return metric.min()\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'n_estimators': (20, 500),'max_features': (1,1) }\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "  f=hyperparameter_function_rf,\n",
        "  pbounds=pbounds,\n",
        "  random_state=17,\n",
        "  allow_duplicate_points= True\n",
        ")\n",
        "\n",
        "optimizer.maximize(\n",
        "  init_points=10,\n",
        "  n_iter=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_x_TArL7jxwD"
      },
      "outputs": [],
      "source": [
        "rf_regressor = RandomForestRegressor(n_estimators=264,max_features=1)\n",
        "rf_regressor.fit(X_train_4_opt,y_train_4_opt.values.ravel())\n",
        "y_pred_4_opt = rf_regressor.predict(X_test_4_opt)\n",
        "print(np.sqrt(mean_squared_error(y_test_4_opt, y_pred_4_opt)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGK4kdthbcIK"
      },
      "source": [
        "Result: 39.53\n",
        "\n",
        "Notes: The best model for dataset 4 and the only one to breach below 40. Despite extensive optimization it wasn't possible to further reduce the RMSE with the RandomForest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKaR3T4Vjxn4"
      },
      "outputs": [],
      "source": [
        "def hyperparameter_function_lasso(alpha, max_iter, ):\n",
        "      \"\"\" Function for hyperparameter optimization\n",
        "      \"\"\"\n",
        "      max_iter = max_iter.round().astype(int)\n",
        "\n",
        "      lasso_regressor = Lasso(alpha=alpha,max_iter=max_iter,random_state=17)\n",
        "      metric = cross_val_score(lasso_regressor, X=X_train_4_opt, y=y_train_4_opt.values.ravel(), cv=5, scoring='neg_root_mean_squared_error')\n",
        "      return metric.min()\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'alpha': (0.001, 1),'max_iter': (100,10000) }\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "  f=hyperparameter_function_lasso,\n",
        "  pbounds=pbounds,\n",
        "  random_state=17,\n",
        "  allow_duplicate_points= True\n",
        ")\n",
        "\n",
        "optimizer.maximize(\n",
        "  init_points=10,\n",
        "  n_iter=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "0xWa442UjxfK",
        "outputId": "f209bad4-e849-4cdd-e799-0d232d1078a4"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Lasso' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a1404f2be5b7>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# alpha= , max_iter= ->\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlasso_regressor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLasso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9457\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m694\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlasso_regressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_4_opt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_4_opt\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_pred_4_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlasso_regressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_4_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_4_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_4_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Lasso' is not defined"
          ]
        }
      ],
      "source": [
        "lasso_regressor = Lasso(alpha=0.9457,max_iter=694)\n",
        "lasso_regressor.fit(X_train_4_opt,y_train_4_opt )\n",
        "y_pred_4_opt = lasso_regressor.predict(X_test_4_opt)\n",
        "print(np.sqrt(mean_squared_error(y_test_4_opt, y_pred_4_opt)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM8WK0Dpb4U0"
      },
      "source": [
        "Result: 50.39\n",
        "\n",
        "Notes: The best overall result for the Lasso Regressor despite the most difficult dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsGT8w9KjxRL"
      },
      "outputs": [],
      "source": [
        "def hyperparameter_function_xgboost(eta, gamma ,max_depth,reg_lambda,reg_alpha ):\n",
        "      \"\"\" Function for hyperparameter optimization\n",
        "      \"\"\"\n",
        "      max_depth = max_depth.round().astype(int)\n",
        "\n",
        "      xgb_regressor = XGBRegressor(eta=eta,gamma=gamma,max_depth=max_depth,reg_lambda=reg_lambda, reg_alpha=reg_alpha)\n",
        "      metric = cross_val_score(xgb_regressor, X=X_train_4_opt, y=y_train_4_opt.values.ravel(), cv=5, scoring='neg_root_mean_squared_error')\n",
        "      return metric.min()\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'eta': (0, 1),'gamma': (0,2),'max_depth':(1,10),'reg_lambda':(1,1),'reg_alpha':(0,0) }\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "  f=hyperparameter_function_xgboost,\n",
        "  pbounds=pbounds,\n",
        "  random_state=17,\n",
        "  allow_duplicate_points= True\n",
        ")\n",
        "\n",
        "optimizer.maximize(\n",
        "  init_points=10,\n",
        "  n_iter=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1bc5CwnjxDF"
      },
      "outputs": [],
      "source": [
        "# eta= , gamma= , max_depth= ,lambda=1 , alpha=0 ->\n",
        "xgb_regressor = XGBRegressor(eta=0.1149,gamma=0.4352,max_depth=4,reg_lambda=1, reg_alpha=0)\n",
        "xgb_regressor.fit(X_train_4_opt,y_train_4_opt )\n",
        "y_pred_4_opt = xgb_regressor.predict(X_test_4_opt)\n",
        "print(np.sqrt(mean_squared_error(y_test_4_opt, y_pred_4_opt)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqyEDpgbcE5l"
      },
      "source": [
        "Result: 43.26\n",
        "\n",
        "Notes: Solid result but really didn't outperform any other regressor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMe36ysHPzfr"
      },
      "source": [
        "# Findings\n",
        "\n",
        "Certain regressors are heavily hyperparameter dependent, especially lasso.\n",
        "The feature selection was a mixed bag. While some regressors seemed to profit others less so. A more extensive evaluation could be performed in the future.\n",
        "The datasets 2 & 4 were as promised by the EDA much more dificult to get a lower rmse. While datasets 1 & 3 we were able to reduce the rmse to the low twenties. This was not directly the case for 2 and 4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEQWL6rTP1SL"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wab3ajvQcWE8"
      },
      "source": [
        "ExtraTrees and RandomForest were overall our best performers. Although we had some problems with certain datasets and some of our results seem to good to be true we couldnt find any fault in our method.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z1-qtd6dIIt"
      },
      "source": [
        "Final Notes: The original notebooks are in the referenced Repository in case any of our merged results do not reflect the communicated results, they should be used to compare.\n",
        "\n",
        "Part of this was done in colab. The part can be found under the following link:\n",
        "https://colab.research.google.com/drive/1F_hpmXcxYoJT3LsvXjF65c3_lZ7ltEr6?usp=sharing"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "5KGGq6krfQcv",
        "6b78kYcudRq6",
        "PD53NKy9dZyC"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
