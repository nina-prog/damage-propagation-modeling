{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Topic: EX2 - Turbofan RUL Prediction with Transformer\n",
    "**Task**: Predict the remaining useful life (RUL) of turbofan engines based on given sensor data (time series data). It is a regression problem.\n",
    "**Data**: Turbofan engine degradation simulation data (NASA) - [Link](https://data.nasa.gov/dataset/Turbofan-Engine-Degradation-Simulation-Data-Set/vrks-gjie). See also in the topic [introduction notebook](https://github.com/nina-prog/damage-propagation-modeling/blob/2fb8c1a1102a48d7abbf04e4031807790a913a99/notebooks/Turbofan%20remaining%20useful%20life%20Prediction.ipynb).\n",
    "\n",
    "**Subtasks**:\n",
    "4. Create **neural network models** to predict the remaining useful life. Includes different architectures like Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), or Attention Models. Note: You can search for SOTA research papers and reproduce current state-of-the-art models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Description\n",
    "Next to the previous CNN Architecture, we also had a look at Transformer Models.\n",
    "The Transformer Model consists of a projection layer, an Transformer Encoder Block and a output layer at the end. <br>\n",
    "The Transformer Models in general are bigger than the CNN Models with 1 to 10 million parameters depending on the projection size and window length, because at the end the (projection size x window_size) is flattened and inputed into the output layer.\n",
    "\n",
    "### Results\n",
    "The Transformer Model showed impressive results on the first and third dataset with an RMSE of 18.6 and 21.8. The models are stored in the models folder in pt format. However our Transformer Model was not able to find a fit for dataset 2 and 4. Having in mind the good results of the CNN approach, we did not further investigate the Transformer Model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Imports + Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: colorlog in /opt/conda/lib/python3.11/site-packages (6.8.2)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: ray in /opt/conda/lib/python3.11/site-packages (2.23.0)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.11/site-packages (from ray) (8.1.7)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from ray) (3.13.3)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.11/site-packages (from ray) (4.21.1)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from ray) (1.0.7)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from ray) (24.0)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/conda/lib/python3.11/site-packages (from ray) (4.25.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from ray) (6.0.1)\n",
      "Requirement already satisfied: aiosignal in /opt/conda/lib/python3.11/site-packages (from ray) (1.3.1)\n",
      "Requirement already satisfied: frozenlist in /opt/conda/lib/python3.11/site-packages (from ray) (1.4.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from ray) (2.31.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.11/site-packages (from jsonschema->ray) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.11/site-packages (from jsonschema->ray) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.11/site-packages (from jsonschema->ray) (0.34.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from jsonschema->ray) (0.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->ray) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->ray) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->ray) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->ray) (2024.2.2)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: ax-platform in /opt/conda/lib/python3.11/site-packages (0.4.0)\n",
      "Requirement already satisfied: botorch in /opt/conda/lib/python3.11/site-packages (0.11.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from ax-platform) (3.1.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from ax-platform) (2.2.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (from ax-platform) (1.13.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (from ax-platform) (1.4.1.post1)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.11/site-packages (from ax-platform) (8.1.2)\n",
      "Requirement already satisfied: plotly>=5.12.0 in /opt/conda/lib/python3.11/site-packages (from ax-platform) (5.20.0)\n",
      "Requirement already satisfied: typeguard in /opt/conda/lib/python3.11/site-packages (from ax-platform) (2.13.3)\n",
      "Requirement already satisfied: pyre-extensions in /opt/conda/lib/python3.11/site-packages (from ax-platform) (0.0.30)\n",
      "Requirement already satisfied: multipledispatch in /opt/conda/lib/python3.11/site-packages (from botorch) (1.0.0)\n",
      "Requirement already satisfied: mpmath<=1.3,>=0.19 in /opt/conda/lib/python3.11/site-packages (from botorch) (1.3.0)\n",
      "Requirement already satisfied: torch>=1.13.1 in /opt/conda/lib/python3.11/site-packages (from botorch) (2.2.2+cu121)\n",
      "Requirement already satisfied: pyro-ppl>=1.8.4 in /opt/conda/lib/python3.11/site-packages (from botorch) (1.9.0)\n",
      "Requirement already satisfied: gpytorch==1.11 in /opt/conda/lib/python3.11/site-packages (from botorch) (1.11)\n",
      "Requirement already satisfied: linear-operator==0.5.1 in /opt/conda/lib/python3.11/site-packages (from botorch) (0.5.1)\n",
      "Requirement already satisfied: jaxtyping>=0.2.9 in /opt/conda/lib/python3.11/site-packages (from linear-operator==0.5.1->botorch) (0.2.29)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.11/site-packages (from plotly>=5.12.0->ax-platform) (8.2.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from plotly>=5.12.0->ax-platform) (24.0)\n",
      "Requirement already satisfied: numpy>=1.7 in /opt/conda/lib/python3.11/site-packages (from pyro-ppl>=1.8.4->botorch) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from pyro-ppl>=1.8.4->botorch) (3.3.0)\n",
      "Requirement already satisfied: pyro-api>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from pyro-ppl>=1.8.4->botorch) (0.1.2)\n",
      "Requirement already satisfied: tqdm>=4.36 in /opt/conda/lib/python3.11/site-packages (from pyro-ppl>=1.8.4->botorch) (4.66.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.1->botorch) (3.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.1->botorch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.1->botorch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.1->botorch) (3.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.1->botorch) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.1->botorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.1->botorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.1->botorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.1->botorch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.1->botorch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.1->botorch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.1->botorch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.1->botorch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.1->botorch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.1->botorch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.1->botorch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.1->botorch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1->botorch) (12.4.127)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.11/site-packages (from ipywidgets->ax-platform) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.11/site-packages (from ipywidgets->ax-platform) (8.22.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.11/site-packages (from ipywidgets->ax-platform) (5.14.2)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in /opt/conda/lib/python3.11/site-packages (from ipywidgets->ax-platform) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in /opt/conda/lib/python3.11/site-packages (from ipywidgets->ax-platform) (3.0.10)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->ax-platform) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->ax-platform) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->ax-platform) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->ax-platform) (2024.1)\n",
      "Requirement already satisfied: typing-inspect in /opt/conda/lib/python3.11/site-packages (from pyre-extensions->ax-platform) (0.9.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->ax-platform) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->ax-platform) (3.4.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->ax-platform) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->ax-platform) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->ax-platform) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->ax-platform) (3.0.42)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->ax-platform) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->ax-platform) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->ax-platform) (4.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->ax-platform) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from typing-inspect->pyre-extensions->ax-platform) (1.0.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets->ax-platform) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets->ax-platform) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets->ax-platform) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets->ax-platform) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets->ax-platform) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets->ax-platform) (0.2.2)\n"
     ]
    }
   ],
   "source": [
    "## download important libraries\n",
    "!pip install colorlog\n",
    "!pip install ray\n",
    "!pip install ax-platform botorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# third-party libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import List, Union\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from scipy import stats\n",
    "from scipy.stats import multivariate_normal, zscore\n",
    "from scipy.stats._mstats_basic import winsorize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T20:33:54.874777700Z",
     "start_time": "2024-05-20T20:33:54.679873200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# source code\n",
    "os.chdir(\"../\") # set working directory to root of project\n",
    "\n",
    "# import own libraries\n",
    "from src.utils import load_data, load_config, train_val_split_by_group\n",
    "from src.rolling_window_creator import RollingWindowDatasetCreator, calculate_RUL\n",
    "from src.data_cleaning import identify_missing_values, identify_single_unique_features, format_dtype, clean_data\n",
    "import src.nn_utils as nu\n",
    "import src.transformer_fred as tff\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Load config + Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T19:22:00.302961200Z",
     "start_time": "2024-05-20T19:22:00.174245100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "PATH_TO_CONFIG = \"configs/config.yaml\"\n",
    "config = load_config(PATH_TO_CONFIG) # config is dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T21:30:59.785347900Z",
     "start_time": "2024-05-20T21:30:59.336554Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-31 15:24:56 [\u001b[34msrc.utils:60\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loading data set 1...\u001b[0m\n",
      "2024-05-31 15:24:56 [\u001b[34msrc.utils:89\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loaded raw data for dataset 1.\u001b[0m\n",
      "2024-05-31 15:24:56 [\u001b[34msrc.utils:90\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Train Data: (20631, 26)\u001b[0m\n",
      "2024-05-31 15:24:56 [\u001b[34msrc.utils:91\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test Data: (13096, 26)\u001b[0m\n",
      "2024-05-31 15:24:56 [\u001b[34msrc.utils:92\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test RUL Data: (100, 1)\u001b[0m\n",
      "CPU times: user 57.7 ms, sys: 18.8 ms, total: 76.4 ms\n",
      "Wall time: 76.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## define data set you want to use in dataset_num\n",
    "dataset_num = 1\n",
    "train_data, test_data, test_RUL_data = load_data(config_path=PATH_TO_CONFIG, dataset_num=dataset_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T21:27:47.244817900Z",
     "start_time": "2024-05-20T21:27:47.163989800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique unit numbers in test set: 100\n",
      "Min number of cycles in test set for a unit number:  31\n"
     ]
    }
   ],
   "source": [
    "## overview over test dataset\n",
    "# count unit numbers in test set\n",
    "print(f\"Number of unique unit numbers in test set: {test_data['UnitNumber'].nunique()}\")\n",
    "# count min number of cycles in test set for each unit number --> window size must be in the range of these values, for example a window size of 10 would be too large if there is a unit number with only 10 cycles\n",
    "print(\"Min number of cycles in test set for a unit number: \", test_data.groupby(\"UnitNumber\")[\"Cycle\"].count().min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "---\n",
    "Test Data Cleaning Functionality and its impact on Rolling Window Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T21:30:34.171343100Z",
     "start_time": "2024-05-20T21:30:33.935594200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-31 15:24:56 [\u001b[34msrc.data_cleaning:134\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Cleaning train and test data...\u001b[0m\n",
      "2024-05-31 15:24:56 [\u001b[34msrc.data_cleaning:136\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Formatting column types...\u001b[0m\n",
      "2024-05-31 15:24:56 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
      "2024-05-31 15:24:56 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
      "2024-05-31 15:24:56 [\u001b[34msrc.data_cleaning:141\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Handling duplicates...\u001b[0m\n",
      "2024-05-31 15:24:56 [\u001b[34msrc.data_cleaning:146\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Removing outliers...\u001b[0m\n",
      "2024-05-31 15:24:56 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: winsorize ...\u001b[0m\n",
      "2024-05-31 15:24:56 [\u001b[34msrc.outlier_detection:98\u001b[0m] [DEBUG\u001b[0m] >>>> Found 1031 outliers to be replaced (winsorized).\u001b[0m\n",
      "2024-05-31 15:24:56 [\u001b[34msrc.outlier_detection:100\u001b[0m] [DEBUG\u001b[0m] >>>> Original DataFrame shape: (20631, 26), Resulting DataFrame shape: (20631, 26)\u001b[0m\n",
      "2024-05-31 15:24:56 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: winsorize ...\u001b[0m\n",
      "2024-05-31 15:24:56 [\u001b[34msrc.outlier_detection:98\u001b[0m] [DEBUG\u001b[0m] >>>> Found 654 outliers to be replaced (winsorized).\u001b[0m\n",
      "2024-05-31 15:24:56 [\u001b[34msrc.outlier_detection:100\u001b[0m] [DEBUG\u001b[0m] >>>> Original DataFrame shape: (13096, 26), Resulting DataFrame shape: (13096, 26)\u001b[0m\n",
      "2024-05-31 15:24:56 [\u001b[34msrc.data_cleaning:150\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Filter features based train data...\u001b[0m\n",
      "2024-05-31 15:24:56 [\u001b[34msrc.data_cleaning:26\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 features with missing values above the threshold of 0.1.\u001b[0m\n",
      "2024-05-31 15:24:56 [\u001b[34msrc.data_cleaning:46\u001b[0m] [DEBUG\u001b[0m] >>>> Found 8 features with only a single unique value: ['Operation Setting 3', 'Sensor Measure 1', 'Sensor Measure 5', 'Sensor Measure 6', 'Sensor Measure 10', 'Sensor Measure 16', 'Sensor Measure 18', 'Sensor Measure 19']\u001b[0m\n",
      "2024-05-31 15:24:56 [\u001b[34msrc.data_cleaning:103\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 uncorrelated features with a correlation threshold of 0.0: []\u001b[0m\n",
      "2024-05-31 15:24:56 [\u001b[34msrc.data_cleaning:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Dropping features based on missing values, single unique values, and no target correlation...\u001b[0m\n",
      "2024-05-31 15:24:56 [\u001b[34msrc.data_cleaning:172\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Data cleaning completed.\u001b[0m\n",
      "2024-05-31 15:24:56 [\u001b[34msrc.data_cleaning:173\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original train DataFrame shape: (20631, 26), Resulting train DataFrame shape: (20631, 18)\u001b[0m\n",
      "2024-05-31 15:24:56 [\u001b[34msrc.data_cleaning:174\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original test DataFrame shape: (13096, 26), Resulting test DataFrame shape: (13096, 18)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "## Finding: The Transformer Model works better, if it contains all variable.\n",
    "##          Even those that have no clear correlation to RUL.\n",
    "##          The Transformer Model can still use the contextual information given in complex time series\n",
    "\n",
    "# clean data (with outlier removal, where no samples are dropped but the outliers are replaced, method='winsorize')\n",
    "cleaned_train, cleaned_test = clean_data(train_data, test_data, method='winsorize', ignore_columns=['UnitNumber', 'Cycle'], threshold_missing=0.1, threshold_corr=0.0, contamination=0.05)\n",
    "\n",
    "# calculate RUL for test dataset\n",
    "cleaned_train_data = calculate_RUL(cleaned_train, time_column= \"Cycle\", group_column= \"UnitNumber\")\n",
    "cleaned_test_data = nu.calculate_RUL_test(cleaned_test, test_RUL_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min group size: 31\n",
      "Max group size: 303\n",
      "Mean group size: 130.96\n",
      "Sd group size: 53.593479175185195\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "## Finding: The minimun window length in the test datasets are\n",
    "##          significantly smaller than in the train datasets\n",
    "##          --> one explanation for sometimes poorer performance on test dataset\n",
    "group_sizes = test_data.groupby('UnitNumber').size()\n",
    "\n",
    "# Calculate min, max, and mean of the group sizes\n",
    "min_size = group_sizes.min()\n",
    "max_size = group_sizes.max()\n",
    "mean_size = group_sizes.mean()\n",
    "sd_size = group_sizes.std()\n",
    "\n",
    "print(f\"Min group size: {min_size}\")\n",
    "print(f\"Max group size: {max_size}\")\n",
    "print(f\"Mean group size: {mean_size}\")\n",
    "print(f\"Sd group size: {sd_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameter search with Ray Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyper parameter search was done with Ray Tune on the cluster.\n",
    "However we were not fully satisfied by the library and many tests and finetuning was also\n",
    "done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "## Finding: The window length is a import hyperparameter.\n",
    "##          The Transformer Model needs bigger window size than the CNN models.\n",
    "##          A good window size is around 80 instead of 30.\n",
    "##          We have the feeling that an even bigger window size is better (140+) but it is limited \n",
    "##          due to the test dataset and its limited length.\n",
    "##          Another aspect is the model size: only one layer is suitable due to size issues and also the \n",
    "##          combination of window size * project dim should not be too big\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from ray import tune, train\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune import CLIReporter\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Import your necessary functions and modules (assuming nu module and TransformerModel are defined)\n",
    "# from your_module import TransformerModel, TurbofanDataset, scale_data, create_sliding_window, train_val_split_by_group, count_parameters\n",
    "\n",
    "# Define training function\n",
    "def train_model(config, checkpoint_dir=None):\n",
    "    window_size = config[\"window_size\"]\n",
    "    project_dim = config[\"project_dim\"]\n",
    "    num_heads = config[\"num_heads\"]\n",
    "    \n",
    "    # Assume your data preparation functions are defined and work as shown in your example\n",
    "    train_data, val_data = nu.scale_data(cleaned_train_data, cleaned_test_data)\n",
    "    X_train, y_train = nu.create_sliding_window(train_data, window_size=window_size)\n",
    "    X_val, y_val = nu.create_sliding_window(val_data, window_size=window_size, typ = \"test\")\n",
    "    y_train = np.clip(y_train, a_min=None, a_max=130)\n",
    "    y_train, X_train = nu.cut_high_RUL(y_train, X_train, 140, delete = 0.3)\n",
    "    \n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = tff.TurbofanDataset(X_train, y_train)\n",
    "    val_dataset = tff.TurbofanDataset(X_val, y_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = tff.TransformerModel(feature_size=X_train.shape[2], num_heads=num_heads, num_layers=config[\"num_layers\"], project_dim=project_dim, window_size=window_size, dropout = config[\"dropout\"]).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    scheduler = StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "    \n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            targets = targets.view(-1, 1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                targets = targets.view(-1, 1)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                ## preprocessing\n",
    "                outputs = torch.round(outputs)\n",
    "                # Set minimal value to 1\n",
    "                min_value = 1\n",
    "                outputs = torch.where(outputs < min_value, torch.tensor(min_value), outputs)\n",
    "\n",
    "                loss = criterion(outputs, targets)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "        val_loss = running_loss / len(val_loader.dataset)\n",
    "        \n",
    "        train.report({\"val_loss\":val_loss, \"train_loss\":train_loss})\n",
    "        scheduler.step()\n",
    "\n",
    "# Define search space and Ray Tune configuration\n",
    "search_space = {\n",
    "    \"window_size\": tune.choice([50, 60, 70, 80, 90, 100]),\n",
    "    \"project_dim\": tune.choice([16*3*2, 16*3*3]),\n",
    "    \"num_heads\": tune.choice([8, 12]),\n",
    "    \"num_layers\": 1,\n",
    "    \"batch_size\": 128,\n",
    "    \"dropout\": tune.choice([0.12, 0.18]),\n",
    "    \"num_epochs\": tune.choice([5, 8, 11, 13, 17, 19, 22])  # Reduced for quicker tuning\n",
    "}\n",
    "\n",
    "# Use ASHAScheduler for efficient hyperparameter search\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    max_t=25,\n",
    "    grace_period=5,\n",
    "    reduction_factor=2\n",
    ")\n",
    "\n",
    "# Configure the reporter\n",
    "reporter = CLIReporter(\n",
    "    metric_columns=[\"val_loss\", \"train_loss\", \"training_iteration\"]\n",
    ")\n",
    "\n",
    "# Run hyperparameter search\n",
    "result = tune.run(\n",
    "    train_model,\n",
    "    resources_per_trial={\"cpu\": 1, \"gpu\": 1},\n",
    "    config=search_space,\n",
    "    num_samples=50,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter\n",
    ")\n",
    "\n",
    "# Get the best trial\n",
    "best_trial = result.get_best_trial(\"val_loss\", \"min\", \"last\")\n",
    "print(\"Best trial config: {}\".format(best_trial.config))\n",
    "print(\"Best trial final validation loss: {}\".format(best_trial.last_result[\"val_loss\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the best model again from scratch and save the best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "The model has in total 2850721 parameters!!\n",
      "new best RMSE: 929.6300048828125\n",
      "Epoch 1/100, Train_L: 2281.26, Test_L: 929.63, Test_RMSE: 30.49 \n",
      "Epoch 2/100, Train_L: 496.54, Test_L: 1497.66, Test_RMSE: 38.70 \n",
      "Epoch 3/100, Train_L: 222.20, Test_L: 1635.33, Test_RMSE: 40.44 \n",
      "Epoch 4/100, Train_L: 167.65, Test_L: 1492.91, Test_RMSE: 38.64 \n",
      "Epoch 5/100, Train_L: 137.30, Test_L: 1508.10, Test_RMSE: 38.83 \n",
      "Epoch 6/100, Train_L: 120.33, Test_L: 1075.27, Test_RMSE: 32.79 \n",
      "new best RMSE: 819.8800048828125\n",
      "Epoch 7/100, Train_L: 110.36, Test_L: 819.88, Test_RMSE: 28.63 \n",
      "Epoch 8/100, Train_L: 99.14, Test_L: 1149.84, Test_RMSE: 33.91 \n",
      "new best RMSE: 696.4199829101562\n",
      "Epoch 9/100, Train_L: 89.77, Test_L: 696.42, Test_RMSE: 26.39 \n",
      "Epoch 10/100, Train_L: 82.87, Test_L: 774.73, Test_RMSE: 27.83 \n",
      "Epoch 11/100, Train_L: 81.13, Test_L: 714.82, Test_RMSE: 26.74 \n",
      "new best RMSE: 488.6600036621094\n",
      "Epoch 12/100, Train_L: 76.54, Test_L: 488.66, Test_RMSE: 22.11 \n",
      "Epoch 13/100, Train_L: 76.19, Test_L: 555.40, Test_RMSE: 23.57 \n",
      "new best RMSE: 477.72998046875\n",
      "Epoch 14/100, Train_L: 71.82, Test_L: 477.73, Test_RMSE: 21.86 \n",
      "new best RMSE: 399.0899963378906\n",
      "Epoch 15/100, Train_L: 67.71, Test_L: 399.09, Test_RMSE: 19.98 \n",
      "Epoch 16/100, Train_L: 66.81, Test_L: 563.33, Test_RMSE: 23.73 \n",
      "new best RMSE: 394.8199768066406\n",
      "Epoch 17/100, Train_L: 65.99, Test_L: 394.82, Test_RMSE: 19.87 \n",
      "new best RMSE: 392.489990234375\n",
      "Epoch 18/100, Train_L: 62.76, Test_L: 392.49, Test_RMSE: 19.81 \n",
      "new best RMSE: 358.4599914550781\n",
      "Epoch 19/100, Train_L: 61.76, Test_L: 358.46, Test_RMSE: 18.93 \n",
      "Epoch 20/100, Train_L: 56.55, Test_L: 360.69, Test_RMSE: 18.99 \n",
      "Epoch 21/100, Train_L: 55.63, Test_L: 367.20, Test_RMSE: 19.16 \n",
      "Epoch 22/100, Train_L: 57.79, Test_L: 372.27, Test_RMSE: 19.29 \n",
      "Epoch 23/100, Train_L: 53.39, Test_L: 371.55, Test_RMSE: 19.28 \n",
      "Epoch 24/100, Train_L: 50.63, Test_L: 392.95, Test_RMSE: 19.82 \n",
      "Epoch 25/100, Train_L: 49.79, Test_L: 391.48, Test_RMSE: 19.79 \n",
      "Epoch 26/100, Train_L: 48.67, Test_L: 408.62, Test_RMSE: 20.21 \n",
      "Epoch 27/100, Train_L: 46.20, Test_L: 381.09, Test_RMSE: 19.52 \n",
      "Epoch 28/100, Train_L: 45.13, Test_L: 401.28, Test_RMSE: 20.03 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Training loop save the best model on the fly\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 50\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m#val_loss = evaluate_model(model, val_loader, criterion, device)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m tff\u001b[38;5;241m.\u001b[39mevaluate_model(model, test_loader, criterion, device)\n",
      "File \u001b[0;32m~/damage-propagation-modeling/src/transformer_fred.py:157\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m    154\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    155\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 157\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    159\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m epoch_loss\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##################################\n",
    "## Data specifics\n",
    "window_size = best_trial.config[\"window_size\"]\n",
    "train_data, test_data = nu.scale_data(cleaned_train_data, cleaned_test_data)\n",
    "\n",
    "## the validation split is done in away that one UnitNumber is either in Train or val but not in both\n",
    "#train, val = train_val_split_by_group(train_data)\n",
    "\n",
    "X_train, y_train = nu.create_sliding_window(train_data, window_size = window_size)\n",
    "\n",
    "# Set values to maximum of the 130 according to the papers because in the early stage, everything looks the same\n",
    "y_train = np.clip(y_train, a_min=None, a_max=130)\n",
    "# now we have a overrepresentation of high RUL values in the train data --> randomly delete some of them\n",
    "y_train, X_train = nu.cut_high_RUL(y_train, X_train, 130, delete = 0.3)\n",
    "\n",
    "#X_val, y_val = nu.create_sliding_window(val, window_size = window_size)\n",
    "\n",
    "#test_data = nu.scale_data(cleaned_test_data)\n",
    "X_test, y_test = nu.create_sliding_window(test_data, typ = \"test\", window_size = window_size)\n",
    "\n",
    "##################################\n",
    "## Model specifics\n",
    "seq_len, batch_size, feature_size = X_train.shape[1], best_trial.config[\"batch_size\"], X_train.shape[2]\n",
    "num_heads, num_layers, project_dim  = best_trial.config[\"num_heads\"], best_trial.config[\"num_layers\"], best_trial.config[\"project_dim\"]\n",
    "num_epochs = 40\n",
    "learning_rate = 0.0001\n",
    "\n",
    "print(seq_len)\n",
    "# Create dataset and dataloaders\n",
    "train_dataset = tff.TurbofanDataset(X_train, y_train)\n",
    "test_dataset = tff.TurbofanDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "# Initialize model, criterion, optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = tff.TransformerModel(feature_size, num_heads, num_layers, project_dim = project_dim, window_size = seq_len, dropout = best_trial.config[\"dropout\"]).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "## scheduler plays no role more in the later experiements\n",
    "scheduler = StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "\n",
    "print(f\"The model has in total {tff.count_parameters(model)} parameters!!\")\n",
    "\n",
    "prev_acc = 1200\n",
    "# Training loop save the best model on the fly\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = tff.train_model(model, train_loader, criterion, optimizer, device)\n",
    "    #val_loss = evaluate_model(model, val_loader, criterion, device)\n",
    "    test_loss = tff.evaluate_model(model, test_loader, criterion, device)\n",
    "    scheduler.step()\n",
    "\n",
    "    if prev_acc > test_loss:\n",
    "        save_path = f\"tranM_df_{dataset_num}_{int(test_loss)}.pth\"\n",
    "        print(f\"new best RMSE: {test_loss}\")\n",
    "        prev_acc = test_loss\n",
    "        #torch.save(model, save_path)\n",
    "\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train_L: {train_loss:.2f}, Test_L: {test_loss:.2f}, Test_RMSE: {np.sqrt(test_loss):.2f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "## Finding: good config for df1 and df3:\n",
    "window_size = 80\n",
    "seq_len, batch_size, feature_size = X_train.shape[1], 128, X_train.shape[2]\n",
    "num_heads, num_layers, project_dim  = 8, 1, 120\n",
    "num_epochs = 100\n",
    "learning_rate = 0.0001\n",
    "drop_out = 0.18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load best Transformer Models for DF1 and DF3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load best models for dataset 1 and 3\n",
    "PATH_DF1 = \"models/tranM_df_1_351.pth\"\n",
    "PATH_DF3= \"models/tranM_df_3_476.pth\"\n",
    "loaded_model_1 = torch.load(PATH_DF1)\n",
    "loaded_model_3 = torch.load(PATH_DF3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
