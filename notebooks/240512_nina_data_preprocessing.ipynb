{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Topic: EX2 - Turbofan RUL Prediction\n",
    "**Task**: Predict the remaining useful life (RUL) of turbofan engines based on given sensor data (time series data). It is a regression problem.\n",
    "**Data**: Turbofan engine degradation simulation data (NASA) - [Link](https://data.nasa.gov/dataset/Turbofan-Engine-Degradation-Simulation-Data-Set/vrks-gjie). See also in the topic [introduction notebook](https://github.com/nina-prog/damage-propagation-modeling/blob/2fb8c1a1102a48d7abbf04e4031807790a913a99/notebooks/Turbofan%20remaining%20useful%20life%20Prediction.ipynb).\n",
    "\n",
    "**Subtasks**:\n",
    "1. Perform a deep **exploratory data analysis (EDA)** on the given data.\n",
    "2. Implement a more efficient **sliding window method** for time series data analysis.\n",
    "3. Apply **traditional machine learning methods** (SOTA) to predict the remaining useful life. Includes data preparation, feature extraction, feature selection, model selection, and model parameter optimization. -> ðŸŽ¯ **Focus on this task** data preparation and feature selection (feature extraction part of sliding window method).\n",
    "4. Create **neural network models** to predict the remaining useful life. Includes different architectures like Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), or Attention Models. Note: You can search for SOTA research papers and reproduce current state-of-the-art models.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports + Settings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# third-party libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import List, Union\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from scipy import stats\n",
    "from scipy.stats import multivariate_normal, zscore\n",
    "from scipy.stats._mstats_basic import winsorize"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# source code\n",
    "from src.utils import load_data, load_config\n",
    "from src.rolling_window_creator import RollingWindowDatasetCreator, calculate_RUL\n",
    "from src.data_cleaning import identify_missing_values, identify_single_unique_features, format_dtype, clean_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-20T20:33:54.874777700Z",
     "start_time": "2024-05-20T20:33:54.679873200Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"Set2\")\n",
    "sns.set(rc={\"figure.dpi\":100, 'savefig.dpi':200})\n",
    "sns.set_context('notebook')"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "np.random.seed(42)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Paths"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "os.chdir(\"../\") # set working directory to root of project\n",
    "#os.getcwd() # check current working directory"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "PATH_TO_CONFIG = \"configs/config.yaml\""
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load config + Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "config = load_config(PATH_TO_CONFIG) # config is dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-20T19:22:00.302961200Z",
     "start_time": "2024-05-20T19:22:00.174245100Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "%%time\n",
    "train_data, test_data, test_RUL_data = load_data(config_path=PATH_TO_CONFIG, dataset_num=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-20T21:30:59.785347900Z",
     "start_time": "2024-05-20T21:30:59.336554Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "# count unit numbers in test set\n",
    "print(f\"Number of unique unit numbers in test set: {test_data['UnitNumber'].nunique()}\")\n",
    "# count min number of cycles in test set for each unit number --> window size must be in the range of these values, for example a window size of 10 would be too large if there is a unit number with only 10 cycles\n",
    "print(\"Min number of cycles in test set for a unit number: \", test_data.groupby(\"UnitNumber\")[\"Cycle\"].count().min())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-20T21:27:47.244817900Z",
     "start_time": "2024-05-20T21:27:47.163989800Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "Test Data Cleaning Functionality and its impact on Rolling Window Creation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "# clean data (with outlier removal, where no samples are dropped but the outliers are replaced, method='winsorize')\n",
    "# TODO: outsource settings to config file\n",
    "cleaned_train, cleaned_test = clean_data(train_data, test_data, method='winsorize', ignore_columns=['UnitNumber', 'Cycle'], threshold_missing=0.1, threshold_corr=0.3, contamination=0.05)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-20T21:30:34.171343100Z",
     "start_time": "2024-05-20T21:30:33.935594200Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "# clean data (without outlier removal/replacement)\n",
    "cleaned_train, cleaned_test = clean_data(train_data, test_data, method=None, ignore_columns=['UnitNumber', 'Cycle'], threshold_missing=0.1, threshold_corr=0.3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-20T21:31:06.779790600Z",
     "start_time": "2024-05-20T21:31:06.631807Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Rolling Window Datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "%%time\n",
    "creator = RollingWindowDatasetCreator(column_id=\"UnitNumber\",\n",
    "                                      column_sort=\"Cycle\",\n",
    "                                      max_timeshift=config[\"preprocessing\"][\"max_window_size\"],\n",
    "                                      min_timeshift=config[\"preprocessing\"][\"min_window_size\"],\n",
    "                                      feature_extraction_mode='minimal')\n",
    "X_train, y_train, X_test, y_test = creator.create_rolling_windows_datasets(train_data, test_data, test_RUL_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-20T21:18:51.456861600Z",
     "start_time": "2024-05-20T21:17:29.317407300Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Findings:\n",
    "* Set 1 (with current settings):\n",
    "    * Took ```Wall time: 54.4 s``` to create rolling windows for the training and test data **with** data cleaning.\n",
    "    * Took ```Wall time: 1min 22s``` to create rolling windows for the training and test data **without** data cleaning.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load saved preprocessed data from pickle\n",
    "#X_train = pd.read_pickle(\"data/processed/ex2_X_train_20240512-155504.pkl\")\n",
    "#y_train = pd.read_pickle(\"data/processed/ex2_y_train_20240512-155504.pkl\")\n",
    "#X_test = pd.read_pickle(\"data/processed/ex2_X_test_20240512-155504.pkl\")\n",
    "#y_test = pd.read_pickle(\"data/processed/ex2_y_test_20240512-155504.pkl\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Clean Data\n",
    "* format column types - only necessary if categorical columns are present âœ…\n",
    "* handle missing values âœ…\n",
    "* handle single unique values âœ…\n",
    "* handle duplicates âœ…\n",
    "* handle outliers âœ…\n",
    "* handle correlated features âœ…\n",
    "* feature engineering ?\n",
    "    * group features with rarely occurring realizations into buckets\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Format Column Types"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "formatted_df = format_dtype(train_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Handle Missing Values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "missing_features = identify_missing_values(train_data, threshold=0.1)\n",
    "missing_features"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# drop features with missing values\n",
    "train_data.drop(missing_features, axis=1, inplace=True)\n",
    "test_data.drop(missing_features, axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Handle Single Unique Values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "single_unique_features = identify_single_unique_features(train_data)\n",
    "single_unique_features"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# drop features with single unique values\n",
    "train_data.drop(single_unique_features, axis=1, inplace=True)\n",
    "test_data.drop(single_unique_features, axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Handle duplicates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get number of duplicates\n",
    "print(f\"Number of duplicates in train data: {train_data.duplicated().sum()}\")\n",
    "print(f\"Number of duplicates in test data: {test_data.duplicated().sum()}\")\n",
    "# drop duplicates\n",
    "train_data.drop_duplicates(inplace=True)\n",
    "test_data.drop_duplicates(inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Handle Outliers\n",
    "Possible Methods to investigate and possibly test:\n",
    "* Z-Score âœ…\n",
    "* IQR âœ…\n",
    "* Winsorization âœ…\n",
    "* Isolation Forest âŒ\n",
    "* Local Outlier Factor (LOF) âœ…\n",
    "* Elliptic Envelope âœ…"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Z-Score\n",
    "**Use Case**: Normally distributed data\n",
    "**Assumption**: Assumes that the data follows a normal distribution. It identifies outliers based on how many standard deviations away from the mean a data point is.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [
    "# visual inspection\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i, col in enumerate(train_data.drop(['UnitNumber', 'Cycle'], axis = 1).columns):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    sns.histplot(train_data[col], kde=True)\n",
    "    plt.title(col)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T21:39:29.649888700Z",
     "start_time": "2024-05-17T21:38:50.996721600Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Findings:\n",
    "* ```Sensor Measure 6``` is highly skewed -> **investigate further**(drop feature), indicating non-normal distribution\n",
    "* ```Operation Setting 2``` and ```Sensor Measure 17``` have multiple peaks and seem to only have a few unique numeric values -> **investigate further** (group into buckets, drop, etc.), indicating non-normal distribution\n",
    "* All other features exhibit approximately normal distributions, indicated by a single peak with bell-shaped curves."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "# Q-Q Plot for normally distributed data\n",
    "plt.figure(figsize=(14, 12))\n",
    "for i, col in enumerate(train_data.drop(['UnitNumber', 'Cycle'], axis = 1).columns):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    stats.probplot(train_data[col], dist=\"norm\", plot=plt, fit=True, rvalue=True)\n",
    "    plt.title(col)\n",
    "plt.suptitle('Q-Q Plots for Sensor Measures and Operational Settings',fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T21:38:50.979408400Z",
     "start_time": "2024-05-17T21:38:38.408896Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "source": [
    "def remove_outliers_zscore(df: pd.DataFrame, soft_drop: bool = False, threshold_sd: float = 0.5) -> pd.DataFrame:\n",
    "    \"\"\"Remove outliers using the Z-score method.\n",
    "\n",
    "    :param df: The input DataFrame.\n",
    "    :type df: pd.DataFrame\n",
    "    :param soft_drop: Boolean to indicate whether to softly drop outliers.\n",
    "    :type soft_drop: bool\n",
    "    :param threshold_sd: The minimum proportion of outliers in a sample (row) to consider for soft dropping.\n",
    "    :type threshold_sd: float\n",
    "\n",
    "    :return: The DataFrame with outliers removed.\n",
    "    :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "    z_scores = zscore(df)\n",
    "    threshold = 3\n",
    "\n",
    "    if soft_drop:\n",
    "        outlier_proportion = (np.abs(z_scores) > threshold).mean(axis=1)\n",
    "        num_samples_soft_dropped = outlier_proportion[outlier_proportion > threshold_sd].shape[0]\n",
    "        print(f\"Number of samples to be softly dropped: {num_samples_soft_dropped}\")\n",
    "        result_df = df[(np.abs(z_scores) < threshold).all(axis=1) | (outlier_proportion <= threshold_sd)]\n",
    "    else:\n",
    "        num_samples_dropped = df[~(np.abs(z_scores) < threshold).all(axis=1)].shape[0]\n",
    "        print(f\"Number of samples to be dropped: {num_samples_dropped}\")\n",
    "        result_df = df[(np.abs(z_scores) < threshold).all(axis=1)]\n",
    "\n",
    "    print(f\"Original DataFrame shape: {df.shape}, Resulting DataFrame shape: {result_df.shape}\")\n",
    "    return result_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T21:03:48.219515200Z",
     "start_time": "2024-05-19T21:03:48.109655100Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "source": [
    "train_data_cleaned = remove_outliers_zscore(train_data.drop(['UnitNumber', 'Cycle'], axis=1), soft_drop=True, threshold_sd=0.3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T21:16:44.854491500Z",
     "start_time": "2024-05-19T21:16:44.689698900Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### IQR\n",
    "**Use Case**: Non-normally distributed data\n",
    "**Assumption**: Suitable for data that may not follow a normal distribution. It defines outliers based on the spread of the middle 50% of the data, rather than assuming a specific distribution.\n",
    "\n",
    "* --> Use IQR for non-normally distributed data. Could be useful since depending on the datset the data might not be normally distributed. As can also be seen in some of the histograms above.\n",
    "* âœ… **Use IQR for outlier detection.**\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "source": [
    "def remove_outliers_iqr(df: pd.DataFrame, threshold_iqr: float = 1.5, threshold_sd: float = 0.3 , soft_drop: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"Remove outliers using the IQR method.\n",
    "\n",
    "    :param df: The input DataFrame.\n",
    "    :type df: pd.DataFrame\n",
    "    :param threshold: The threshold for the outlier detection method.\n",
    "    :type threshold: float\n",
    "    :param soft_drop: Boolean to indicate whether to softly drop outliers.\n",
    "    :type soft_drop: bool\n",
    "\n",
    "    :return: The DataFrame with outliers removed.\n",
    "    :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - threshold_iqr * IQR\n",
    "    upper_bound = Q3 + threshold_iqr * IQR\n",
    "\n",
    "    if soft_drop:\n",
    "        outlier_proportion = ((df < lower_bound) | (df > upper_bound)).mean(axis=1)\n",
    "        num_samples_soft_dropped = outlier_proportion[outlier_proportion > threshold_sd].shape[0]\n",
    "        print(f\"Number of samples to be softly dropped: {num_samples_soft_dropped}\")\n",
    "        result_df = df[((df < lower_bound) | (df > upper_bound)).all(axis=1) | (outlier_proportion <= threshold_sd)]\n",
    "    else:\n",
    "        num_samples_dropped = df[~((df < lower_bound) | (df > upper_bound)).all(axis=1)].shape[0]\n",
    "        print(f\"Number of samples to be dropped: {num_samples_dropped}\")\n",
    "        result_df = df[((df < lower_bound) | (df > upper_bound)).all(axis=1)]\n",
    "\n",
    "    print(f\"Original DataFrame shape: {df.shape}, Resulting DataFrame shape: {result_df.shape}\")\n",
    "    return result_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T21:15:49.142605600Z",
     "start_time": "2024-05-19T21:15:49.032674200Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "source": [
    "train_data_cleaned = remove_outliers_iqr(train_data.drop(['UnitNumber', 'Cycle'], axis=1), soft_drop=True, threshold_sd=0.3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T21:16:20.831110500Z",
     "start_time": "2024-05-19T21:16:20.594853900Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Winsorization\n",
    "**Use Case**: Non-normally distributed data, when you need to reduce the influence of outliers without removing data points.\n",
    "**Assumption**: It doesn't assume any specific distribution but instead modifies extreme values to be less influential while preserving the data's overall distribution.\n",
    "\n",
    "* --> Could be useful since it doesn't assume a specific distribution. Also it does not remove data points, which could be beneficial in some cases.\n",
    "* âœ… **Use Winsorization for outlier detection.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "source": [
    "def remove_outliers_winsorize(df: pd.DataFrame, ignore_columns: List[str] = None, contamination: float = 0.05) -> pd.DataFrame:\n",
    "    \"\"\"Remove outliers using the Winsorization method. This method replaces the extreme values with the threshold value. The threshold value is determined by the proportion of outliers in the data set.\n",
    "\n",
    "    :param df: The input DataFrame.\n",
    "    :type df: pd.DataFrame\n",
    "    :param ignore_columns: The columns to ignore when handling outliers.\n",
    "    :type ignore_columns: list\n",
    "    :param contamination: The proportion of outliers which are considered as such. Default is 0.05. If 0.05 then the upper and lower 5% of the data are considered as outliers. They are replaced by the 5th and 95th percentiles respectively.\n",
    "    :type contamination: float\n",
    "\n",
    "    :return: The DataFrame with outliers removed.\n",
    "    :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "    if ignore_columns is None:\n",
    "        ignore_columns = []\n",
    "\n",
    "    num_outliers = int(df.shape[0] * contamination)\n",
    "    print(f\"Number of outliers to be replaced: {num_outliers}\")\n",
    "    result_df = df.apply(lambda x: winsorize(x, limits=[contamination, contamination]) if x.name not in ignore_columns else x)\n",
    "    print(f\"Original DataFrame shape: {df.shape}, Resulting DataFrame shape: {result_df.shape}\")\n",
    "\n",
    "    return result_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T21:24:30.309858500Z",
     "start_time": "2024-05-19T21:24:30.150546Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "source": [
    "train_data_cleaned = remove_outliers_winsorize(train_data.drop(['UnitNumber', 'Cycle'], axis=1), contamination=0.05)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-19T21:24:33.123462700Z",
     "start_time": "2024-05-19T21:24:32.903930800Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Isolation Forest\n",
    "**Use Case**: High-dimensional data\n",
    "**Assumption**: Effective for high-dimensional data where traditional distance-based methods might struggle. It works by isolating outliers in a way that separates them from the rest of the data using a tree-based approach.\n",
    "\n",
    "* --> High-dimensional data refers to data sets for which the number of variables or dimensions $p$ is much larger than the number of observations $n$, typically $p >> n$. Source: Giraud, C. (2015). *Introduction to High-Dimensional Statistics*. CRC Press.\n",
    "* âŒ **Not** our case (Shape of X_train: (20131, 170))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Local Outlier Factor (LOF)\n",
    "**Use Case**: Effective for datasets with varying densities.\n",
    "**Assumption**: LOF is useful for datasets where the density of points varies across the dataset. It calculates the local density around each data point and identifies outliers as points with significantly lower densities compared to their neighbors.\n",
    "\n",
    "* --> LOF well-suited for our dataset, which exhibits heterogeneous density distributions (see plots).\n",
    "* âœ… **Use LOF for outlier detection.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "source": [
    "# fit LOF model\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)  # Adjust parameters as needed\n",
    "lof.fit(X_train)\n",
    "\n",
    "# calc LOF scores\n",
    "lof_scores = -lof.negative_outlier_factor_\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(lof_scores, bins=50, edgecolor='k')\n",
    "plt.xlabel('LOF Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of LOF Scores')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-20T18:57:59.565624600Z",
     "start_time": "2024-05-20T18:56:51.215611900Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Findings:\n",
    "* Presence of scores below 1 indicates regions of exceptionally high data density. --> No outliers in these regions.\n",
    "* Peak around 1.0 indicates regions of average data density, clustered around a normal density level.\n",
    "* Gradual decrease in frequency suggests lower density regions.\n",
    "* Sharp drop at 1.3 may represent areas of lower data density, where outliers may start to emerge."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "source": [
    "def remove_outliers_with_lof(df: pd.DataFrame, contamination: float = 0.05) -> pd.DataFrame:\n",
    "    \"\"\"Handle outliers using the Local Outlier Factor (LOF) method. This method calculates the local density around each data point and identifies outliers as points with significantly lower densities compared to their neighbors.\n",
    "\n",
    "    :param df: The input DataFrame.\n",
    "    :type df: pd.DataFrame\n",
    "    :param contamination: The proportion of outliers in the data set. Default is 0.05.\n",
    "    :type contamination: float\n",
    "\n",
    "    :return: The DataFrame with outliers removed.\n",
    "    :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "    # fit the LOF model\n",
    "    lof = LocalOutlierFactor(contamination=contamination, novelty=False)\n",
    "    yhat = lof.fit_predict(df)\n",
    "    lof_scores = -lof.negative_outlier_factor_\n",
    "\n",
    "    result_df = df[yhat != -1]\n",
    "    print(f\"Number of samples to be dropped: {df.shape[0] - result_df.shape[0]}\")\n",
    "    print(f\"Original DataFrame shape: {df.shape}, Resulting DataFrame shape: {result_df.shape}\")\n",
    "\n",
    "    return result_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-20T18:54:20.273725200Z",
     "start_time": "2024-05-20T18:54:20.185164800Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "source": [
    "train_data_cleaned = remove_outliers_with_lof(X_train, contamination=0.05)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-20T18:55:33.529442300Z",
     "start_time": "2024-05-20T18:54:21.865145900Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Elliptic Envelope\n",
    "**Use Case**: Assumes the data is normally distributed and identifies points that are far from the center. Suitable for multivariate normally distributed data.\n",
    "**Assumption**: Assumes the data is multivariate normally distributed and aims to identify outliers by fitting an ellipse around the central data points. Points outside this ellipse are considered outliers.\n",
    "\n",
    "* --> The data is not perfectly multivariate normal, but the method can still be used. See below in the plot.\n",
    "* âœ… **Use Elliptic Envelope for outlier detection.*\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "source": [
    "# check for multivariate normality\n",
    "# fit a multivariate normal distribution to the data\n",
    "mu = X_train.mean(axis=0)\n",
    "sigma = X_train.cov()\n",
    "mvn = multivariate_normal(mean=mu, cov=sigma, allow_singular=True)\n",
    "# calculate the PDF for the data\n",
    "X_train['mvn'] = mvn.pdf(X_train)\n",
    "# plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(X_train['mvn'], bins=50, edgecolor='k')\n",
    "plt.xlabel('Multivariate Normal PDF')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Multivariate Normal PDF')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T21:39:32.122531900Z",
     "start_time": "2024-05-17T21:39:29.649888700Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Findings:\n",
    "* Majority of data points: Most have multivariate normal PDF value close to 0. Considered typical or normal.\n",
    "* Sparse spread: Some data points spread up to 1.0. Possibly outliers or less typical observations.\n",
    "* Gap between 0.5 and 1.0: Indicates regions of lower density. Less common patterns in the data.\n",
    "--> Majority close to center, typical. Some deviate, potentially outliers. The data is not perfectly multivariate normal, but the method can still be used."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Summary of Outlier Detection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "source": [
    "def remove_outliers(df: pd.DataFrame, method: str = 'winsorize', ignore_columns: List[str] = None, contamination: float = 0.05, threshold_sd: float = 0.8, soft_drop: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"Remove outliers from the input DataFrame using the specified method.\n",
    "\n",
    "    :param df: The input DataFrame.\n",
    "    :type df: pd.DataFrame\n",
    "    :param method: The outlier detection method to use. Options: 'zscore', 'iqr', 'winsorize', 'lof', 'elliptic'.\n",
    "    :type method: str\n",
    "    :param ignore_columns: The columns to ignore when handling outliers.\n",
    "    :type ignore_columns: list\n",
    "    :param contamination: The proportion of outliers in the data set. Default is 0.05.\n",
    "    :type contamination: float\n",
    "    :param threshold_sd: The minimum proportion of outliers in a sample (row) to consider for soft dropping. Default is 0.8.\n",
    "    :type threshold_sd: float\n",
    "    :param soft_drop: Boolean to indicate whether to softly drop outliers.\n",
    "    :type soft_drop: bool\n",
    "\n",
    "    :return: The DataFrame with outliers removed.\n",
    "    :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "    if ignore_columns is None:\n",
    "        ignore_columns = []\n",
    "\n",
    "    if method == 'zscore':\n",
    "        result_df = remove_outliers_zscore(df, soft_drop=soft_drop, threshold_sd=threshold_sd)\n",
    "    elif method == 'iqr':\n",
    "        result_df = remove_outliers_iqr(df, soft_drop=soft_drop, threshold_sd=threshold_sd)\n",
    "    elif method == 'winsorize':\n",
    "        result_df = remove_outliers_winsorize(df, ignore_columns=ignore_columns, contamination=contamination)\n",
    "    elif method == 'lof':\n",
    "        result_df = remove_outliers_with_lof(df, contamination=contamination)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid method: {method}. Please choose from 'zscore', 'iqr', 'winsorize', 'lof'.\")\n",
    "\n",
    "    return result_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-20T19:19:50.945546200Z",
     "start_time": "2024-05-20T19:19:50.857765700Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "source": [
    "# remove outliers using the specified method\n",
    "train_data_cleaned = remove_outliers(X_train, method='winsorize', ignore_columns=['UnitNumber', 'Cycle'], contamination=0.05, threshold_sd=0.8, soft_drop=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-20T19:07:15.925633200Z",
     "start_time": "2024-05-20T19:07:14.336152800Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Handle Correlated Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def get_uncorrelated_features(df: pd.DataFrame, threshold: float = 0.9, target: str = None) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get features from the input DataFrame that are not highly correlated with the target column.\n",
    "\n",
    "    :param df: The input DataFrame.\n",
    "    :type df: pd.DataFrame\n",
    "    :param threshold: The correlation threshold to use. Default is 0.9.\n",
    "                      Features with an absolute correlation coefficient less than this value with the target are considered uncorrelated.\n",
    "    :type threshold: float\n",
    "    :param target: The target column name. Default is None.\n",
    "    :type target: str\n",
    "\n",
    "    :return: The list of uncorrelated feature names.\n",
    "    :rtype: List[str]\n",
    "    \"\"\"\n",
    "\n",
    "    if target and target not in df.columns:\n",
    "        raise ValueError(f\"Target column '{target}' is not in the DataFrame.\")\n",
    "\n",
    "    corr_matrix = df.corr().abs()\n",
    "\n",
    "    if target:\n",
    "        target_corr = corr_matrix[target].drop(target)\n",
    "        uncorrelated_features = target_corr[target_corr < threshold].index.tolist()\n",
    "    else:\n",
    "        uncorrelated_features = df.columns.tolist()\n",
    "\n",
    "    print(f\"Found {len(uncorrelated_features)} uncorrelated features with a correlation threshold of {threshold}.\")\n",
    "\n",
    "    return uncorrelated_features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-20T19:44:05.556838700Z",
     "start_time": "2024-05-20T19:44:05.468144600Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "train_with_target = calculate_RUL(train_data, time_column='Cycle', group_column='UnitNumber')\n",
    "uncorrelated_features = get_uncorrelated_features(train_with_target, threshold=0.3, target='RUL')\n",
    "uncorrelated_features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-20T19:49:51.125245200Z",
     "start_time": "2024-05-20T19:49:51.008983500Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary of data cleaning in one function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# clean data without outlier removal\n",
    "cleaned_train, cleaned_test = clean_data(train_data, test_data, method=None, ignore_columns=['UnitNumber', 'Cycle'], threshold_missing=0.1, threshold_corr=0.3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-20T21:03:06.074648900Z",
     "start_time": "2024-05-20T21:03:05.854621600Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "# clean data with outlier removal, where no samples are dropped but the outliers are replaced\n",
    "cleaned_train, cleaned_test = clean_data(train_data, test_data, method='winsorize', ignore_columns=['UnitNumber', 'Cycle'], threshold_missing=0.1, threshold_corr=0.3, contamination=0.05)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-20T21:09:11.019537300Z",
     "start_time": "2024-05-20T21:09:10.799626Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Post-Windowing Preprocessing (if necessary)\n",
    "Additional preprocessing steps after feature extraction if needed\n",
    "* handle correlated features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Feature Selection [TBD]\n",
    "\n",
    "Orientation:\n",
    "![Feature Selection](https://machinelearningmastery.com/wp-content/uploads/2019/11/How-to-Choose-Feature-Selection-Methods-For-Machine-Learning.png)\n",
    "\n",
    "Potential Feature Selection Methods:\n",
    "* Supervised:\n",
    "    * Filter Methods:\n",
    "        * Numerical Input, Numerical Output:\n",
    "                * Pearsonâ€™s correlation coefficient (linear)\n",
    "                * Spearmanâ€™s rank coefficient (nonlinear)\n",
    "        * --> Using Pearsonâ€™s Correlation Coefficient via the f_regression() function and SelectKBest (feature selection strategy).\n",
    "        * --> Using Mutual Information via the mutual_info_regression() function and SelectKBest (feature selection strategy).\n",
    "    * Wrapper Methods:\n",
    "        * Recursive Feature Elimination (RFE)\n",
    "        * --> Using RFE with a linear model (e.g., linear regression)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Dimensionality Reduction [TBD]\n",
    "Only if certain number of features is above a certain threshold.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
