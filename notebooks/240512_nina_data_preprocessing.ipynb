{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T15:26:04.088107200Z",
     "start_time": "2024-05-13T15:26:03.976004700Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T15:26:04.222102700Z",
     "start_time": "2024-05-13T15:26:04.088107200Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Topic: EX2 - Turbofan RUL Prediction\n",
    "**Task**: Predict the remaining useful life (RUL) of turbofan engines based on given sensor data (time series data). It is a regression problem.\n",
    "**Data**: Turbofan engine degradation simulation data (NASA) - [Link](https://data.nasa.gov/dataset/Turbofan-Engine-Degradation-Simulation-Data-Set/vrks-gjie). See also in the topic [introduction notebook](https://github.com/nina-prog/damage-propagation-modeling/blob/2fb8c1a1102a48d7abbf04e4031807790a913a99/notebooks/Turbofan%20remaining%20useful%20life%20Prediction.ipynb).\n",
    "\n",
    "**Subtasks**:\n",
    "1. Perform a deep **exploratory data analysis (EDA)** on the given data.\n",
    "2. Implement a more efficient **sliding window method** for time series data analysis.\n",
    "3. Apply **traditional machine learning methods** (SOTA) to predict the remaining useful life. Includes data preparation, feature extraction, feature selection, model selection, and model parameter optimization. -> 🎯 **Focus on this task** data preparation and feature selection (feature extraction part of sliding window method).\n",
    "4. Create **neural network models** to predict the remaining useful life. Includes different architectures like Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), or Attention Models. Note: You can search for SOTA research papers and reproduce current state-of-the-art models.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports + Settings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "# third-party libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import List, Union\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from scipy import stats"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T21:41:46.854544600Z",
     "start_time": "2024-05-15T21:41:46.676078200Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "# source code\n",
    "from src.utils import load_data, load_config\n",
    "from src.data_preprocessing import create_rolling_windows_datasets\n",
    "from src.data_cleaning import identify_missing_values, identify_single_unique_features, format_dtype"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T16:18:03.232858700Z",
     "start_time": "2024-05-15T16:18:02.261222500Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"Set2\")\n",
    "sns.set(rc={\"figure.dpi\":100, 'savefig.dpi':200})\n",
    "sns.set_context('notebook')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T15:26:05.731130600Z",
     "start_time": "2024-05-13T15:26:05.636584200Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "np.random.seed(42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T15:26:05.826159100Z",
     "start_time": "2024-05-13T15:26:05.716210Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Paths"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "os.chdir(\"../\") # set working directory to root of project\n",
    "#os.getcwd() # check current working directory"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T15:26:05.905696900Z",
     "start_time": "2024-05-13T15:26:05.816052700Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "PATH_TO_CONFIG = \"configs/config.yaml\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T15:26:06.001109800Z",
     "start_time": "2024-05-13T15:26:05.896117800Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load config + Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "config = load_config(PATH_TO_CONFIG) # config is dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T15:39:25.989524200Z",
     "start_time": "2024-05-13T15:39:25.889635200Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "%%time\n",
    "train_data, test_data, test_RUL_data = load_data(config_path=PATH_TO_CONFIG, dataset_num=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T21:46:17.782333Z",
     "start_time": "2024-05-15T21:46:17.228618300Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "%%time\n",
    "# 1 min 35 s without clean before window generation -> 59.4s with clean before window generation (fewer features)\n",
    "X_train, y_train, X_test, y_test = create_rolling_windows_datasets(train_data, test_data, test_RUL_data, column_id=\"UnitNumber\", column_sort=\"Cycle\", max_timeshift=config[\"preprocessing\"][\"max_window_size\"], min_timeshift=config[\"preprocessing\"][\"min_window_size\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T09:54:35.555815400Z",
     "start_time": "2024-05-14T09:53:36.051711500Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load saved preprocessed data from pickle\n",
    "#X_train = pd.read_pickle(\"data/processed/ex2_X_train_20240512-155504.pkl\")\n",
    "#y_train = pd.read_pickle(\"data/processed/ex2_y_train_20240512-155504.pkl\")\n",
    "#X_test = pd.read_pickle(\"data/processed/ex2_X_test_20240512-155504.pkl\")\n",
    "#y_test = pd.read_pickle(\"data/processed/ex2_y_test_20240512-155504.pkl\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Clean Data\n",
    "* format column types - only necessary if categorical columns are present ✅\n",
    "* handle missing values ✅\n",
    "* handle single unique values ✅\n",
    "* handle outliers\n",
    "* feature engineering\n",
    "    * group features with rarely occurring realizations into buckets\n",
    "* drop duplicates\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Format Column Types"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "formatted_df = format_dtype(train_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T21:46:23.425668900Z",
     "start_time": "2024-05-15T21:46:23.339325300Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Handle Missing Values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "missing_features = identify_missing_values(train_data, threshold=0.1)\n",
    "missing_features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T21:46:27.826349300Z",
     "start_time": "2024-05-15T21:46:27.748141900Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "# drop features with missing values\n",
    "train_data.drop(missing_features, axis=1, inplace=True)\n",
    "test_data.drop(missing_features, axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T21:53:09.004339900Z",
     "start_time": "2024-05-15T21:53:08.911404600Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Handle Single Unique Values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "single_unique_features = identify_single_unique_features(train_data)\n",
    "single_unique_features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T21:46:31.678048900Z",
     "start_time": "2024-05-15T21:46:31.583375500Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "# drop features with single unique values\n",
    "train_data.drop(single_unique_features, axis=1, inplace=True)\n",
    "test_data.drop(single_unique_features, axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T21:52:58.516841500Z",
     "start_time": "2024-05-15T21:52:58.438555Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Handle Outliers\n",
    "Possible Methods to investigate:\n",
    "* Z-Score\n",
    "* IQR\n",
    "* Winsorization\n",
    "* Isolation Forest ❌\n",
    "* Local Outlier Factor (LOF) ✅\n",
    "* Elliptic Envelope"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Z-Score\n",
    "**Use Case**: Normally distributed data\n",
    "**Assumption**: Assumes that the data follows a normal distribution. It identifies outliers based on how many standard deviations away from the mean a data point is.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "# visual inspection\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i, col in enumerate(train_data.drop(['UnitNumber', 'Cycle'], axis = 1).columns):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    sns.histplot(train_data[col], kde=True)\n",
    "    plt.title(col)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T21:53:28.543039600Z",
     "start_time": "2024-05-15T21:53:21.263786500Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Findings:\n",
    "* ```Sensor Measure 6``` is highly skewed -> **investigate further**(drop feature), indicating non-normal distribution\n",
    "* ```Operation Setting 2``` and ```Sensor Measure 17``` have multiple peaks and seem to only have a few unique numeric values -> **investigate further** (group into buckets, drop, etc.), indicating non-normal distribution\n",
    "* All other features exhibit approximately normal distributions, indicated by a single peak with bell-shaped curves."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Step 2: Summary Statistics\n",
    "summary_stats = {'Mean': np.mean(train_data, axis=0),\n",
    "                 'Median': np.median(train_data, axis=0),\n",
    "                 'Standard Deviation': np.std(train_data, axis=0),\n",
    "                 'Skewness': stats.skew(train_data, axis=0),\n",
    "                 'Kurtosis': stats.kurtosis(train_data, axis=0)}\n",
    "\n",
    "for stat, values in summary_stats.items():\n",
    "    print(f'{stat}: {values}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### IQR\n",
    "**Use Case**: Non-normally distributed data\n",
    "**Assumption**: Suitable for data that may not follow a normal distribution. It defines outliers based on the spread of the middle 50% of the data, rather than assuming a specific distribution.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Winsorization\n",
    "**Use Case**: Non-normally distributed data, when you need to reduce the influence of outliers without removing data points.\n",
    "**Assumption**: It doesn't assume any specific distribution but instead modifies extreme values to be less influential while preserving the data's overall distribution."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Isolation Forest\n",
    "**Use Case**: High-dimensional data\n",
    "**Assumption**: Effective for high-dimensional data where traditional distance-based methods might struggle. It works by isolating outliers in a way that separates them from the rest of the data using a tree-based approach.\n",
    "\n",
    "* --> High-dimensional data refers to data sets for which the number of variables or dimensions $p$ is much larger than the number of observations $n$, typically $p >> n$. Source: Giraud, C. (2015). *Introduction to High-Dimensional Statistics*. CRC Press.\n",
    "* ❌ **Not** our case (Shape of X_train: (20131, 170))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Local Outlier Factor (LOF)\n",
    "**Use Case**: Effective for datasets with varying densities.\n",
    "**Assumption**: LOF is useful for datasets where the density of points varies across the dataset. It calculates the local density around each data point and identifies outliers as points with significantly lower densities compared to their neighbors.\n",
    "\n",
    "* --> LOF well-suited for our dataset, which exhibits heterogeneous density distributions (see plots).\n",
    "* ✅ **Use LOF for outlier detection.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "# fit LOF model\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)  # Adjust parameters as needed\n",
    "lof.fit(X_train)\n",
    "\n",
    "# calc LOF scores\n",
    "lof_scores = -lof.negative_outlier_factor_\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(lof_scores, bins=50, edgecolor='k')\n",
    "plt.xlabel('LOF Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of LOF Scores')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-15T20:51:40.850711500Z",
     "start_time": "2024-05-15T20:51:32.723902700Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Findings:\n",
    "* Presence of scores below 1 indicates regions of exceptionally high data density. --> No outliers in these regions.\n",
    "* Peak around 1.0 indicates regions of average data density, clustered around a normal density level.\n",
    "* Gradual decrease in frequency suggests lower density regions.\n",
    "* Sharp drop at 1.3 may represent areas of lower data density, where outliers may start to emerge."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Elliptic Envelope\n",
    "**Use Case**: Assumes the data is normally distributed and identifies points that are far from the center. Suitable for multivariate normally distributed data.\n",
    "**Assumption**: Assumes the data is multivariate normally distributed and aims to identify outliers by fitting an ellipse around the central data points. Points outside this ellipse are considered outliers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def remove_outliers(df: pd.DataFrame, method: str = 'winsorize', ignore_columns: List[str] = None,\n",
    "                    threshold: float = 1.5, contamination: float = 0.05, soft_drop: bool = False):\n",
    "    \"\"\"\n",
    "    Handle outliers in a DataFrame using various methods.\n",
    "\n",
    "    :param df: The input DataFrame.\n",
    "    :type df: pd.DataFrame\n",
    "    :param method: The method to use for handling outliers. Choose from 'iqr', 'zscore', 'winsorize',\n",
    "    'isolation_forest', 'lof', 'elliptic_envelope'.\n",
    "    :type method: str\n",
    "    :param ignore_columns: The columns to ignore when handling outliers.\n",
    "    :type ignore_columns: list\n",
    "    :param threshold: The threshold for the outlier detection method.\n",
    "    :type threshold: float\n",
    "    :param contamination: The proportion of outliers which are\n",
    "    :type contamination: float\n",
    "    :param soft_drop: Boolean to indicate whether to softly drop outliers.\n",
    "    :type soft_drop: bool\n",
    "\n",
    "    :return: The DataFrame with outliers removed.\n",
    "    :rtype: pd.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    if ignore_columns is None:\n",
    "        ignore_columns = []\n",
    "\n",
    "    df_out = df.copy()\n",
    "\n",
    "    if method == 'iqr':\n",
    "        df_out = remove_outliers_iqr(df, threshold, soft_drop)\n",
    "    elif method == 'zscore':\n",
    "        df_out = remove_outliers_zscore(df, soft_drop)\n",
    "    elif method == 'winsorize':\n",
    "        df_out = remove_outliers_winsorize(df, ignore_columns, contamination)\n",
    "    elif method in ['isolation_forest', 'lof', 'elliptic_envelope']:\n",
    "        df_out = remove_outliers_machine_learning(df, method, contamination, soft_drop)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Unsupported method: Choose from 'iqr', 'zscore', 'winsorize', 'isolation_forest', 'lof', \"\n",
    "            \"'elliptic_envelope'\")\n",
    "\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def remove_outliers_iqr(df, threshold, soft_drop):\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - threshold * IQR\n",
    "    upper_bound = Q3 + threshold * IQR\n",
    "\n",
    "    if soft_drop:\n",
    "        outlier_proportion = ((df < lower_bound) | (df > upper_bound)).mean(axis=1)\n",
    "        return df[~((df < lower_bound) | (df > upper_bound) & (outlier_proportion > 0.3)).any(axis=1)]\n",
    "    else:\n",
    "        return df[~((df < lower_bound) | (df > upper_bound)).any(axis=1)]\n",
    "\n",
    "\n",
    "def remove_outliers_zscore(df, soft_drop):\n",
    "    z_scores = zscore(df)\n",
    "    threshold = 3\n",
    "\n",
    "    if soft_drop:\n",
    "        return df[(np.abs(z_scores) < threshold).all(axis=1) & (np.abs(z_scores) < threshold).mean(axis=1) > 0.3]\n",
    "    else:\n",
    "        return df[(np.abs(z_scores) < threshold).all(axis=1)]\n",
    "\n",
    "\n",
    "def remove_outliers_winsorize(df, ignore_columns, contamination):\n",
    "    return df.apply(\n",
    "        lambda x: winsorize(x, limits=[contamination, contamination]) if x.name not in ignore_columns else x)\n",
    "\n",
    "\n",
    "def remove_outliers_machine_learning(df, method, contamination, soft_drop):\n",
    "    outlier_detector = None\n",
    "    if method == 'isolation_forest':\n",
    "        outlier_detector = IsolationForest(contamination=contamination)\n",
    "    elif method == 'lof':\n",
    "        outlier_detector = LocalOutlierFactor(contamination=contamination)\n",
    "    elif method == 'elliptic_envelope':\n",
    "        outlier_detector = EllipticEnvelope(contamination=contamination)\n",
    "\n",
    "    yhat = outlier_detector.fit_predict(df)\n",
    "\n",
    "    if soft_drop:\n",
    "        return df[yhat != -1 & ((yhat != -1).mean(axis=1) > 0.3)]\n",
    "    else:\n",
    "        return df[yhat != -1]"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Summary of all preprocessing steps in one function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Post-Windowing Preprocessing (if necessary)\n",
    "Additional preprocessing steps after feature extraction if needed\n",
    "* handle correlated features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Feature Selection\n",
    "\n",
    "Orientation:\n",
    "![Feature Selection](https://machinelearningmastery.com/wp-content/uploads/2019/11/How-to-Choose-Feature-Selection-Methods-For-Machine-Learning.png)\n",
    "\n",
    "Potential Feature Selection Methods:\n",
    "* Supervised:\n",
    "    * Filter Methods:\n",
    "        * Numerical Input, Numerical Output:\n",
    "                * Pearson’s correlation coefficient (linear)\n",
    "                * Spearman’s rank coefficient (nonlinear)\n",
    "        * --> Using Pearson’s Correlation Coefficient via the f_regression() function and SelectKBest (feature selection strategy).\n",
    "        * --> Using Mutual Information via the mutual_info_regression() function and SelectKBest (feature selection strategy).\n",
    "    * Wrapper Methods:\n",
    "        * Recursive Feature Elimination (RFE)\n",
    "        * --> Using RFE with a linear model (e.g., linear regression)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Dimensionality Reduction\n",
    "Only if certain number of features is above a certain threshold.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
