{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Topic: EX2 - Turbofan RUL Prediction\n",
    "**Task**: Predict the remaining useful life (RUL) of turbofan engines based on given sensor data (time series data). It is a regression problem.\n",
    "**Data**: Turbofan engine degradation simulation data (NASA) - [Link](https://data.nasa.gov/dataset/Turbofan-Engine-Degradation-Simulation-Data-Set/vrks-gjie). See also in the topic [introduction notebook](https://github.com/nina-prog/damage-propagation-modeling/blob/2fb8c1a1102a48d7abbf04e4031807790a913a99/notebooks/Turbofan%20remaining%20useful%20life%20Prediction.ipynb).\n",
    "\n",
    "**Subtasks**:\n",
    "1. Perform a deep **exploratory data analysis (EDA)** on the given data.\n",
    "2. Implement a more efficient **sliding window method** for time series data analysis.\n",
    "3. Apply **traditional machine learning methods** (SOTA) to predict the remaining useful life. Includes data preparation, feature extraction, feature selection, model selection, and model parameter optimization. -> ðŸŽ¯ **Focus on this task** data preparation and feature selection (feature extraction part of sliding window method).\n",
    "4. Create **neural network models** to predict the remaining useful life. Includes different architectures like Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), or Attention Models. Note: You can search for SOTA research papers and reproduce current state-of-the-art models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports + Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# third-party libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import List, Union\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from scipy import stats\n",
    "from scipy.stats import multivariate_normal, zscore\n",
    "from scipy.stats._mstats_basic import winsorize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T20:33:54.874777700Z",
     "start_time": "2024-05-20T20:33:54.679873200Z"
    },
    "collapsed": false
   },
   "source": [
    "# source code\n",
    "os.chdir(\"../\") # set working directory to root of project\n",
    "#os.getcwd() # check current working directory\n",
    "\n",
    "from src.utils import load_data, load_config\n",
    "from src.rolling_window_creator import RollingWindowDatasetCreator, calculate_RUL\n",
    "from src.data_cleaning import identify_missing_values, identify_single_unique_features, format_dtype, clean_data\n",
    "import src.nn_utils as nu"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"Set2\")\n",
    "sns.set(rc={\"figure.dpi\":100, 'savefig.dpi':200})\n",
    "sns.set_context('notebook')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "np.random.seed(42)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "source": [
    "PATH_TO_CONFIG = \"configs/config.yaml\""
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load config + Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T19:22:00.302961200Z",
     "start_time": "2024-05-20T19:22:00.174245100Z"
    },
    "collapsed": false
   },
   "source": [
    "config = load_config(PATH_TO_CONFIG) # config is dict"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T21:30:59.785347900Z",
     "start_time": "2024-05-20T21:30:59.336554Z"
    },
    "collapsed": false
   },
   "source": [
    "%%time\n",
    "train_data, test_data, test_RUL_data = load_data(config_path=PATH_TO_CONFIG, dataset_num=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T21:27:47.244817900Z",
     "start_time": "2024-05-20T21:27:47.163989800Z"
    },
    "collapsed": false
   },
   "source": [
    "# count unit numbers in test set\n",
    "print(f\"Number of unique unit numbers in test set: {test_data['UnitNumber'].nunique()}\")\n",
    "# count min number of cycles in test set for each unit number --> window size must be in the range of these values, for example a window size of 10 would be too large if there is a unit number with only 10 cycles\n",
    "print(\"Min number of cycles in test set for a unit number: \", test_data.groupby(\"UnitNumber\")[\"Cycle\"].count().min())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "train_data"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "Test Data Cleaning Functionality and its impact on Rolling Window Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T21:30:34.171343100Z",
     "start_time": "2024-05-20T21:30:33.935594200Z"
    },
    "collapsed": false
   },
   "source": [
    "# clean data (with outlier removal, where no samples are dropped but the outliers are replaced, method='winsorize')\n",
    "# TODO: outsource settings to config file\n",
    "cleaned_train, cleaned_test = clean_data(train_data, test_data, method='winsorize', ignore_columns=['UnitNumber', 'Cycle'], threshold_missing=0.1, threshold_corr=0.3, contamination=0.05)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "cleaned_train_data = calculate_RUL(cleaned_train, time_column= \"Cycle\", group_column= \"UnitNumber\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "cleaned_train_data"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "train_data = nu.scale_data(cleaned_train_data)\n",
    "X_train, y_train = nu.create_sliding_window(cleaned_train_data)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "source": [
    "feature_size"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "source": [
    "# Example data (replace with actual data loading)\n",
    "seq_len, batch_size, feature_size = X_train.shape[1], 32, X_train.shape[2]\n",
    "num_heads, num_layers = 2, 2\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "    \n",
    "# Create dataset and dataloaders\n",
    "dataset = nu.TurbofanDataset(X_train, y_train)\n",
    "train_size = int(0.1 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "# Initialize model, criterion, optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = nu.TransformerModel(feature_size, num_heads, num_layers).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = nu.train_model(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss = nu.evaluate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
