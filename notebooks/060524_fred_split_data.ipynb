{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:37.900316500Z",
     "start_time": "2024-05-09T15:31:37.830560800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:37.980322700Z",
     "start_time": "2024-05-09T15:31:37.900316500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Topic: EX2 - Turbofan RUL Prediction\n",
    "**Task**: Predict the remaining useful life (RUL) of turbofan engines based on given sensor data (time series data). It is a forcasting problem, where the goal is to predict the number of cycles an engine will last before it fails.\n",
    "**Data**: Turbofan engine degradation simulation data (NASA) - [Link](https://data.nasa.gov/dataset/Turbofan-Engine-Degradation-Simulation-Data-Set/vrks-gjie). See also in the topic [introduction notebook](https://github.com/nina-prog/damage-propagation-modeling/blob/2fb8c1a1102a48d7abbf04e4031807790a913a99/notebooks/Turbofan%20remaining%20useful%20life%20Prediction.ipynb).\n",
    "\n",
    "**Subtasks**:\n",
    "1. Perform a deep **exploratory data analysis (EDA)** on the given data.\n",
    "2. Implement a more efficient **sliding window method** for time series data analysis. -> üéØ **Focus on this task**\n",
    "3. Apply **traditional machine learning methods** (SOTA) to predict the remaining useful life. Includes data preparation, feature extraction, feature selection, model selection, and model parameter optimization.\n",
    "4. Create **neural network models** to predict the remaining useful life. Includes different architectures like Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), or Attention Models. Note: You can search for SOTA research papers and reproduce current state-of-the-art models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports + Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:38.193478100Z",
     "start_time": "2024-05-09T15:31:37.980322700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# third-party libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:38.304026600Z",
     "start_time": "2024-05-09T15:31:38.200491900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# source code\n",
    "from src.utils import load_data, load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:38.400456500Z",
     "start_time": "2024-05-09T15:31:38.304026600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"Set2\")\n",
    "sns.set(rc={\"figure.dpi\":100, 'savefig.dpi':200})\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:38.492590100Z",
     "start_time": "2024-05-09T15:31:38.400456500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/frbroy/Library/Mobile Documents/com~apple~CloudDocs/KIT/SoSe2024/PSDA/damage-propagation-modeling'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure to execute this cell only once for one kernel session, before running any other cell below.\n",
    "os.chdir(\"../\") # set working directory to root of project\n",
    "os.getcwd() # check current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:38.652221700Z",
     "start_time": "2024-05-09T15:31:38.560578800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PATH_TO_CONFIG = \"configs/config.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load Config + Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:38.732034900Z",
     "start_time": "2024-05-09T15:31:38.652221700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = load_config(PATH_TO_CONFIG) # config is dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:39.154853100Z",
     "start_time": "2024-05-09T15:31:38.740553100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-16 11:17:24 [\u001b[34msrc.utils:56\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loading data set 1...\u001b[0m\n",
      "2024-05-16 11:17:24 [\u001b[34msrc.utils:85\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loaded raw data for dataset 1.\u001b[0m\n",
      "2024-05-16 11:17:24 [\u001b[34msrc.utils:86\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Train Data: (20631, 26)\u001b[0m\n",
      "2024-05-16 11:17:24 [\u001b[34msrc.utils:87\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test Data: (13096, 26)\u001b[0m\n",
      "2024-05-16 11:17:24 [\u001b[34msrc.utils:88\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test RUL Data: (100, 1)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, test_rul = load_data(config_path=PATH_TO_CONFIG, dataset_num=1) #, raw=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# üìç << Subtask Train split: Procesing >>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UnitNumber</th>\n",
       "      <th>Cycle</th>\n",
       "      <th>Operation Setting 1</th>\n",
       "      <th>Operation Setting 2</th>\n",
       "      <th>Operation Setting 3</th>\n",
       "      <th>Sensor Measure 1</th>\n",
       "      <th>Sensor Measure 2</th>\n",
       "      <th>Sensor Measure 3</th>\n",
       "      <th>Sensor Measure 4</th>\n",
       "      <th>Sensor Measure 5</th>\n",
       "      <th>...</th>\n",
       "      <th>Sensor Measure 12</th>\n",
       "      <th>Sensor Measure 13</th>\n",
       "      <th>Sensor Measure 14</th>\n",
       "      <th>Sensor Measure 15</th>\n",
       "      <th>Sensor Measure 16</th>\n",
       "      <th>Sensor Measure 17</th>\n",
       "      <th>Sensor Measure 18</th>\n",
       "      <th>Sensor Measure 19</th>\n",
       "      <th>Sensor Measure 20</th>\n",
       "      <th>Sensor Measure 21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>521.66</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.28</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.42</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.86</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.19</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   UnitNumber  Cycle  Operation Setting 1  Operation Setting 2  \\\n",
       "0           1      1              -0.0007              -0.0004   \n",
       "1           1      2               0.0019              -0.0003   \n",
       "2           1      3              -0.0043               0.0003   \n",
       "3           1      4               0.0007               0.0000   \n",
       "4           1      5              -0.0019              -0.0002   \n",
       "\n",
       "   Operation Setting 3  Sensor Measure 1  Sensor Measure 2  Sensor Measure 3  \\\n",
       "0                100.0            518.67            641.82           1589.70   \n",
       "1                100.0            518.67            642.15           1591.82   \n",
       "2                100.0            518.67            642.35           1587.99   \n",
       "3                100.0            518.67            642.35           1582.79   \n",
       "4                100.0            518.67            642.37           1582.85   \n",
       "\n",
       "   Sensor Measure 4  Sensor Measure 5  ...  Sensor Measure 12  \\\n",
       "0           1400.60             14.62  ...             521.66   \n",
       "1           1403.14             14.62  ...             522.28   \n",
       "2           1404.20             14.62  ...             522.42   \n",
       "3           1401.87             14.62  ...             522.86   \n",
       "4           1406.22             14.62  ...             522.19   \n",
       "\n",
       "   Sensor Measure 13  Sensor Measure 14  Sensor Measure 15  Sensor Measure 16  \\\n",
       "0            2388.02            8138.62             8.4195               0.03   \n",
       "1            2388.07            8131.49             8.4318               0.03   \n",
       "2            2388.03            8133.23             8.4178               0.03   \n",
       "3            2388.08            8133.83             8.3682               0.03   \n",
       "4            2388.04            8133.80             8.4294               0.03   \n",
       "\n",
       "   Sensor Measure 17  Sensor Measure 18  Sensor Measure 19  Sensor Measure 20  \\\n",
       "0                392               2388              100.0              39.06   \n",
       "1                392               2388              100.0              39.00   \n",
       "2                390               2388              100.0              38.95   \n",
       "3                392               2388              100.0              38.88   \n",
       "4                393               2388              100.0              38.90   \n",
       "\n",
       "   Sensor Measure 21  \n",
       "0            23.4190  \n",
       "1            23.4236  \n",
       "2            23.3442  \n",
       "3            23.3739  \n",
       "4            23.4044  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit \n",
    "\n",
    "splitter = GroupShuffleSplit(test_size=.18, n_splits=2, random_state = 7)\n",
    "split = splitter.split(train_data, groups=train_data['UnitNumber'])\n",
    "train_inds, test_inds = next(split)\n",
    "\n",
    "train = train_data.iloc[train_inds]\n",
    "test =  train_data.iloc[test_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit \n",
    "from src.logger import setup_logger\n",
    "\n",
    "logger = setup_logger(__name__, level='INFO')\n",
    "\n",
    "def train_val_split_by_group(df, group = \"UnitNumber\", test_size = .18, n_splits = 2, random_state = 7):\n",
    "\n",
    "    splitter = GroupShuffleSplit(test_size=test_size, n_splits=n_splits, random_state = random_state)\n",
    "    split = splitter.split(df, groups=df[group])\n",
    "    train_inds, test_inds = next(split)\n",
    "\n",
    "    train = df.iloc[train_inds]\n",
    "    test = df.iloc[test_inds]\n",
    "    \n",
    "    logger.info(f\"Train set contains {train[group].nunique()} different engines --> in total {len(train)}\")\n",
    "    logger.info(f\" Test set contains {test[group].nunique()} different engines --> in total {len(test)}\")\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold, GroupShuffleSplit\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "from typing import Any, Dict, Generator, Tuple\n",
    "from src.logger import setup_logger\n",
    "\n",
    "# Setup logger\n",
    "logger = setup_logger(__name__, level='INFO')\n",
    "\n",
    "def train_val_split_by_group(\n",
    "    df: pd.DataFrame,\n",
    "    group: str = \"UnitNumber\",\n",
    "    test_size: float = 0.18,\n",
    "    n_splits: int = 2,\n",
    "    random_state: int = 7\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Splits the DataFrame into training and validation sets based on a group identifier.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to split.\n",
    "    group (str): The column name to group by for splitting. Defaults to \"UnitNumber\".\n",
    "    test_size (float): The proportion of the dataset to include in the test split. Defaults to 0.18.\n",
    "    n_splits (int): Number of re-shuffling & splitting iterations. Defaults to 2.\n",
    "    random_state (int): Random state for reproducibility. Defaults to 7.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[pd.DataFrame, pd.DataFrame]: The training and test DataFrames.\n",
    "    \"\"\"\n",
    "    # Initialize GroupShuffleSplit\n",
    "    splitter = GroupShuffleSplit(test_size=test_size, n_splits=n_splits, random_state=random_state)\n",
    "    \n",
    "    # Perform the split\n",
    "    split = splitter.split(df, groups=df[group])\n",
    "    \n",
    "    # Get the indices for the train and test sets\n",
    "    train_inds, test_inds = next(split)\n",
    "    \n",
    "    # Create the train and test DataFrames using the indices\n",
    "    train = df.iloc[train_inds]\n",
    "    test = df.iloc[test_inds]\n",
    "    \n",
    "    # Log the number of unique groups and total rows in the train and test sets\n",
    "    logger.info(f\"Train set contains {train[group].nunique()} different engines --> in total {len(train)}\")\n",
    "    logger.info(f\" Test set contains {test[group].nunique()} different engines --> in total {len(test)}\")\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "\n",
    "\n",
    "# Setup logger\n",
    "logger = setup_logger(__name__, level='INFO')\n",
    "\n",
    "def k_fold_group_cross_validation(\n",
    "    df: pd.DataFrame,\n",
    "    group: str = \"UnitNumber\",\n",
    "    n_splits: int = 5\n",
    ") -> Generator[Tuple[pd.DataFrame, pd.DataFrame], None, None]:\n",
    "    \"\"\"\n",
    "    Performs K-fold group cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to split.\n",
    "    group (str): The column name to group by for splitting. Defaults to \"UnitNumber\".\n",
    "    n_splits (int): Number of folds. Defaults to 5.\n",
    "    random_state (int): Random state for reproducibility. Defaults to None.\n",
    "\n",
    "    Yields:\n",
    "    Generator[Tuple[pd.DataFrame, pd.DataFrame], None, None]: \n",
    "        A generator yielding tuples of (train DataFrame, validation DataFrame) for each fold.\n",
    "    \"\"\"\n",
    "    # Initialize GroupKFold\n",
    "    group_kfold = GroupKFold(n_splits=n_splits)\n",
    "    \n",
    "    # Iterate over each fold\n",
    "    for fold, (train_inds, val_inds) in enumerate(group_kfold.split(df, groups=df[group])):\n",
    "        # Create the train and validation DataFrames using the indices\n",
    "        train = df.iloc[train_inds]\n",
    "        val = df.iloc[val_inds]\n",
    "        \n",
    "        # Log the number of unique groups and total rows in the train and validation sets\n",
    "        logger.info(f\"Fold {fold + 1}:\")\n",
    "        logger.info(f\"Train set contains {train[group].nunique()} different engines --> in total {len(train)}\")\n",
    "        logger.info(f\"Validation set contains {val[group].nunique()} different engines --> in total {len(val)}\")\n",
    "        \n",
    "        yield train, val\n",
    "\n",
    "# Example usage:\n",
    "# for train_df, val_df in k_fold_group_cross_validation(df):\n",
    "#     # train your model on train_df\n",
    "#     # validate your model on val_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Setup logger\n",
    "logger = setup_logger(__name__, level='INFO')\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(\n",
    "    model: Any,\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    groups: pd.Series,\n",
    "    n_splits: int = 5,\n",
    ") -> Dict[str, list]:\n",
    "    \"\"\"\n",
    "    Train and evaluate a model using the specified cross-validation strategy.\n",
    "\n",
    "    Parameters:\n",
    "    model (Any): The model to be trained and evaluated.\n",
    "    X (pd.DataFrame): The feature matrix.\n",
    "    y (pd.Series): The target variable.\n",
    "    groups (pd.Series): The group labels for cross-validation.\n",
    "    cv (Generator): Cross-validation strategy.\n",
    "    scoring (Dict[str, make_scorer]): The scoring metrics.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, list]: Cross-validation scores for each defined metric.\n",
    "    \"\"\"\n",
    "    cv = GroupKFold(n_splits=n_splits)\n",
    "    # Define the scoring metrics for regression\n",
    "    scoring: Dict[str, make_scorer] = {\n",
    "        'mae': make_scorer(mean_absolute_error),\n",
    "        'mse': make_scorer(mean_squared_error),\n",
    "        'r2': make_scorer(r2_score)\n",
    "    }\n",
    "\n",
    "    # Perform cross-validation\n",
    "    scores = cross_validate(model, X, y, cv=cv, groups=groups, scoring=scoring, return_train_score=False)\n",
    "    \n",
    "    # Log the results\n",
    "    for metric in scoring.keys():\n",
    "        logger.info(f\"{metric.upper()} Scores: {scores['test_' + metric]}\")\n",
    "        logger.info(f\"Average {metric.upper()}: {scores['test_' + metric].mean():.4f}\")\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-16 14:09:04 [\u001b[34m__main__:42\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Train set contains 82 different engines --> in total 16807\u001b[0m\n",
      "2024-05-16 14:09:04 [\u001b[34m__main__:43\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>>  Test set contains 18 different engines --> in total 3824\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train, val  = train_val_split_by_group(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = k_fold_group_cross_validation(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MLPRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMLPRegressor\u001b[49m(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, early_stopping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m)\n\u001b[1;32m      3\u001b[0m train_and_evaluate_model(SVM)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MLPRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "model = MLPRegressor(random_state=42, max_iter=1000, early_stopping=True, alpha=0.05)\n",
    "train_and_evaluate_model(model, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object k_fold_group_cross_validation at 0x1594afb50>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"UnitNumber\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "[TEMPLATE]\n",
    "\n",
    "Findings:\n",
    "* Interpretation of plots\n",
    "* or other key take aways from previous code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:39.220343800Z",
     "start_time": "2024-05-09T15:31:39.160435700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m      3\u001b[0m timestamp \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m df\u001b[38;5;241m.\u001b[39mto_pickle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mconfig\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaths\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_data_dir\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mex2_topic_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "# [TEMPLATE] - save processed data (as pickle)\n",
    "df = pd.DataFrame()\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "df.to_pickle(f\"{config['paths']['processed_data_dir']}ex2_topic_{timestamp}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:39.318667400Z",
     "start_time": "2024-05-09T15:31:39.220343800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# [TEMPLATE] - save data predictions (as csv)\n",
    "df = pd.DataFrame()\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "df.to_csv(f\"{config['paths']['prediction_dir']}ex2_topic_{timestamp}.csv\", sep=',', decimal='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:39.485116500Z",
     "start_time": "2024-05-09T15:31:39.320350800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 900x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [TEMPLATE] - save plot results (as png)\n",
    "fig = plt.figure(figsize=(9, 6))\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "fig.savefig(f\"{config['paths']['plot_dir']}ex2_topic_{timestamp}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T15:31:39.522027500Z",
     "start_time": "2024-05-09T15:31:39.474058600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
