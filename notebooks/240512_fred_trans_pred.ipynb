{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Topic: EX2 - Turbofan RUL Prediction\n",
    "**Task**: Predict the remaining useful life (RUL) of turbofan engines based on given sensor data (time series data). It is a regression problem.\n",
    "**Data**: Turbofan engine degradation simulation data (NASA) - [Link](https://data.nasa.gov/dataset/Turbofan-Engine-Degradation-Simulation-Data-Set/vrks-gjie). See also in the topic [introduction notebook](https://github.com/nina-prog/damage-propagation-modeling/blob/2fb8c1a1102a48d7abbf04e4031807790a913a99/notebooks/Turbofan%20remaining%20useful%20life%20Prediction.ipynb).\n",
    "\n",
    "**Subtasks**:\n",
    "1. Perform a deep **exploratory data analysis (EDA)** on the given data.\n",
    "2. Implement a more efficient **sliding window method** for time series data analysis.\n",
    "3. Apply **traditional machine learning methods** (SOTA) to predict the remaining useful life. Includes data preparation, feature extraction, feature selection, model selection, and model parameter optimization. -> ðŸŽ¯ **Focus on this task** data preparation and feature selection (feature extraction part of sliding window method).\n",
    "4. Create **neural network models** to predict the remaining useful life. Includes different architectures like Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), or Attention Models. Note: You can search for SOTA research papers and reproduce current state-of-the-art models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Imports + Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# third-party libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import List, Union\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from scipy import stats\n",
    "from scipy.stats import multivariate_normal, zscore\n",
    "from scipy.stats._mstats_basic import winsorize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T20:33:54.874777700Z",
     "start_time": "2024-05-20T20:33:54.679873200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# source code\n",
    "os.chdir(\"../\") # set working directory to root of project\n",
    "#os.getcwd() # check current working directory\n",
    "\n",
    "from src.utils import load_data, load_config, train_val_split_by_group\n",
    "from src.rolling_window_creator import RollingWindowDatasetCreator, calculate_RUL\n",
    "from src.data_cleaning import identify_missing_values, identify_single_unique_features, format_dtype, clean_data\n",
    "import src.nn_utils as nu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"Set2\")\n",
    "sns.set(rc={\"figure.dpi\":100, 'savefig.dpi':200})\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "PATH_TO_CONFIG = \"configs/config.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Load config + Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T19:22:00.302961200Z",
     "start_time": "2024-05-20T19:22:00.174245100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "config = load_config(PATH_TO_CONFIG) # config is dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T21:30:59.785347900Z",
     "start_time": "2024-05-20T21:30:59.336554Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-24 14:30:51 [\u001b[34msrc.utils:60\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loading data set 1...\u001b[0m\n",
      "2024-05-24 14:30:51 [\u001b[34msrc.utils:89\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Loaded raw data for dataset 1.\u001b[0m\n",
      "2024-05-24 14:30:51 [\u001b[34msrc.utils:90\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Train Data: (20631, 26)\u001b[0m\n",
      "2024-05-24 14:30:51 [\u001b[34msrc.utils:91\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test Data: (13096, 26)\u001b[0m\n",
      "2024-05-24 14:30:51 [\u001b[34msrc.utils:92\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Test RUL Data: (100, 1)\u001b[0m\n",
      "CPU times: user 62.6 ms, sys: 12.8 ms, total: 75.4 ms\n",
      "Wall time: 75.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data, test_data, test_RUL_data = load_data(config_path=PATH_TO_CONFIG, dataset_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T21:27:47.244817900Z",
     "start_time": "2024-05-20T21:27:47.163989800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique unit numbers in test set: 100\n",
      "Min number of cycles in test set for a unit number:  31\n"
     ]
    }
   ],
   "source": [
    "# count unit numbers in test set\n",
    "print(f\"Number of unique unit numbers in test set: {test_data['UnitNumber'].nunique()}\")\n",
    "# count min number of cycles in test set for each unit number --> window size must be in the range of these values, for example a window size of 10 would be too large if there is a unit number with only 10 cycles\n",
    "print(\"Min number of cycles in test set for a unit number: \", test_data.groupby(\"UnitNumber\")[\"Cycle\"].count().min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "---\n",
    "Test Data Cleaning Functionality and its impact on Rolling Window Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_RUL_test(test_data, RUL_data):\n",
    "    RUL = []\n",
    "    for i in RUL_data.iterrows():\n",
    "        unit_num = i[0]\n",
    "        val = i[1][\"RUL\"]\n",
    "        tmp = test_data[test_data[\"UnitNumber\"] == unit_num + 1]\n",
    "        li = list(range(val + len(tmp) - 1, val - 1, -1))\n",
    "        for j in li:\n",
    "            RUL.append(j)\n",
    "        assert RUL[-1] == val\n",
    "    assert len(RUL) == len(test_data)\n",
    "    test_data[\"RUL\"] = RUL\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T21:30:34.171343100Z",
     "start_time": "2024-05-20T21:30:33.935594200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-24 14:30:52 [\u001b[34msrc.data_cleaning:134\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Cleaning train and test data...\u001b[0m\n",
      "2024-05-24 14:30:52 [\u001b[34msrc.data_cleaning:136\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Formatting column types...\u001b[0m\n",
      "2024-05-24 14:30:52 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
      "2024-05-24 14:30:52 [\u001b[34msrc.data_cleaning:69\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 categorical columns: []\u001b[0m\n",
      "2024-05-24 14:30:52 [\u001b[34msrc.data_cleaning:141\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Handling duplicates...\u001b[0m\n",
      "2024-05-24 14:30:52 [\u001b[34msrc.data_cleaning:146\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Removing outliers...\u001b[0m\n",
      "2024-05-24 14:30:52 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: winsorize ...\u001b[0m\n",
      "2024-05-24 14:30:52 [\u001b[34msrc.outlier_detection:98\u001b[0m] [DEBUG\u001b[0m] >>>> Found 1031 outliers to be replaced (winsorized).\u001b[0m\n",
      "2024-05-24 14:30:52 [\u001b[34msrc.outlier_detection:100\u001b[0m] [DEBUG\u001b[0m] >>>> Original DataFrame shape: (20631, 26), Resulting DataFrame shape: (20631, 26)\u001b[0m\n",
      "2024-05-24 14:30:52 [\u001b[34msrc.outlier_detection:150\u001b[0m] [DEBUG\u001b[0m] >>>> Removing outliers using method: winsorize ...\u001b[0m\n",
      "2024-05-24 14:30:52 [\u001b[34msrc.outlier_detection:98\u001b[0m] [DEBUG\u001b[0m] >>>> Found 654 outliers to be replaced (winsorized).\u001b[0m\n",
      "2024-05-24 14:30:52 [\u001b[34msrc.outlier_detection:100\u001b[0m] [DEBUG\u001b[0m] >>>> Original DataFrame shape: (13096, 26), Resulting DataFrame shape: (13096, 26)\u001b[0m\n",
      "2024-05-24 14:30:52 [\u001b[34msrc.data_cleaning:150\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Filter features based train data...\u001b[0m\n",
      "2024-05-24 14:30:52 [\u001b[34msrc.data_cleaning:26\u001b[0m] [DEBUG\u001b[0m] >>>> Found 0 features with missing values above the threshold of 0.1.\u001b[0m\n",
      "2024-05-24 14:30:52 [\u001b[34msrc.data_cleaning:46\u001b[0m] [DEBUG\u001b[0m] >>>> Found 8 features with only a single unique value: ['Operation Setting 3', 'Sensor Measure 1', 'Sensor Measure 5', 'Sensor Measure 6', 'Sensor Measure 10', 'Sensor Measure 16', 'Sensor Measure 18', 'Sensor Measure 19']\u001b[0m\n",
      "2024-05-24 14:30:52 [\u001b[34msrc.data_cleaning:103\u001b[0m] [DEBUG\u001b[0m] >>>> Found 3 uncorrelated features with a correlation threshold of 0.3: ['UnitNumber', 'Operation Setting 1', 'Operation Setting 2']\u001b[0m\n",
      "2024-05-24 14:30:52 [\u001b[34msrc.data_cleaning:162\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Dropping features based on missing values, single unique values, and no target correlation...\u001b[0m\n",
      "2024-05-24 14:30:52 [\u001b[34msrc.data_cleaning:172\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Data cleaning completed.\u001b[0m\n",
      "2024-05-24 14:30:52 [\u001b[34msrc.data_cleaning:173\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original train DataFrame shape: (20631, 26), Resulting train DataFrame shape: (20631, 16)\u001b[0m\n",
      "2024-05-24 14:30:52 [\u001b[34msrc.data_cleaning:174\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Original test DataFrame shape: (13096, 26), Resulting test DataFrame shape: (13096, 16)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# clean data (with outlier removal, where no samples are dropped but the outliers are replaced, method='winsorize')\n",
    "# TODO: outsource settings to config file\n",
    "cleaned_train, cleaned_test = clean_data(train_data, test_data, method='winsorize', ignore_columns=['UnitNumber', 'Cycle'], threshold_missing=0.1, threshold_corr=0.3, contamination=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_train_data = calculate_RUL(cleaned_train, time_column= \"Cycle\", group_column= \"UnitNumber\")\n",
    "cleaned_test_data = calculate_RUL_test(cleaned_test, test_RUL_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UnitNumber</th>\n",
       "      <th>Cycle</th>\n",
       "      <th>Sensor Measure 2</th>\n",
       "      <th>Sensor Measure 3</th>\n",
       "      <th>Sensor Measure 4</th>\n",
       "      <th>Sensor Measure 7</th>\n",
       "      <th>Sensor Measure 8</th>\n",
       "      <th>Sensor Measure 9</th>\n",
       "      <th>Sensor Measure 11</th>\n",
       "      <th>Sensor Measure 12</th>\n",
       "      <th>Sensor Measure 13</th>\n",
       "      <th>Sensor Measure 14</th>\n",
       "      <th>Sensor Measure 15</th>\n",
       "      <th>Sensor Measure 17</th>\n",
       "      <th>Sensor Measure 20</th>\n",
       "      <th>Sensor Measure 21</th>\n",
       "      <th>RUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>641.92</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>554.36</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>9046.19</td>\n",
       "      <td>47.47</td>\n",
       "      <td>521.66</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>392</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>553.75</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>9044.07</td>\n",
       "      <td>47.49</td>\n",
       "      <td>522.28</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>392</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>554.26</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>9052.94</td>\n",
       "      <td>47.27</td>\n",
       "      <td>522.42</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>391</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>554.45</td>\n",
       "      <td>2388.11</td>\n",
       "      <td>9049.48</td>\n",
       "      <td>47.15</td>\n",
       "      <td>522.50</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3859</td>\n",
       "      <td>392</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>554.00</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>9055.15</td>\n",
       "      <td>47.28</td>\n",
       "      <td>522.19</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>393</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20626</th>\n",
       "      <td>100</td>\n",
       "      <td>196</td>\n",
       "      <td>643.49</td>\n",
       "      <td>1597.98</td>\n",
       "      <td>1425.67</td>\n",
       "      <td>551.74</td>\n",
       "      <td>2388.19</td>\n",
       "      <td>9065.52</td>\n",
       "      <td>48.05</td>\n",
       "      <td>520.04</td>\n",
       "      <td>2388.23</td>\n",
       "      <td>8137.60</td>\n",
       "      <td>8.4956</td>\n",
       "      <td>396</td>\n",
       "      <td>38.49</td>\n",
       "      <td>23.0934</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20627</th>\n",
       "      <td>100</td>\n",
       "      <td>197</td>\n",
       "      <td>643.54</td>\n",
       "      <td>1601.47</td>\n",
       "      <td>1425.67</td>\n",
       "      <td>551.74</td>\n",
       "      <td>2388.22</td>\n",
       "      <td>9065.11</td>\n",
       "      <td>48.04</td>\n",
       "      <td>520.04</td>\n",
       "      <td>2388.22</td>\n",
       "      <td>8136.50</td>\n",
       "      <td>8.5110</td>\n",
       "      <td>395</td>\n",
       "      <td>38.49</td>\n",
       "      <td>23.1594</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20628</th>\n",
       "      <td>100</td>\n",
       "      <td>198</td>\n",
       "      <td>643.42</td>\n",
       "      <td>1601.47</td>\n",
       "      <td>1425.67</td>\n",
       "      <td>551.74</td>\n",
       "      <td>2388.22</td>\n",
       "      <td>9065.90</td>\n",
       "      <td>48.05</td>\n",
       "      <td>520.04</td>\n",
       "      <td>2388.23</td>\n",
       "      <td>8141.05</td>\n",
       "      <td>8.5110</td>\n",
       "      <td>396</td>\n",
       "      <td>38.49</td>\n",
       "      <td>23.0934</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20629</th>\n",
       "      <td>100</td>\n",
       "      <td>199</td>\n",
       "      <td>643.23</td>\n",
       "      <td>1601.47</td>\n",
       "      <td>1425.67</td>\n",
       "      <td>551.74</td>\n",
       "      <td>2388.22</td>\n",
       "      <td>9073.72</td>\n",
       "      <td>48.05</td>\n",
       "      <td>520.04</td>\n",
       "      <td>2388.23</td>\n",
       "      <td>8139.29</td>\n",
       "      <td>8.5110</td>\n",
       "      <td>395</td>\n",
       "      <td>38.49</td>\n",
       "      <td>23.0934</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20630</th>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>643.58</td>\n",
       "      <td>1600.38</td>\n",
       "      <td>1425.67</td>\n",
       "      <td>551.74</td>\n",
       "      <td>2388.22</td>\n",
       "      <td>9061.48</td>\n",
       "      <td>48.05</td>\n",
       "      <td>520.04</td>\n",
       "      <td>2388.23</td>\n",
       "      <td>8137.33</td>\n",
       "      <td>8.5036</td>\n",
       "      <td>396</td>\n",
       "      <td>38.49</td>\n",
       "      <td>23.0934</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20631 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       UnitNumber  Cycle  Sensor Measure 2  Sensor Measure 3  \\\n",
       "0               1      1            641.92           1589.70   \n",
       "1               1      2            642.15           1591.82   \n",
       "2               1      3            642.35           1587.99   \n",
       "3               1      4            642.35           1582.79   \n",
       "4               1      5            642.37           1582.85   \n",
       "...           ...    ...               ...               ...   \n",
       "20626         100    196            643.49           1597.98   \n",
       "20627         100    197            643.54           1601.47   \n",
       "20628         100    198            643.42           1601.47   \n",
       "20629         100    199            643.23           1601.47   \n",
       "20630         100    200            643.58           1600.38   \n",
       "\n",
       "       Sensor Measure 4  Sensor Measure 7  Sensor Measure 8  Sensor Measure 9  \\\n",
       "0               1400.60            554.36           2388.06           9046.19   \n",
       "1               1403.14            553.75           2388.04           9044.07   \n",
       "2               1404.20            554.26           2388.08           9052.94   \n",
       "3               1401.87            554.45           2388.11           9049.48   \n",
       "4               1406.22            554.00           2388.06           9055.15   \n",
       "...                 ...               ...               ...               ...   \n",
       "20626           1425.67            551.74           2388.19           9065.52   \n",
       "20627           1425.67            551.74           2388.22           9065.11   \n",
       "20628           1425.67            551.74           2388.22           9065.90   \n",
       "20629           1425.67            551.74           2388.22           9073.72   \n",
       "20630           1425.67            551.74           2388.22           9061.48   \n",
       "\n",
       "       Sensor Measure 11  Sensor Measure 12  Sensor Measure 13  \\\n",
       "0                  47.47             521.66            2388.02   \n",
       "1                  47.49             522.28            2388.07   \n",
       "2                  47.27             522.42            2388.03   \n",
       "3                  47.15             522.50            2388.08   \n",
       "4                  47.28             522.19            2388.04   \n",
       "...                  ...                ...                ...   \n",
       "20626              48.05             520.04            2388.23   \n",
       "20627              48.04             520.04            2388.22   \n",
       "20628              48.05             520.04            2388.23   \n",
       "20629              48.05             520.04            2388.23   \n",
       "20630              48.05             520.04            2388.23   \n",
       "\n",
       "       Sensor Measure 14  Sensor Measure 15  Sensor Measure 17  \\\n",
       "0                8138.62             8.4195                392   \n",
       "1                8131.49             8.4318                392   \n",
       "2                8133.23             8.4178                391   \n",
       "3                8133.83             8.3859                392   \n",
       "4                8133.80             8.4294                393   \n",
       "...                  ...                ...                ...   \n",
       "20626            8137.60             8.4956                396   \n",
       "20627            8136.50             8.5110                395   \n",
       "20628            8141.05             8.5110                396   \n",
       "20629            8139.29             8.5110                395   \n",
       "20630            8137.33             8.5036                396   \n",
       "\n",
       "       Sensor Measure 20  Sensor Measure 21  RUL  \n",
       "0                  39.06            23.4190  192  \n",
       "1                  39.00            23.4236  191  \n",
       "2                  38.95            23.3442  190  \n",
       "3                  38.88            23.3739  189  \n",
       "4                  38.90            23.4044  188  \n",
       "...                  ...                ...  ...  \n",
       "20626              38.49            23.0934    5  \n",
       "20627              38.49            23.1594    4  \n",
       "20628              38.49            23.0934    3  \n",
       "20629              38.49            23.0934    2  \n",
       "20630              38.49            23.0934    1  \n",
       "\n",
       "[20631 rows x 17 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (seq_len, batch_size, feature_size)\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, feature_size: int, num_heads: int, num_layers: int, project_dim : int, window_size: int = 30, dropout: float = 0.05):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.feature_size = feature_size\n",
    "        self.project_dim = project_dim\n",
    "        #self.intermediate_dim = intermediate_dim\n",
    "        # pseudo emb\n",
    "        self.project_emb = nn.Linear(feature_size, project_dim)\n",
    "        \n",
    "        # Positional Encoding\n",
    "        self.positional_encoding = PositionalEncoding(project_dim, dropout)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=project_dim, nhead=num_heads, dropout=dropout, batch_first = True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Fully Connected layers to output\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(window_size * project_dim, 512)  # First fully connected layer\n",
    "        self.fc2 = nn.Linear(512, 64)       # Second fully connected layer \n",
    "        self.fc3 = nn.Linear(64, 1)  \n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "        #self.intermediate = nn.Linear(project_dim, intermediate)\n",
    "        # Output layer\n",
    "        #self.fc_out = nn.Linear(intermediate_dim, 1) \n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (seq_len, batch_size, feature_size)\n",
    "            mask: Optional mask of shape (seq_len, seq_len)\n",
    "            \n",
    "        Returns:\n",
    "            out: Tensor of shape (batch_size, 1) for regression\n",
    "        \"\"\"\n",
    "        #print(f\"Input dim: {x.shape}\")\n",
    "        # Pseudo projection\n",
    "        x = self.project_emb(x)\n",
    "        #print(f\"Projection dim: {x.shape}\")\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.positional_encoding(x)# (seq_len, batch_size, feature_size)\n",
    "        x = x.to(torch.float32)\n",
    "        #print(f\"Positional dim: {x.shape}\")\n",
    "        \n",
    "        # Pass through transformer encoder\n",
    "        x = self.transformer_encoder(x, mask)  # (seq_len, batch_size, feature_size)\n",
    "        #print(x.var())\n",
    "        #print(f\"Transformer dim: {x.shape}\")\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        #print(f\"Flatten: {x.shape}\")\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        #print(f\"FC 1: {x.shape}\")\n",
    "        x = self.fc2(x)\n",
    "        x = self.act(x)\n",
    "        #print(f\"FC2: {x.shape}\")\n",
    "        out = self.fc3(x)\n",
    "        \n",
    "        # Take the mean across the sequence length dimension\n",
    "        #x = torch.mean(x, dim=1)# (batch_size, feature_size)\n",
    "        #print(x.var())\n",
    "        #print(f\"Mean dim: {x.shape}\")\n",
    "        #print(f\"After Median: {x.shape}\")\n",
    "        # Output layer\n",
    "        #out = self.fc_out(x)  # (batch_size, 1)\n",
    "        #print(f\"Out dim: {out.shape}\")\n",
    "        #print(out.var())\n",
    "\n",
    "        return out\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "# Example dataset class\n",
    "class TurbofanDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = torch.from_numpy(data).to(torch.float32)\n",
    "        self.targets = torch.from_numpy(targets).to(torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "# Training function\n",
    "def train_model(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    count = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        #if count % 44 == 0:\n",
    "        #    print(f\"--> {count}/{len(dataloader)}\")\n",
    "        count += 1\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        targets = targets.view(-1, 1)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            targets = targets.view(-1, 1)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            #if count == 1 or count == 10:\n",
    "                #print(count)\n",
    "                #print(outputs[:10], targets[:10])\n",
    "            count += 1\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-24 15:02:05 [\u001b[34msrc.utils:131\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>> Train set contains 82 different engines --> in total 16807\u001b[0m\n",
      "2024-05-24 15:02:05 [\u001b[34msrc.utils:132\u001b[0m] [\u001b[32mINFO\u001b[0m] >>>>  Test set contains 18 different engines --> in total 3824\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## create dataset\n",
    "window_size = 160\n",
    "train_data = nu.scale_data(cleaned_train_data)\n",
    "#train, val = train_val_split_by_group(train_data)\n",
    "\n",
    "X_train, y_train = nu.create_sliding_window(train_data, window_size = window_size)\n",
    "#X_val, y_val = nu.create_sliding_window(val, window_size = window_size)\n",
    "\n",
    "test_data = nu.scale_data(cleaned_test_data)\n",
    "X_test, y_test = nu.create_sliding_window(test_data, typ = \"test\", window_size = window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'window_size': 160, 'project_dim': 192, 'num_heads': 8, 'num_layers': 1, 'batch_size': 64, 'num_epochs': 175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n",
      "The model has in total 7290593 parameters!!\n",
      "Epoch 1/450, Train_L: 1959.89, Val_L: 2190.69, VAl_RMSE: 46.80, Test_L: 2872.58, Test_RMSE: 53.60 \n",
      "Epoch 2/450, Train_L: 1706.62, Val_L: 2217.32, VAl_RMSE: 47.09, Test_L: 3102.86, Test_RMSE: 55.70 \n",
      "Epoch 3/450, Train_L: 1710.26, Val_L: 2203.09, VAl_RMSE: 46.94, Test_L: 3008.56, Test_RMSE: 54.85 \n",
      "Epoch 4/450, Train_L: 1708.03, Val_L: 2190.14, VAl_RMSE: 46.80, Test_L: 2864.58, Test_RMSE: 53.52 \n",
      "Epoch 5/450, Train_L: 1710.28, Val_L: 2192.77, VAl_RMSE: 46.83, Test_L: 2908.32, Test_RMSE: 53.93 \n",
      "Epoch 6/450, Train_L: 1708.43, Val_L: 2197.46, VAl_RMSE: 46.88, Test_L: 2960.86, Test_RMSE: 54.41 \n",
      "Epoch 7/450, Train_L: 1708.94, Val_L: 2196.04, VAl_RMSE: 46.86, Test_L: 2946.72, Test_RMSE: 54.28 \n",
      "Epoch 8/450, Train_L: 1708.87, Val_L: 2188.49, VAl_RMSE: 46.78, Test_L: 2819.87, Test_RMSE: 53.10 \n",
      "Epoch 9/450, Train_L: 1717.11, Val_L: 2189.35, VAl_RMSE: 46.79, Test_L: 2848.03, Test_RMSE: 53.37 \n",
      "Epoch 10/450, Train_L: 1704.89, Val_L: 2189.84, VAl_RMSE: 46.80, Test_L: 2728.00, Test_RMSE: 52.23 \n",
      "Epoch 11/450, Train_L: 1711.91, Val_L: 2226.63, VAl_RMSE: 47.19, Test_L: 3156.66, Test_RMSE: 56.18 \n",
      "Epoch 12/450, Train_L: 1710.19, Val_L: 2196.73, VAl_RMSE: 46.87, Test_L: 2962.80, Test_RMSE: 54.43 \n",
      "Epoch 13/450, Train_L: 1707.85, Val_L: 2191.38, VAl_RMSE: 46.81, Test_L: 2911.29, Test_RMSE: 53.96 \n",
      "Epoch 14/450, Train_L: 1705.63, Val_L: 2225.00, VAl_RMSE: 47.17, Test_L: 3178.00, Test_RMSE: 56.37 \n",
      "Epoch 15/450, Train_L: 1708.79, Val_L: 2216.72, VAl_RMSE: 47.08, Test_L: 3138.30, Test_RMSE: 56.02 \n",
      "Epoch 16/450, Train_L: 1703.52, Val_L: 2185.90, VAl_RMSE: 46.75, Test_L: 3043.64, Test_RMSE: 55.17 \n",
      "Epoch 17/450, Train_L: 1696.37, Val_L: 2143.66, VAl_RMSE: 46.30, Test_L: 2977.01, Test_RMSE: 54.56 \n",
      "Epoch 18/450, Train_L: 1623.91, Val_L: 1933.53, VAl_RMSE: 43.97, Test_L: 2128.02, Test_RMSE: 46.13 \n",
      "Epoch 19/450, Train_L: 1506.68, Val_L: 1432.23, VAl_RMSE: 37.84, Test_L: 2941.08, Test_RMSE: 54.23 \n",
      "Epoch 20/450, Train_L: 1219.77, Val_L: 962.95, VAl_RMSE: 31.03, Test_L: 1312.38, Test_RMSE: 36.23 \n",
      "Epoch 21/450, Train_L: 915.91, Val_L: 767.37, VAl_RMSE: 27.70, Test_L: 1091.26, Test_RMSE: 33.03 \n",
      "Epoch 22/450, Train_L: 806.43, Val_L: 414.69, VAl_RMSE: 20.36, Test_L: 1152.32, Test_RMSE: 33.95 \n",
      "Epoch 23/450, Train_L: 717.25, Val_L: 365.87, VAl_RMSE: 19.13, Test_L: 1344.43, Test_RMSE: 36.67 \n",
      "Epoch 24/450, Train_L: 673.33, Val_L: 513.61, VAl_RMSE: 22.66, Test_L: 749.92, Test_RMSE: 27.38 \n",
      "Epoch 25/450, Train_L: 645.75, Val_L: 319.57, VAl_RMSE: 17.88, Test_L: 1132.04, Test_RMSE: 33.65 \n",
      "Epoch 26/450, Train_L: 614.63, Val_L: 372.18, VAl_RMSE: 19.29, Test_L: 815.10, Test_RMSE: 28.55 \n",
      "Epoch 27/450, Train_L: 585.67, Val_L: 313.35, VAl_RMSE: 17.70, Test_L: 964.82, Test_RMSE: 31.06 \n",
      "Epoch 28/450, Train_L: 559.66, Val_L: 281.50, VAl_RMSE: 16.78, Test_L: 1098.36, Test_RMSE: 33.14 \n",
      "Epoch 29/450, Train_L: 525.22, Val_L: 374.97, VAl_RMSE: 19.36, Test_L: 755.47, Test_RMSE: 27.49 \n",
      "Epoch 30/450, Train_L: 513.31, Val_L: 284.33, VAl_RMSE: 16.86, Test_L: 967.74, Test_RMSE: 31.11 \n",
      "Epoch 31/450, Train_L: 487.85, Val_L: 333.72, VAl_RMSE: 18.27, Test_L: 744.25, Test_RMSE: 27.28 \n",
      "Epoch 32/450, Train_L: 489.20, Val_L: 314.29, VAl_RMSE: 17.73, Test_L: 752.89, Test_RMSE: 27.44 \n",
      "Epoch 33/450, Train_L: 467.81, Val_L: 319.97, VAl_RMSE: 17.89, Test_L: 677.54, Test_RMSE: 26.03 \n",
      "Epoch 34/450, Train_L: 460.80, Val_L: 269.02, VAl_RMSE: 16.40, Test_L: 900.39, Test_RMSE: 30.01 \n",
      "Epoch 35/450, Train_L: 437.91, Val_L: 272.39, VAl_RMSE: 16.50, Test_L: 799.50, Test_RMSE: 28.28 \n",
      "Epoch 36/450, Train_L: 442.98, Val_L: 265.29, VAl_RMSE: 16.29, Test_L: 796.38, Test_RMSE: 28.22 \n",
      "Epoch 37/450, Train_L: 426.26, Val_L: 375.72, VAl_RMSE: 19.38, Test_L: 519.52, Test_RMSE: 22.79 \n",
      "Epoch 38/450, Train_L: 419.29, Val_L: 330.14, VAl_RMSE: 18.17, Test_L: 590.21, Test_RMSE: 24.29 \n",
      "Epoch 39/450, Train_L: 407.91, Val_L: 269.16, VAl_RMSE: 16.41, Test_L: 736.62, Test_RMSE: 27.14 \n",
      "Epoch 40/450, Train_L: 391.89, Val_L: 238.91, VAl_RMSE: 15.46, Test_L: 775.60, Test_RMSE: 27.85 \n",
      "Epoch 41/450, Train_L: 391.91, Val_L: 219.61, VAl_RMSE: 14.82, Test_L: 878.84, Test_RMSE: 29.65 \n",
      "Epoch 42/450, Train_L: 387.99, Val_L: 223.16, VAl_RMSE: 14.94, Test_L: 835.83, Test_RMSE: 28.91 \n",
      "Epoch 43/450, Train_L: 375.97, Val_L: 216.65, VAl_RMSE: 14.72, Test_L: 868.01, Test_RMSE: 29.46 \n",
      "Epoch 44/450, Train_L: 365.41, Val_L: 209.09, VAl_RMSE: 14.46, Test_L: 894.96, Test_RMSE: 29.92 \n",
      "Epoch 45/450, Train_L: 376.79, Val_L: 221.82, VAl_RMSE: 14.89, Test_L: 854.66, Test_RMSE: 29.23 \n",
      "Epoch 46/450, Train_L: 379.15, Val_L: 227.71, VAl_RMSE: 15.09, Test_L: 764.22, Test_RMSE: 27.64 \n",
      "Epoch 47/450, Train_L: 367.06, Val_L: 238.72, VAl_RMSE: 15.45, Test_L: 713.55, Test_RMSE: 26.71 \n",
      "Epoch 48/450, Train_L: 363.56, Val_L: 243.26, VAl_RMSE: 15.60, Test_L: 703.59, Test_RMSE: 26.53 \n",
      "Epoch 49/450, Train_L: 351.11, Val_L: 291.06, VAl_RMSE: 17.06, Test_L: 582.70, Test_RMSE: 24.14 \n",
      "Epoch 50/450, Train_L: 351.20, Val_L: 214.06, VAl_RMSE: 14.63, Test_L: 792.92, Test_RMSE: 28.16 \n",
      "Epoch 51/450, Train_L: 343.70, Val_L: 234.49, VAl_RMSE: 15.31, Test_L: 714.37, Test_RMSE: 26.73 \n",
      "Epoch 52/450, Train_L: 358.09, Val_L: 246.21, VAl_RMSE: 15.69, Test_L: 666.34, Test_RMSE: 25.81 \n",
      "Epoch 53/450, Train_L: 344.55, Val_L: 192.93, VAl_RMSE: 13.89, Test_L: 897.05, Test_RMSE: 29.95 \n",
      "Epoch 54/450, Train_L: 336.27, Val_L: 203.00, VAl_RMSE: 14.25, Test_L: 838.59, Test_RMSE: 28.96 \n",
      "Epoch 55/450, Train_L: 335.92, Val_L: 192.10, VAl_RMSE: 13.86, Test_L: 884.99, Test_RMSE: 29.75 \n",
      "Epoch 56/450, Train_L: 321.87, Val_L: 200.01, VAl_RMSE: 14.14, Test_L: 822.98, Test_RMSE: 28.69 \n",
      "Epoch 57/450, Train_L: 326.63, Val_L: 194.94, VAl_RMSE: 13.96, Test_L: 811.97, Test_RMSE: 28.50 \n",
      "Epoch 58/450, Train_L: 322.90, Val_L: 199.73, VAl_RMSE: 14.13, Test_L: 783.12, Test_RMSE: 27.98 \n",
      "Epoch 59/450, Train_L: 327.69, Val_L: 221.06, VAl_RMSE: 14.87, Test_L: 717.47, Test_RMSE: 26.79 \n",
      "Epoch 60/450, Train_L: 312.69, Val_L: 190.17, VAl_RMSE: 13.79, Test_L: 820.81, Test_RMSE: 28.65 \n",
      "Epoch 61/450, Train_L: 304.41, Val_L: 206.55, VAl_RMSE: 14.37, Test_L: 723.42, Test_RMSE: 26.90 \n",
      "Epoch 62/450, Train_L: 315.56, Val_L: 178.74, VAl_RMSE: 13.37, Test_L: 859.60, Test_RMSE: 29.32 \n",
      "Epoch 63/450, Train_L: 308.86, Val_L: 192.32, VAl_RMSE: 13.87, Test_L: 782.39, Test_RMSE: 27.97 \n",
      "Epoch 64/450, Train_L: 320.32, Val_L: 215.47, VAl_RMSE: 14.68, Test_L: 685.16, Test_RMSE: 26.18 \n",
      "Epoch 65/450, Train_L: 311.56, Val_L: 194.72, VAl_RMSE: 13.95, Test_L: 760.90, Test_RMSE: 27.58 \n",
      "Epoch 66/450, Train_L: 304.88, Val_L: 202.65, VAl_RMSE: 14.24, Test_L: 716.11, Test_RMSE: 26.76 \n",
      "Epoch 67/450, Train_L: 312.18, Val_L: 190.20, VAl_RMSE: 13.79, Test_L: 787.09, Test_RMSE: 28.06 \n",
      "Epoch 68/450, Train_L: 302.37, Val_L: 174.10, VAl_RMSE: 13.19, Test_L: 887.74, Test_RMSE: 29.80 \n",
      "Epoch 69/450, Train_L: 312.06, Val_L: 181.69, VAl_RMSE: 13.48, Test_L: 829.60, Test_RMSE: 28.80 \n",
      "Epoch 70/450, Train_L: 307.91, Val_L: 176.11, VAl_RMSE: 13.27, Test_L: 876.92, Test_RMSE: 29.61 \n",
      "Epoch 71/450, Train_L: 296.25, Val_L: 187.65, VAl_RMSE: 13.70, Test_L: 791.49, Test_RMSE: 28.13 \n",
      "Epoch 72/450, Train_L: 305.77, Val_L: 244.28, VAl_RMSE: 15.63, Test_L: 606.42, Test_RMSE: 24.63 \n",
      "Epoch 73/450, Train_L: 302.16, Val_L: 196.30, VAl_RMSE: 14.01, Test_L: 750.13, Test_RMSE: 27.39 \n",
      "Epoch 74/450, Train_L: 301.52, Val_L: 171.67, VAl_RMSE: 13.10, Test_L: 906.86, Test_RMSE: 30.11 \n",
      "Epoch 75/450, Train_L: 292.20, Val_L: 180.73, VAl_RMSE: 13.44, Test_L: 811.76, Test_RMSE: 28.49 \n",
      "Epoch 76/450, Train_L: 303.02, Val_L: 187.97, VAl_RMSE: 13.71, Test_L: 771.47, Test_RMSE: 27.78 \n",
      "Epoch 77/450, Train_L: 285.81, Val_L: 205.03, VAl_RMSE: 14.32, Test_L: 707.22, Test_RMSE: 26.59 \n",
      "Epoch 78/450, Train_L: 291.86, Val_L: 181.26, VAl_RMSE: 13.46, Test_L: 820.08, Test_RMSE: 28.64 \n",
      "Epoch 79/450, Train_L: 288.83, Val_L: 167.79, VAl_RMSE: 12.95, Test_L: 931.39, Test_RMSE: 30.52 \n",
      "Epoch 80/450, Train_L: 285.81, Val_L: 181.05, VAl_RMSE: 13.46, Test_L: 796.39, Test_RMSE: 28.22 \n",
      "Epoch 81/450, Train_L: 283.39, Val_L: 185.41, VAl_RMSE: 13.62, Test_L: 767.55, Test_RMSE: 27.70 \n",
      "Epoch 82/450, Train_L: 290.28, Val_L: 181.02, VAl_RMSE: 13.45, Test_L: 794.61, Test_RMSE: 28.19 \n",
      "Epoch 83/450, Train_L: 281.12, Val_L: 184.52, VAl_RMSE: 13.58, Test_L: 776.92, Test_RMSE: 27.87 \n",
      "Epoch 84/450, Train_L: 282.69, Val_L: 192.84, VAl_RMSE: 13.89, Test_L: 742.08, Test_RMSE: 27.24 \n",
      "Epoch 85/450, Train_L: 285.58, Val_L: 174.88, VAl_RMSE: 13.22, Test_L: 815.77, Test_RMSE: 28.56 \n",
      "Epoch 86/450, Train_L: 276.94, Val_L: 197.85, VAl_RMSE: 14.07, Test_L: 713.27, Test_RMSE: 26.71 \n",
      "Epoch 87/450, Train_L: 280.35, Val_L: 180.63, VAl_RMSE: 13.44, Test_L: 772.63, Test_RMSE: 27.80 \n",
      "Epoch 88/450, Train_L: 271.92, Val_L: 183.80, VAl_RMSE: 13.56, Test_L: 755.60, Test_RMSE: 27.49 \n",
      "Epoch 89/450, Train_L: 275.54, Val_L: 197.12, VAl_RMSE: 14.04, Test_L: 698.14, Test_RMSE: 26.42 \n",
      "Epoch 90/450, Train_L: 273.97, Val_L: 183.75, VAl_RMSE: 13.56, Test_L: 756.13, Test_RMSE: 27.50 \n",
      "Epoch 91/450, Train_L: 275.79, Val_L: 178.30, VAl_RMSE: 13.35, Test_L: 773.21, Test_RMSE: 27.81 \n",
      "Epoch 92/450, Train_L: 276.28, Val_L: 190.38, VAl_RMSE: 13.80, Test_L: 718.18, Test_RMSE: 26.80 \n",
      "Epoch 93/450, Train_L: 281.45, Val_L: 186.50, VAl_RMSE: 13.66, Test_L: 737.54, Test_RMSE: 27.16 \n",
      "Epoch 94/450, Train_L: 273.34, Val_L: 193.10, VAl_RMSE: 13.90, Test_L: 716.32, Test_RMSE: 26.76 \n",
      "Epoch 95/450, Train_L: 272.59, Val_L: 211.07, VAl_RMSE: 14.53, Test_L: 653.02, Test_RMSE: 25.55 \n",
      "Epoch 96/450, Train_L: 272.33, Val_L: 203.99, VAl_RMSE: 14.28, Test_L: 672.42, Test_RMSE: 25.93 \n",
      "Epoch 97/450, Train_L: 281.03, Val_L: 170.20, VAl_RMSE: 13.05, Test_L: 812.39, Test_RMSE: 28.50 \n",
      "Epoch 98/450, Train_L: 259.29, Val_L: 159.25, VAl_RMSE: 12.62, Test_L: 906.35, Test_RMSE: 30.11 \n",
      "Epoch 99/450, Train_L: 279.44, Val_L: 176.88, VAl_RMSE: 13.30, Test_L: 772.68, Test_RMSE: 27.80 \n",
      "Epoch 100/450, Train_L: 269.92, Val_L: 204.41, VAl_RMSE: 14.30, Test_L: 666.93, Test_RMSE: 25.82 \n",
      "Epoch 101/450, Train_L: 269.39, Val_L: 195.16, VAl_RMSE: 13.97, Test_L: 698.26, Test_RMSE: 26.42 \n",
      "Epoch 102/450, Train_L: 259.65, Val_L: 195.57, VAl_RMSE: 13.98, Test_L: 695.37, Test_RMSE: 26.37 \n",
      "Epoch 103/450, Train_L: 273.98, Val_L: 195.77, VAl_RMSE: 13.99, Test_L: 694.80, Test_RMSE: 26.36 \n",
      "Epoch 104/450, Train_L: 273.78, Val_L: 200.71, VAl_RMSE: 14.17, Test_L: 683.71, Test_RMSE: 26.15 \n",
      "Epoch 105/450, Train_L: 277.04, Val_L: 184.44, VAl_RMSE: 13.58, Test_L: 734.88, Test_RMSE: 27.11 \n",
      "Epoch 106/450, Train_L: 277.73, Val_L: 192.33, VAl_RMSE: 13.87, Test_L: 707.49, Test_RMSE: 26.60 \n",
      "Epoch 107/450, Train_L: 269.88, Val_L: 188.41, VAl_RMSE: 13.73, Test_L: 714.13, Test_RMSE: 26.72 \n",
      "Epoch 108/450, Train_L: 259.16, Val_L: 170.85, VAl_RMSE: 13.07, Test_L: 790.93, Test_RMSE: 28.12 \n",
      "Epoch 109/450, Train_L: 267.22, Val_L: 197.71, VAl_RMSE: 14.06, Test_L: 687.30, Test_RMSE: 26.22 \n",
      "Epoch 110/450, Train_L: 273.88, Val_L: 194.34, VAl_RMSE: 13.94, Test_L: 693.51, Test_RMSE: 26.33 \n",
      "Epoch 111/450, Train_L: 262.16, Val_L: 199.70, VAl_RMSE: 14.13, Test_L: 674.58, Test_RMSE: 25.97 \n",
      "Epoch 112/450, Train_L: 270.66, Val_L: 199.40, VAl_RMSE: 14.12, Test_L: 676.01, Test_RMSE: 26.00 \n",
      "Epoch 113/450, Train_L: 263.16, Val_L: 178.06, VAl_RMSE: 13.34, Test_L: 748.84, Test_RMSE: 27.36 \n",
      "Epoch 114/450, Train_L: 264.35, Val_L: 180.04, VAl_RMSE: 13.42, Test_L: 742.40, Test_RMSE: 27.25 \n",
      "Epoch 115/450, Train_L: 262.75, Val_L: 195.08, VAl_RMSE: 13.97, Test_L: 676.15, Test_RMSE: 26.00 \n",
      "Epoch 116/450, Train_L: 265.74, Val_L: 187.89, VAl_RMSE: 13.71, Test_L: 705.25, Test_RMSE: 26.56 \n",
      "Epoch 117/450, Train_L: 258.12, Val_L: 209.84, VAl_RMSE: 14.49, Test_L: 638.14, Test_RMSE: 25.26 \n",
      "Epoch 118/450, Train_L: 279.40, Val_L: 191.46, VAl_RMSE: 13.84, Test_L: 695.25, Test_RMSE: 26.37 \n",
      "Epoch 119/450, Train_L: 259.06, Val_L: 178.03, VAl_RMSE: 13.34, Test_L: 754.06, Test_RMSE: 27.46 \n",
      "Epoch 120/450, Train_L: 264.82, Val_L: 186.78, VAl_RMSE: 13.67, Test_L: 713.06, Test_RMSE: 26.70 \n",
      "Epoch 121/450, Train_L: 262.21, Val_L: 177.27, VAl_RMSE: 13.31, Test_L: 752.13, Test_RMSE: 27.43 \n",
      "Epoch 122/450, Train_L: 255.37, Val_L: 194.16, VAl_RMSE: 13.93, Test_L: 690.37, Test_RMSE: 26.27 \n",
      "Epoch 123/450, Train_L: 263.48, Val_L: 202.10, VAl_RMSE: 14.22, Test_L: 665.09, Test_RMSE: 25.79 \n"
     ]
    }
   ],
   "source": [
    "# Example data (replace with actual data loading)\n",
    "seq_len, batch_size, feature_size = X_train.shape[1], 64, X_train.shape[2]\n",
    "num_heads, num_layers, project_dim  = 8, 1, 192 #12 * 4 * 2\n",
    "num_epochs = 450\n",
    "learning_rate = 0.0001\n",
    "\n",
    "print(seq_len)\n",
    "# Create dataset and dataloaders\n",
    "train_dataset = nu.TurbofanDataset(X_train, y_train)\n",
    "val_dataset = nu.TurbofanDataset(X_val, y_val)\n",
    "test_dataset = nu.TurbofanDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "# Initialize model, criterion, optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TransformerModel(feature_size, num_heads, num_layers, project_dim = project_dim, window_size = seq_len).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "\n",
    "print(f\"The model has in total {count_parameters(model)} parameters!!\")\n",
    "    \n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_model(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss = evaluate_model(model, val_loader, criterion, device)\n",
    "    test_loss = evaluate_model(model, test_loader, criterion, device)\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train_L: {train_loss:.2f}, Val_L: {val_loss:.2f}, VAl_RMSE: {np.sqrt(val_loss):.2f}, Test_L: {test_loss:.2f}, Test_RMSE: {np.sqrt(test_loss):.2f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def get_predictions(model, dataloader, criterion, device):\n",
    "    pred = []\n",
    "    tar  = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            targets = targets.view(-1, 1).cpu()\n",
    "            outputs = model(inputs).cpu()\n",
    "            for i in outputs:\n",
    "                pred.append(i.detach())\n",
    "            for j in targets:\n",
    "                tar.append(j.detach())\n",
    "    return pred, tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred, targets = get_predictions(model, test_loader, criterion, device)\n",
    "pred, targets = get_predictions(model, val_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.plot((pred[:3000]), label=\"Prediction\")\n",
    "plt.plot((targets[:3000]), label=\"Reale RUL\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = calculate_RUL_test(test_data, test_RUL_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[test_data[\"UnitNumber\"] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> 431: epoch 398, ohne scheduler\n",
    "seq_len, batch_size, feature_size = X_train.shape[1], 32, X_train.shape[2]\n",
    "num_heads, num_layers, project_dim  = 12, 2, 12 * 4 * 2\n",
    "num_epochs = 350\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> 193: 817\n",
    "seq_len, batch_size, feature_size = X_train.shape[1], 32, X_train.shape[2]\n",
    "num_heads, num_layers, project_dim  = 12, 2, 12 * 4 * 2\n",
    "num_epochs = 450\n",
    "learning_rate = 0.0005\n",
    "scheduler = StepLR(optimizer, step_size=30, gamma=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data (replace with actual data loading)\n",
    "seq_len, batch_size, feature_size = X_train.shape[1], 64, X_train.shape[2]\n",
    "num_heads, num_layers, project_dim  = 16, 1, 12 * 4 * 2\n",
    "num_epochs = 450\n",
    "learning_rate = 0.0001"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
